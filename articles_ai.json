[
    {
        "title": "Patronus AI’s Judge-Image wants to keep AI honest — and Etsy is already using it",
        "url": "https://venturebeat.com/ai/patronus-ais-judge-image-wants-to-keep-ai-honest-and-etsy-is-already-using-it/",
        "content": "Patronus AI announced today the launch of what it calls the industry’s first multimodal large language model-as-a-judge (MLLM-as-a-Judge), a tool designed to evaluate AI systems that interpret images and produce text.\nThe new evaluation technology aims to help developers detect and mitigate hallucinations and reliability issues in multimodal AI applications. E-commerce giant Etsy has already implemented the technology to verify caption accuracy for product images across its marketplace of handmade and vintage goods.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\n“Super excited to announce that Etsy is one of our ship customers,” said Anand Kannappan, cofounder of Patronus AI, in an exclusive interview with VentureBeat. “They have hundreds of millions of items in their online marketplace for handmade and vintage products that people are creating around the world. One of the things that their AI team wanted to be able to leverage generative AI for was the ability to auto-generate image captions and to make sure that as they scale across their entire global user base, that the captions that are generated are ultimately correct.”\nWhy Google’s Gemini powers the new AI judge rather than OpenAI\nPatronus built its first MLLM-as-a-Judge, called Judge-Image, on Google’s Gemini model after extensive research comparing it with alternatives like OpenAI’s GPT-4V.\n“We tended to see that there was a slighter preference toward egocentricity with GPT-4V, whereas we saw that Gemini was less biased in those ways and had more of an equitable approach to being able to judge different kinds of input-output pairs,” Kannappan explained. “That was seen in the uniform scoring distribution across the different sources that they looked at.”\nThe company’s research yielded another surprising insight about multimodal evaluation. Unlike text-only evaluations where multi-step reasoning often improves performance, Kannappan noted that it “typically doesn’t actually increase MLLM judge performance” for image-based assessments.\nJudge-Image provides ready-to-use evaluators that assess image captions on multiple criteria, including caption hallucination detection, recognition of primary and non-primary objects, object location accuracy, and text detection and analysis.\nBeyond retail: How marketing teams and law firms can benefit from AI image evaluation\nADVERTISEMENT\nWhile Etsy represents a flagship customer in e-commerce, Patronus sees applications extending far beyond retail.\nThese include “marketing teams across companies that are generally looking at being able to scalably create descriptions and captions against new blocks in design, especially marketing design, but also product design,” Kannappan said.\nHe also highlighted applications for enterprises dealing with document processing: “Larger enterprises like venture services companies and law firms typically might have engineering teams that are using relatively legacy technology to be able to extract different kinds of information from PDFs, to be able to summarize the content inside of larger documents.”\nWhy companies should buy AI evaluation tools instead of building their own\nADVERTISEMENT\nAs AI becomes increasingly critical to business processes, many companies face the build-versus-buy dilemma for evaluation tools. Kannappan argues that outsourcing AI evaluation makes strategic and economic sense.\n“As we’ve worked with teams, [we’ve found that] a lot of folks may start with something to see if they can develop something internally, and then they realize that it’s, one, not core to their value prop or the product they’re developing. And two, it is a very challenging problem, both from an AI perspective, but also from an infrastructure perspective,” he said.\nThis applies particularly to multimodal systems, where failures can occur at multiple points in the process. “When you’re dealing with RAG systems or agents, or even multimodal AI systems, we’re seeing that failures happen across all parts of the system,” Kannappan noted.\nHow Patronus plans to make money while competing with tech giants\nPatronus offers multiple pricing tiers, starting with a free option that allows users to experiment with the platform up to certain volume limits. Beyond that threshold, customers pay as they go for evaluator usage or can engage with the sales team for enterprise arrangements with custom features and tailored pricing.\nDespite using Google’s Gemini model as its foundation, the company positions itself as complementary rather than competitive with foundation model providers like Google, OpenAI and Anthropic.\n“We don’t necessarily see the technology that we build or the solutions that we build as competitive with foundational companies, but rather very complementary and additional new powerful tools in the toolkit that ultimately help folks develop better LLM systems, as opposed to LLMs themselves,” Kannappan said.\nAudio evaluation coming next as Patronus expands multimodal oversight\nToday’s announcement represents one step in Patronus’s broader strategy for AI evaluation across different modalities. The company plans to expand beyond images into audio evaluation soon.\n“We’re excited because this is the next phase of our vision towards multimodal, and specifically focused on images today — and then over time, we’re excited about what we’ll do, especially with audio in the future,” Kannappan confirmed.\nThis roadmap aligns with what Kannappan describes as the company’s “research vision towards scalable oversight” — developing evaluation mechanisms that can keep pace with increasingly sophisticated AI systems.\n“We continue to develop new systems, products, frameworks, methods that ultimately are equally capable as the intelligent systems that we intend to want to have oversight over as humans in the long run,” he said.\nAs businesses race to deploy AI systems that can interpret images, extract text from documents, and generate visual content, the risk of inaccuracies, hallucinations and biases grows. Patronus is betting that even as foundation models improve, the challenges of evaluating complex multimodal AI systems will remain — requiring specialized tools that can serve as impartial judges of increasingly human-like AI output. In the high-stakes world of commercial AI deployment, these digital judges may prove as valuable as the models they evaluate.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "GenLayer offers novel approach for AI agent transactions: getting multiple LLMs to vote on a suitable contract",
        "url": "https://venturebeat.com/ai/genlayer-offers-novel-approach-for-ai-agent-transactions-getting-multiple-llms-to-vote-on-a-suitable-contract/",
        "content": "We’ve been hearing a lot about AI agents — tools powered by generative AI models that can perform actions without much human supervision or intervention.\nBut they still remain largely a novel curiosity for most people, and as far as we can tell, very few people are trusting AI agents to buy or enter contracts on their behalf — for now.\nNavigating AI Regulations in Financial Services - AI Impact Tour 2024\nGenLayer, a startup just out of stealth, believes it has a technology that will provide the missing “trust” component to the AI agent economy.\nGenLayer’s idea is a blockchain-powered infrastructure that allows AI agents to draft contracts, settle payments and execute agreements autonomously.\nLast fall, the company announced it had raised $7.5 million from notable investors, including Arthur Hayes (Maelstrom), Arrington Capital and North Island Ventures, to bring this vision to life.\nHow to make AI agents trustworthy to people — and to one another\nAI agents are already proving their ability to analyze data, make deals and manage assets, but there’s a fundamental problem: They don’t inherently trust each other. Unlike humans, AI agents don’t fear lawsuits or reputational damage — so how do they enforce agreements?\nAlbert Castellana, CEO of YeagerAI (the company building GenLayer), sees this as a critical flaw in today’s AI development.\nAlbert Castellana, CEO of YaegerAI\n“Even in a situation where you have agents that can do commerce between themselves, how do they trust each other?” Castellana said in a recent interview with VentureBeat. “AI doesn’t sleep, AI works globally, AI cannot go to jail. The legal system will have a big issue dealing with that type of situation.”\nTraditional smart contracts, which power blockchain-based transactions, are too rigid for AI-driven commerce.\nThey can’t process unstructured data, understand complex language or adapt to real-world changes.\nADVERTISEMENT\nGenLayer wants to upgrade smart contracts into “intelligent contracts” — more flexible, AI-powered agreements that function much like human contracts.\nUnlike traditional blockchains that require external oracles to access off-chain data, GenLayer integrates AI directly at the protocol level. Intelligent contracts can natively fetch live web data, process natural language inputs and reason about complex, real-world conditions — all without relying on third-party services.\n“Blockchains allow for self-enforcing contracts, but they have limitations,” Castellana explained. “They can’t connect to the outside world, they can’t understand unstructured data. But AI needs contracts that are much more like human contracts — fast, cheap and adaptive.”\nADVERTISEMENT\nGenLayer solves this with “optimistic democracy, an AI-driven consensus model where multiple validators — each using different large language models (LLMs) — vote on whether an AI-generated contract or decision is valid. This ensures that no single AI model has control and prevents manipulation.\n“We’ve created a blockchain where validators, even if they get different responses from AI or the internet, can still reach consensus,” said Castellana. “It’s basically a court system for the future of commerce.”\nEdgars Nemše, co-founder of GenLayer\nJosé María Lago, co-founder of GenLayer\nHow GenLayer’s approach works\nAt its core, GenLayer operates as an AI-native trust layer — an independent system that ensures AI agents operate fairly in financial transactions, contract execution and dispute resolution.\nKey features include:\nIntelligent contracts: AI-powered agreements that process natural language and access live web data.\nAI-driven decision-making: A consensus model where multiple AI models vote on outcomes to ensure reliability.\nOptimistic democracy: A blockchain-based governance model that prevents AI manipulation by using decentralized decision-making.\nOn-chain and off-chain interoperability: The ability to connect smart contracts with real-world data and internet sources.\nZKsync integration: Scalability, low costs and Ethereum-level security.\nAt the heart of GenLayer is “optimistic democracy,” an enhanced delegated proof of stake (dPoS) model that integrates AI directly into blockchain validation. Instead of relying on deterministic logic, validators connect to LLMs to process natural language, interpret data and execute complex decisions on-chain.\nWhen a transaction is submitted:\nA leader validator processes the request and proposes an outcome.\nA set of validators recompute the transaction independently, validating the leader’s proposal.\nIf the majority agrees, the transaction is finalized. If not, a new leader is selected, and the process repeats.\nThis mechanism prevents manipulation and ensures AI-generated decisions are backed by consensus rather than a single entity’s judgment.\nInspired by Condorcet’s Jury Theorem — an 18th-century mathematical and political science theory by the Marquis de Condorcet that says a jury is more likely to reach a correct decision with more participants — the system aggregates AI outputs across multiple validators, ensuring fairness and reliability even for non-deterministic tasks like interpreting legal contracts, verifying supply chain data or setting dynamic pricing models.\nThe approach is described in a whitepaper published by GenLayer’s three co-founders — Castellana, José María Lago and Edgars Nemše. You can find it embedded below.\n66c60f3d7295d3113d4699ba_GenLayer Whitepaper_ The Intelligent Contract NetworkDownload\nWhy GenLayer thinks its moment has arrived\nThe race to create autonomous AI businesses is accelerating. Companies like OpenAI are rolling out AI agents that can work independently, but they still rely on slow, human-driven legal and financial systems.\n“AI won’t wait for lawyers,” Castellana emphasized. “If we want AI to participate in the economy, we need infrastructure that matches its speed.”\nOther startups are tackling AI-agent transactions — such as Skyfire and Pin AI — but GenLayer takes a different approach. Instead of focusing on building AI agents themselves, GenLayer is creating the trust layer that enables them to transact.\n“There are 100 startups working on AI agents,” said Castellana. “But trust requires a third party. We’re building that third party — the infrastructure that makes AI commerce possible.”\nTo incentivize validators and cover the costs of executing Intelligent Contracts, GenLayer introduces a native gas token called GEN. Users pay transaction fees in GEN, which are then distributed to the validators as a reward for their services.\nThis approach ensures that AI-driven transactions remain fast, low-cost and self-sustaining. Additionally, GenLayer’s token-based staking model aligns incentives by rewarding honest validators and penalizing bad actors through slashing mechanisms.\nWhat’s next?\nGenLayer’s testnet is launching early projects focused on:\nAI-run supply chains that autonomously negotiate logistics.\nAI-powered influencer marketing where payments are performance-based.\nDecentralized finance applications that use AI for trade execution.\nAutonomous decentralized autonomous organizations (DAOs) that adjust governance based on real-time community feedback.\nAI-powered insurance and lending that adapt terms based on external risk factors.\nThe company will showcase its technology at the Ethereum Community Conference (EthCC8) taking place June 30-July 3 in Cannes, France.\nThe AI economy is coming — but without trust, it won’t scale. GenLayer is betting that AI-driven contracts, enforced on the blockchain, will be the foundation for a trillion-dollar machine-driven marketplace.\n“We are building the legal framework for global AI commerce,” said Castellana. “This is about enabling AI to work together, not just faster, but with trust.”\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "AMD is powering AI success with smarter, right-sized compute",
        "url": "https://venturebeat.com/ai/amd-is-powering-ai-success-with-smarter-right-sized-compute/",
        "content": "Presented by AMD\nAs AI adoption accelerates, businesses are encountering compute bottlenecks that extend beyond just raw processing power. The challenge is not only about having more compute; it’s about having smarter, more efficient compute, customized to an organization’s needs, with the ability to scale alongside AI innovation. AI models are growing in size and complexity, requiring architectures that can process massive datasets, support continuous learning and provide the efficiency needed for real-time decision-making.\nFrom AI training and inference in hyperscale data centers to AI-driven automation in enterprises, the ability to deploy and scale compute infrastructure seamlessly is now a competitive differentiator.\n“It’s a tall order. Organizations are struggling to stay up-to-date with AI compute demands, scale AI workloads efficiently and optimize their infrastructure,” says Mahesh Balasubramanian, director, datacenter GPU product marketing at AMD. “Every company we talk to wants to be at the forefront of AI adoption and business transformation. The challenge is, they’ve never been faced before with such a massive, era-defining technology.”\nLaunching a nimble AI strategy\nWhere to start? Modernizing existing data centers is an essential first step to removing bottlenecks to AI innovation. This frees up space and power, improves efficiency and greens the data center, all of which helps the organization stay nimble enough to adapt to the changing AI environment.\n“You can upgrade your existing data center from a three-generation-old, Intel Xeon 8280 CPU, to the latest generation of AMD EPYC CPU and save up to 68% on energy while using 87% fewer servers3,” Balasubramanian says. “It’s not just a smart and efficient way of upgrading an existing data center, it opens up options for the next steps in upgrading a company’s compute power.”\nAnd as an organization evolves its AI strategy, it’s critical to have a plan for fast-growing hardware and computational requirements. It’s a complex undertaking, whether you’re working with a single model underlying organizational processes, customized models for each department or agentic AI.\n“If you understand your foundational situation – where AI will deployed, and what infrastructure is already available from a space, power, efficiency and cost perspective – you have a huge number of robust technology solutions to solve these problems,” Balasubramanian says.\nBeyond one-size-fits all compute\nA common perception in the enterprise is that AI solutions require a massive investment right out of the gate, across the board, on hardware, software and services. That has proven to be one of the most common barriers to adoption — and an easy one to overcome, Balasubramanian says. The AI journey kicks off with a look at existing tech and upgrades to the data center; from there, an organization can start scaling for the future by choosing technology that can be right-sized for today’s problems and tomorrow’s goals.\n“Rather than spending everything on one specific type of product or solution, you can now right-size the fit and solution for the organizations you have,” Balasubramanian says. “AMD is unique in that we have a broad set of solutions to meet bespoke requirements. We have solutions from cloud to data center, edge solutions, client and network solutions and more. This broad portfolio lets us provide the best performance across all solutions, and lets us offer in-depth guidance to enterprises looking for the solution that fits their needs.”\nThat AI portfolio is designed to tackle the most demanding AI workloads — from foundation model training to edge inference. The latest AMD InstinctTM MI325X GPUs, powered by HBM3e memory and CDNA architecture, deliver superior performance for generative AI workloads, providing up to 1.3X better inference performance compared to competing solutions1,2. AMD EPYC CPUs continue to set industry standards, delivering unmatched core density, energy efficiency and high-memory bandwidth critical for AI compute scalability.\nCollaboration with a wide range of industry leaders — including OEMs like Dell, Supermicro, Lenovo, and HPE, network vendors like Broadcom and Marvell, and switching vendors like Arista and Cisco — maximizes the modularity of these data center solutions. It scales seamlessly from two or four servers to thousands, all built with next gen Ethernet-based AI networking and backed by industry-leading technology and expertise.\nWhy open-source software is critical for AI advancement\nWhile both hardware and software are crucial for tackling today’s AI challenges, open-source software will drive true innovation.\n“We believe there’s no one company in this world that has the answers for every problem,” Balasubramanian says. “The best way to solve the world’s problems with AI is to have a united front, and to have a united front means having an open software stack that everyone can collaborate on. That’s a key part of our vision.”\nAMD’s open-source software stack, ROCmTM, is widely adopted by industry leaders like OpenAI, Microsoft, Meta, Oracle and more. Meta runs its largest and most complicated model on AMD Instinct GPUs. ROCm comes with standard support for PyTorch, the largest AI framework, and has more than a million models from Hugging Face’s premium model repository enabling customers begin their journey with seamless out of the box experience on ROCm software and Instinct GPUs.\nAMD works with vendors like PyTorch, Tensorflow, JAX, OpenAI’s Triton and others to ensure that no matter what the size of the model, small or large, applications and use cases can scale anywhere from a single GPU all the way to tens of thousands of GPUs — just as its AI hardware can scale to match any size workload.\nROCm’s deep ecosystem engagement with continuous integration and continuous development ensures that new AI functions and features can be securely integrated into the stack. These features go through an automated testing and development process to ensure it fits in, it’s robust, it doesn’t break anything and it can provide support right away to the software developers and data scientists using it.\nAnd as AI evolves, ROCm is pivoting to offer new capabilities, rather than locking an organization into one particular vendor that might not offer the flexibility necessary to grow.\n“We want to give organizations an open-source software stack that is completely open all the way from the top to the bottom, and all the way across an organization,” he says. “Users can choose the layers that meet their needs and modify them as necessary, or run models right out of the box, ensuring that enterprises can run intensive models like DeepSeek, Llama or the latest Gemma models from Google from day one.”\nA look ahead: AMD’s vision for AI compute\nAs organizations embrace AI’s early revolution, they need to avoid getting locked into one particular solution, finding compute solutions that meet their needs now and in the future. Working with an industry expert is critical to identifying those needs, and what is required to carry them forward as AI changes the world.\nAMD is driving that change, collaborating with leading AI labs at the forefront of AI development, and the broader ecosystem of developers and leading software companies. With a growing customer base that includes Microsoft, Meta, Dell Technologies, HPE, Lenovo and others, AMD is shaping the AI landscape by providing high-performance, energy-efficient solutions that drive innovation across industries\nLooking ahead, that collaboration is foundational to AMD’s technology roadmap. The company is investing in comprehensive hardware and software solutions, including the recent acquisition of ZT Systems, bringing essential server & cluster design expertise to bring full-stack solutions to market fast with our OEM, ODM and Cloud partners.\nAnd as models become larger and more sophisticated, hardware demands are increasing exponentially. This is what drives AMD’s product strategy and feature sets: to ensure its portfolio of solutions can scale, with open and flexible AI infrastructure that maintains performance and efficiency.\n“This broad portfolio is designed to right-size AI solutions to provide the best performance across every customer setup and power AI strategies of every size,” Balasubramanian says. “No matter which part of the AI journey an organization is on, whether they’re building a model or using a model for an end use case, we’d like for them to come and talk to us, and learn how we can help solve their biggest problems.”\nNew AMD Instinct MI325X Accelerators are breaking the boundaries of AI performance — learn more now.\nFootnotes\n1. https://www.amd.com/en/legal/claims/instinct.html#q=MI325-014\n2. https://www.amd.com/en/legal/claims/instinct.html#q=MI325-015\n3. https://www.amd.com/en/legal/claims/epyc.html#q=SP9xxTCO-002A\nSponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com."
    },
    {
        "title": "‘Gradually then suddenly’: Is AI job displacement following this pattern?",
        "url": "https://venturebeat.com/ai/gradually-then-suddenly-is-ai-job-displacement-following-this-pattern/",
        "content": "Whether by automating tasks, serving as copilots or generating text, images, video and software from plain English, AI is rapidly altering how we work. Yet, for all the talk about AI revolutionizing jobs, widespread workforce displacement has yet to happen. \nIt seems likely that this could be the lull before the storm. According to a recent World Economic Forum (WEF) survey, 40% of employers anticipate reducing their workforce between 2025 and 2030 in areas wherever AI can automate tasks. This statistic dovetails well with earlier predictions. For example, Goldman Sachs said in a research report two years ago that “generative AI could expose the equivalent of 300 million full-time jobs to automation leading to “significant disruption” in the labor market. \nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024Why Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nAccording to the International Monetary Fund (IMF) “almost 40% of global employment is exposed to AI.” Brookings said last fall in another report that “more than 30% of all workers could see at least 50% of their occupation’s tasks disrupted by gen AI.” Several years ago, Kai-Fu Lee, one of the world’s foremost AI experts, said in a 60 Minutes interview that AI could displace 40% of global jobs within 15 years.\nIf AI is such a disruptive force, why aren’t we seeing large layoffs?\nSome have questioned those predictions, especially as job displacement from AI so far appears negligible. For example, an October 2024 Challenger Report that tracks job cuts said that in the 17 months between May 2023 and September 2024, fewer than 17,000 jobs in the U.S. had been lost due to AI.  \nOn the surface, this contradicts the dire warnings. But does it? Or does it suggest that we are still in a gradual phase before a possible sudden shift? History shows that technology-driven change does not always happen in a steady, linear fashion. Rather, it builds up over time until a sudden shift reshapes the landscape.\nIn a recent Hidden Brain podcast on inflection points, researcher Rita McGrath of Columbia University referenced Ernest Hemingway’s 1926 novel The Sun Also Rises. When one character was asked how they went bankrupt, they answered: “Two ways. Gradually, then suddenly.” This could be an allegory for the impact of AI on jobs.\nThis pattern of change — slow and nearly imperceptible at first, then suddenly undeniable — has been experienced across business, technology and society. Malcolm Gladwell calls this a “tipping point,” or the moment when a trend reaches critical mass, then dramatically accelerates. \nADVERTISEMENT\nIn cybernetics — the study of complex natural and social systems — a tipping point can occur when recent technology becomes so widespread that it fundamentally changes the way people live and work. In such scenarios, the change becomes self-reinforcing. This often happens when innovation and economic incentives align, making change inevitable.\nGradually, then suddenly\nWhile employment impacts from AI are (so far) nascent, that is not true of AI adoption. In a new survey by McKinsey, 78% of respondents said their organizations use AI in at least one business function, up more than 40% from 2023. Other research found that 74% of enterprise C-suite executives are now more confident in AI for business advice than colleagues or friends. The research also revealed that 38% trust AI to make business decisions for them, while 44% defer to AI reasoning over their own insights. \nADVERTISEMENT\nIt is not only business executives who are increasing their use of AI tools. A new chart from the investment firm Evercore depicts increased use among all age groups over the last 9 months, regardless of application. \nSource: Business Insider\nThis data reveals both broad and growing adoption of AI tools. However, true enterprise AI integration remains in its infancy — just 1% of executives describe their gen AI rollouts as mature, according to another McKinsey survey. This suggests that while AI adoption is surging, companies have yet to fully integrate it into core operations in a way that might displace jobs at scale. But that could change quickly. If economic pressures intensify, businesses may not have the luxury of gradual AI adoption and may feel the need to automate fast.\nCanary in the coal mine\nOne of the first job categories likely to be hit by AI is software development. Numerous AI tools based on large language models (LLMs) exist to augment programming, and soon the function could be entirely automated. Anthropic CEO Dario Amodei said recently on Reddit that “we’re 3 to 6 months from a world where AI is writing 90% of the code. And then in 12 months, we may be in a world where AI is writing essentially all of the code.”\nSource: Reddit\nThis trend is becoming clear, as evidenced by startups in the winter 2025 cohort of incubator Y Combinator. Managing partner Jared Friedman said that 25% of this startup batch have 95% of their codebases generated by AI. He added: “A year ago, [the companies] would have built their product from scratch — but now 95% of it is built by an AI.” \nThe LLMs underlying code generation, such as Claude, Gemini, Grok, Llama and ChatGPT, are all advancing rapidly and increasingly perform well on an array of quantitative benchmark tests. For example, reasoning model o3 from OpenAI missed only one question on the 2024 American Invitational Mathematics Exam, scoring 97.7%, and achieved 87.7% on GPQA Diamond, which has graduate-level biology, physics and chemistry questions.\nEven more striking is a qualitative impression of the new GPT 4.5, as described in a Confluence post. GPT 4.5 correctly answered a broad and vague prompt that other models could not. This might not seem remarkable, but the authors noted: “This insignificant exchange was the first conversation with an LLM where we walked away thinking, ‘Now that feels like general intelligence.’” Did OpenAI just cross a threshold with GPT 4.5?\nTipping points\nWhile software engineering may be among the first knowledge-worker professions to face widespread AI automation, it will not be the last. Many other white-collar jobs covering research, customer service and financial analysis are similarly exposed to AI-driven disruption. \nWhat might prompt a sudden shift in workplace adoption of AI? History shows that economic recessions often accelerate technological adoption, and the next downturn may be the tipping point when AI’s impact on jobs shifts from gradual to sudden. \nDuring economic downturns, businesses face pressure to cut costs and improve efficiency, making automation more attractive. Labor becomes more expensive compared to technology investments, especially when companies need to do more with fewer human resources. This phenomenon is sometimes called “forced productivity.” As an example, the Great Recession of 2007 to 2009 saw significant advances in automation, cloud computing and digital platforms.\nIf a recession materializes in 2025 or 2026, companies facing pressure to reduce headcount may well turn to AI technologies, particularly tools and processes based on LLMs, as a strategy to support efficiency and productivity with fewer people. This could be even more pronounced — and more sudden — given business worries about falling behind in AI adoption.\nWill there be a recession in 2025?\nIt is always difficult to tell when a recession will occur. J.P. Morgan’s chief economist recently estimated a 40% chance. Former Treasury Secretary Larry Summers said it could be around 50%. The betting markets are aligned with these views, predicting a greater than 40% probability that a recession will occur in 2025. \nSource: Polymarket\nIf a recession does occur later in 2025, it could indeed be characterized as an “AI recession.” However, AI itself will not be the cause. Instead, economic necessity could force companies to accelerate automation decisions. This would not be a technological inevitability, but a strategic response to financial pressure.   \nThe extent of AI’s impact will depend on several factors, including the pace of technological sophistication and integration, the effectiveness of workforce retraining programs and the adaptability of businesses and employees to an evolving landscape. \nWhenever it occurs, the next recession may not just lead to temporary job losses. Companies that have been experimenting with AI or adopting it in limited deployments may suddenly find automation not optional, but essential for survival. If such a scenario happens, it may signal a permanent shift toward a more AI-driven workforce. \nAs Salesforce CEO Marc Benioff put it in a recent earnings call: “We’re the last generation of CEOs to only manage humans. Every CEO going forward is going to manage humans and agents together. I know that’s what I’m doing. … You can see it also in the global economy. I think productivity is going to rise without additions to more human labor, which is good because human labor is not increasing in the global workforce.”\nMany of history’s biggest technological shifts have coincided with economic downturns. AI may be next. The only question left is: Will 2025 be the year AI not only augments jobs but begins to replace them?\nGradually, then suddenly.\nGary Grossman is EVP of technology practice at Edelman and global lead of the Edelman AI Center of Excellence. \nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "The open-source AI debate: Why selective transparency poses a serious risk",
        "url": "https://venturebeat.com/ai/the-open-source-ai-debate-why-selective-transparency-poses-a-serious-risk/",
        "content": "As tech giants declare their AI releases open — and even put the word in their names — the once insider term “open source” has burst into the modern zeitgeist. During this precarious time in which one company’s misstep could set back the public’s comfort with AI by a decade or more, the concepts of openness and transparency are being wielded haphazardly, and sometimes dishonestly, to breed trust. \nAt the same time, with the new White House administration taking a more hands-off approach to tech regulation, the battle lines have been drawn — pitting innovation against regulation and predicting dire consequences if the “wrong” side prevails. \nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nThere is, however, a third way that has been tested and proven through other waves of technological change. Grounded in the principles of openness and transparency, true open source collaboration unlocks faster rates of innovation even as it empowers the industry to develop technology that is unbiased, ethical and beneficial to society. \nUnderstanding the power of true open source collaboration\nPut simply, open-source software features freely available source code that can be viewed, modified, dissected, adopted and shared for commercial and noncommercial purposes — and historically, it has been monumental in breeding innovation. Open-source offerings Linux, Apache, MySQL and PHP, for example, unleashed the internet as we know it. \nNow, by democratizing access to AI models, data, parameters and open-source AI tools, the community can once again unleash faster innovation instead of continually recreating the wheel — which is why a recent IBM study of 2,400 IT decision-makers revealed a growing interest in using open-source AI tools to drive ROI. While faster development and innovation were at the top of the list when it came to determining ROI in AI, the research also confirmed that embracing open solutions may correlate to greater financial viability.\nInstead of short-term gains that favor fewer companies, open-source AI invites the creation of more diverse and tailored applications across industries and domains that might not otherwise have the resources for proprietary models. \nPerhaps as importantly, the transparency of open source allows for independent scrutiny and auditing of AI systems’ behaviors and ethics — and when we leverage the existing interest and drive of the masses, they will find the problems and mistakes as they did with the LAION 5B dataset fiasco. \nADVERTISEMENT\nIn that case, the crowd rooted out more than 1,000 URLs containing verified child sexual abuse material hidden in the data that fuels generative AI models like Stable Diffusion and Midjourney — which produce images from text and image prompts and are foundational in many online video-generating tools and apps. \nWhile this finding caused an uproar, if that dataset had been closed, as with OpenAI’s Sora or Google’s Gemini, the consequences could have been far worse. It’s hard to imagine the backlash that would ensue if AI’s most exciting video creation tools started churning out disturbing content.\nThankfully, the open nature of the LAION 5B dataset empowered the community to motivate its creators to partner with industry watchdogs to find a fix and release RE-LAION 5B — which exemplifies why the transparency of true open-source AI not only benefits users, but the industry and creators who are working to build trust with consumers and the general public. \nThe danger of open sourcery in AI\nADVERTISEMENT\nWhile source code alone is relatively easy to share, AI systems are far more complicated than software. They rely on system source code, as well as the model parameters, dataset, hyperparameters, training source code, random number generation and software frameworks — and each of these components must work in concert for an AI system to work properly.\nAmid concerns around safety in AI, it has become commonplace to state that a release is open or open source. For this to be accurate, however, innovators must share all the pieces of the puzzle so that other players can fully understand, analyze and assess the AI system’s properties to ultimately reproduce, modify and extend its capabilities. \nMeta, for example, touted Llama 3.1 405B as “the first frontier-level open-source AI model,” but only publicly shared the system’s pre-trained parameters, or weights, and a bit of software. While this allows users to download and use the model at will, key components like the source code and dataset remain closed — which becomes more troubling in the wake of the announcement that Meta will inject AI bot profiles into the ether even as it stops vetting content for accuracy. \nTo be fair, what is being shared certainly contributes to the community. Open weight models offer flexibility, accessibility, innovation and a level of transparency. DeepSeek’s decision to open source its weights, release its technical reports for R1 and make it free to use, for example, has enabled the AI community to study and verify its methodology and weave it into their work. \nIt is misleading, however, to call an AI system open source when no one can actually look at, experiment with and understand each piece of the puzzle that went into creating it.\nThis misdirection does more than threaten public trust. Instead of empowering everyone in the community to collaborate, build and advance upon models like Llama X, it forces innovators using such AI systems to blindly trust the components that are not shared.\nEmbracing the challenge before us\nAs self-driving cars take to the streets in major cities and AI systems assist surgeons in the operating room, we are only at the beginning of letting this technology take the proverbial wheel. The promise is immense, as is the potential for error — which is why we need new measures of what it means to be trustworthy in the world of AI.\nEven as Anka Reuel and colleagues at Stanford University recently attempted to set up a new framework for the AI benchmarks used to assess how well models perform, for example, the review practice the industry and the public rely on is not yet sufficient. Benchmarking fails to account for the fact that datasets at the core of learning systems are constantly changing and that appropriate metrics vary from use case to use case. The field also still lacks a rich mathematical language to describe the capabilities and limitations in contemporary AI. \nBy sharing entire AI systems to enable openness and transparency instead of relying on insufficient reviews and paying lip service to buzzwords, we can foster greater collaboration and cultivate innovation with safe and ethically developed AI. \nWhile true open-source AI offers a proven framework for achieving these goals, there’s a concerning lack of transparency in the industry. Without bold leadership and cooperation from tech companies to self-govern, this information gap could hurt public trust and acceptance. Embracing openness, transparency and open source is not just a strong business model — it’s also about choosing between an AI future that benefits everyone instead of just the few. \nJason Corso is a professor at the University of Michigan and co-founder of Voxel51.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Less is more: UC Berkeley and Google unlock LLM potential through simple sampling",
        "url": "https://venturebeat.com/ai/less-is-more-uc-berkeley-and-google-unlock-llm-potential-through-simple-sampling/",
        "content": "A new paper by researchers from Google Research and the University of California, Berkeley, demonstrates that a surprisingly simple test-time scaling approach can boost the reasoning abilities of large language models (LLMs). The key? Scaling up sampling-based search, a technique that relies on generating multiple responses and using the model itself to verify them. \nThe core finding is that even a minimalist implementation of sampling-based search, using random sampling and self-verification, can elevate the reasoning performance of models like Gemini 1.5 Pro beyond that of o1-Preview on popular benchmarks. The findings can have important implications for enterprise applications and challenge the assumption that highly specialized training or complex architectures are always necessary for achieving top-tier performance.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nThe limits of current test-time compute scaling\nThe current popular method for test-time scaling in LLMs is to train the model through reinforcement learning to generate longer responses with chain-of-thought (CoT) traces. This approach is used in models such as OpenAI o1 and DeepSeek-R1. While beneficial, these methods usually require substantial investment in the training phase.\nAnother test-time scaling method is “self-consistency,” where the model generates multiple responses to the query and chooses the answer that appears more often. Self-consistency reaches its limits when handling complex problems, as in these cases, the most repeated answer is not necessarily the correct one.\nSampling-based search offers a simpler and highly scalable alternative to test-time scaling: Let the model generate multiple responses and select the best one through a verification mechanism. Sampling-based search can complement other test-time compute scaling strategies and, as the researchers write in their paper, “it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses.”\nMore importantly, sampling-based search can be applied to any LLM, including those that have not been explicitly trained for reasoning.\nHow sampling-based search works\nThe researchers focus on a minimalist implementation of sampling-based search, using a language model to both generate candidate responses and verify them. This is a “self-verification” process, where the model assesses its own outputs without relying on external ground-truth answers or symbolic verification systems.\nSearch-based sampling Credit: VentureBeat\nADVERTISEMENT\nThe algorithm works in a few simple steps: \n1—The algorithm begins by generating a set of candidate solutions to the given problem using a language model. This is done by giving the model the same prompt multiple times and using a non-zero temperature setting to create a diverse set of responses.\n2—Each candidate’s response undergoes a verification process in which the LLM is prompted multiple times to determine whether the response is correct. The verification outcomes are then averaged to create a final verification score for the response.\nADVERTISEMENT\n3— The algorithm selects the highest-scored response as the final answer. If multiple candidates are within close range of each other, the LLM is prompted to compare them pairwise and choose the best one. The response that wins the most pairwise comparisons is chosen as the final answer.\nThe researchers considered two key axes for test-time scaling:\nSampling: The number of responses the model generates for each input problem.\nVerification: The number of verification scores computed for each generated solution\nHow sampling-based search compares to other techniques\nThe study revealed that reasoning performance continues to improve with sampling-based search, even when test-time compute is scaled far beyond the point where self-consistency saturates. \nAt a sufficient scale, this minimalist implementation significantly boosts reasoning accuracy on reasoning benchmarks like AIME and MATH. For example, Gemini 1.5 Pro’s performance surpassed that of o1-Preview, which has explicitly been trained on reasoning problems, and Gemini 1.5 Flash surpassed Gemini 1.5 Pro.\n“This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models’ search capabilities,” the researchers write.\nIt is worth noting that while the results of search-based sampling are impressive, the costs can also become prohibitive. For example, with 200 samples and 50 verification steps per sample, a query from AIME will generate around 130 million tokens, which costs $650 with Gemini 1.5 Pro. However, this is a very minimalistic approach to sampling-based search, and it is compatible with optimization techniques proposed in other studies. With smarter sampling and verification methods, the inference costs can be reduced considerably by using smaller models and generating fewer tokens. For example, by using Gemini 1.5 Flash to perform the verification, the costs drop to $12 per question.\nEffective self-verification strategies\nThere is an ongoing debate on whether LLMs can verify their own answers. The researchers identified two key strategies for improving self-verification using test-time compute:\nDirectly comparing response candidates: Disagreements between candidate solutions strongly indicate potential errors. By providing the verifier with multiple responses to compare, the model can better identify mistakes and hallucinations, addressing a core weakness of LLMs. The researchers describe this as an instance of “implicit scaling.”\nTask-specific rewriting: The researchers propose that the optimal output style of an LLM depends on the task. Chain-of-thought is effective for solving reasoning tasks, but responses are easier to verify when written in a more formal, mathematically conventional style. Verifiers can rewrite candidate responses into a more structured format (e.g., theorem-lemma-proof) before evaluation.\n“We anticipate model self-verification capabilities to rapidly improve in the short term, as models learn to leverage the principles of implicit scaling and output style suitability, and drive improved scaling rates for sampling-based search,” the researchers write.\nImplications for real-world applications\nThe study demonstrates that a relatively simple technique can achieve impressive results, potentially reducing the need for complex and costly model architectures or training regimes.\nThis is also a scalable technique, enabling enterprises to increase performance by allocating more compute resources to sampling and verification. It also enables developers to push frontier language models beyond their limitations on complex tasks.\n“Given that it complements other test-time compute scaling strategies, is parallelizable and allows for arbitrarily scaling, and admits simple implementations that are demonstrably effective, we expect sampling-based search to play a crucial role as language models are tasked with solving increasingly complex problems with increasingly large compute budgets,” the researchers write. \nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Small models as paralegals: LexisNexis distills models to build AI assistant",
        "url": "https://venturebeat.com/ai/small-models-as-paralegals-lexisnexis-distills-models-to-build-ai-assistant/",
        "content": "When legal research company LexisNexis created its AI assistant Protégé, it wanted to figure out the best way to leverage its expertise without deploying a large model. \nProtégé aims to help lawyers, associates and paralegals write and proof legal documents and ensure that anything they cite in complaints and briefs is accurate. However, LexisNexis didn’t want a general legal AI assistant; they wanted to build one that learns a firm’s workflow and is more customizable. \nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nLexisNexis saw the opportunity to bring the power of large language models (LLMs) from Anthropic and Mistral and find the best models that answer user questions the best, Jeff Reihl, CTO of LexisNexis Legal and Professional, told VentureBeat.\n“We use the best model for the specific use case as part of our multi-model approach. We use the model that provides the best result with the fastest response time,” Reihl said. “For some use cases, that will be a small language model like Mistral or we perform distillation to improve performance and reduce cost.”\nWhile LLMs still provide value in building AI applications, some organizations turn to using small language models (SLMs) or distilling LLMs to become small versions of the same model. \nDistillation, where an LLM “teaches” a smaller model, has become a popular method for many organizations. \nSmall models often work best for apps like chatbots or simple code completion, which is what LexisNexis wanted to use for Protégé. \nThis is not the first time LexisNexis built AI applications, even before launching its legal research hub LexisNexis + AI in July 2024.\n“We have used a lot of AI in the past, which was more around natural language processing, some deep learning and machine learning,” Reihl said. “That really changed in November 2022 when ChatGPT was launched, because prior to that, a lot of the AI capabilities were kind of behind the scenes. But once ChatGPT came out, the generative capabilities, the conversational capabilities of it was very, very intriguing to us.”\nSmall, fine-tuned models and model routing \nReihl said LexisNexis uses different models from most of the major model providers when building its AI platforms. LexisNexis + AI used Claude models from Anthropic, OpenAI’s GPT models and a model from Mistral. \nThis multimodal approach helped break down each task users wanted to perform on the platform. To do this, LexisNexis had to architect its platform to switch between models. \nADVERTISEMENT\n“We would break down whatever task was being performed into individual components, and then we would identify the best large language model to support that component. One example of that is we will use Mistral to assess the query that the user entered in,” Reihl said. \nFor Protégé, the company wanted faster response times and models more fine-tuned for legal use cases. So it turned to what Reihl calls “fine-tuned” versions of models, essentially smaller weight versions of LLMs or distilled models. \n“You don’t need GPT-4o to do the assessment of a query, so we use it for more sophisticated work, and we switch models out,” he said. \nADVERTISEMENT\nWhen a user asks Protégé a question about a specific case, the first model it pings is a fine-tuned Mistral “for assessing the query, then determining what the purpose and intent of that query is” before switching to the model best suited to complete the task. Reihl said the next model could be an LLM that generates new queries for the search engine or another model that summarizes results. \nRight now, LexisNexis mostly relies on a fine-tuned Mistral model though Reihl said it used a fine-tuned version of Claude “when it first came out; we are not using it in the product today but in other ways.” LexisNexis is also interested in using other OpenAI models especially since the company came out with new reinforcement fine-tuning capabilities last year. LexisNexis is in the process of evaluating OpenAI’s reasoning models including o3 for its platforms. \nReihl added that it may also look at using Gemini models from Google. \nLexisNexis backs all of its AI platforms with its own knowledge graph to perform retrieval augmented generation (RAG) capabilities, especially as Protégé could help launch agentic processes later. \nThe AI legal suite\nEven before the advent of generative AI, LexisNexis tested the possibility of putting chatbots to work in the legal industry. In 2017, the company tested an AI assistant that would compete with IBM’s Watson-powered Ross and Protégé sits in the company’s LexisNexis + AI platform, which brings together the AI services of LexisNexis. \nProtégé helps law firms with tasks that paralegals or associates tend to do. It helps write legal briefs and complaints that are grounded in firms’ documents and data, suggest legal workflow next steps, suggest new prompts to refine searches, draft questions for depositions and discovery, link quotes in filings for accuracy, generate timelines and, of course, summarize complex legal documents. \n“We see Protégé as the initial step in personalization and agentic capabilities,” Reihl said. “Think about the different types of lawyers: M&A, litigators, real estate. It’s going to continue to get more and more personalized based on the specific task you do. Our vision is that every legal professional will have a personal assistant to help them do their job based on what they do, not what other lawyers do.”\nProtégé now competes against other legal research and technology platforms. Thomson Reuters customized OpenAI’s o1-mini-model for its CoCounsel legal assistant. Harvey, which raised $300 million from investors including LexisNexis, also has a legal AI assistant. \nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "OpenAI’s new voice AI model gpt-4o-transcribe lets you add speech to your existing text apps in seconds",
        "url": "https://venturebeat.com/ai/openais-new-voice-ai-models-gpt-4o-transcribe-let-you-add-speech-to-your-existing-text-apps-in-seconds/",
        "content": "OpenAI‘s voice AI models have gotten it into trouble before with actor Scarlett Johansson, but that isn’t stopping the company from continuing to advance its offerings in this category.\nToday, the ChatGPT maker has unveiled three new proprietary voice models: gpt-4o-transcribe, gpt-4o-mini-transcribe and gpt-4o-mini-tts. These models will initially be available through the ChatGPT maker’s application programming interface (API) for third-party software developers to build their own apps. They will also be available on a custom demo site, OpenAI.fm, that individual users can access for limited testing and fun.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nMoreover, the gpt-4o-mini-tts model voices can be customized from several pre-sets via text prompt to change their accents, pitch, tone and other vocal qualities — including conveying whatever emotions the user asks them to, which should go a long way to addressing any concerns OpenAI is deliberately imitating any particular user’s voice (the company previously denied that was the case with Johansson, but pulled down the ostensibly imitative voice option, anyway). Now, it’s up to the user to decide how they want their AI voice to sound when speaking back.\nIn a demo with VentureBeat delivered over a video call, OpenAI technical staff member Jeff Harris showed how, using text alone on the demo site, a user could get the same voice to sound like a cackling mad scientist or a zen, calm yoga teacher.\nDiscovering and refining new capabilities within GPT-4o base\nThe models are variants of the existing GPT-4o model OpenAI launched back in May 2024 and which currently powers the ChatGPT text and voice experience for many users, but the company took that base model and post-trained it with additional data to make it excel at transcription and speech. The company didn’t specify when the models might come to ChatGPT.\nADVERTISEMENT\n“ChatGPT has slightly different requirements in terms of cost and performance trade-offs, so while I expect they will move to these models in time, for now, this launch is focused on API users,” Harris said.\nIt is meant to supersede OpenAI’s two-year-old Whisper open-source text-to-speech model, offering lower word error rates across industry benchmarks and improved performance in noisy environments, with diverse accents, and at varying speech speeds across 100+ languages.\nThe company posted a chart on its website showing just how much lower the gpt-4o-transcribe models’ error rates are at identifying words across 33 languages compared to Whisper — with an impressively low 2.46% in English.\nADVERTISEMENT\n“These models include noise cancellation and a semantic voice activity detector, which helps determine when a speaker has finished a thought, improving transcription accuracy,” said Harris.\nHarris told VentureBeat that the new gpt-4o-transcribe model family is not designed to offer “diarization,” or the capability to label and differentiate between different speakers. Instead, it is designed primarily to receive one (or possibly multiple voices) as a single input channel and respond to all inputs with a single output voice in that interaction, however long it takes.\nThe company is also hosting a competition for the general public to find the most creative examples of using its demo voice site OpenAI.fm and share them online by tagging the @openAI account on X. The winner will receive a custom Teenage Engineering radio with the OpenAI logo, which OpenAI Head of Product, Platform Olivier Godement said is one of only three in the world.\nAn audio applications gold mine\nThe enhancements make them particularly well-suited for applications such as customer call centers, meeting note transcription, and AI-powered assistants.\nImpressively, the company’s newly launched Agents SDK from last week also allows those developers who have already built apps atop its text-based large language models like the regular GPT-4o to add fluid voice interactions with only about “nine lines of code,” according to a presenter during an OpenAI YouTube livestream announcing the new models (embedded above).\nFor example, an e-commerce app built atop GPT-4o could now respond to turn-based user questions like “Tell me about my last orders” in speech with just seconds of tweaking the code by adding these new models.\n“For the first time, we’re introducing streaming speech-to-text, allowing developers to continuously input audio and receive a real-time text stream, making conversations feel more natural,” Harris said.\nStill, for those devs looking for low-latency, real-time AI voice experiences, OpenAI recommends using its speech-to-speech models in the Realtime API.\nPricing and availability\nThe new models are available immediately via OpenAI’s API, with pricing as follows:\n• gpt-4o-transcribe: $6.00 per 1M audio input tokens (~$0.006 per minute)\n• gpt-4o-mini-transcribe: $3.00 per 1M audio input tokens (~$0.003 per minute)\n• gpt-4o-mini-tts: $0.60 per 1M text input tokens, $12.00 per 1M audio output tokens (~$0.015 per minute)\nHowever, they arrive at a time of fiercer-than-ever competition in the AI transcription and speech space, with dedicated speech AI firms such as ElevenLabs offering their new Scribe model, which supports diarization and boasts a similarly (but not as low) reduced error rate of 3.3% in English. It is priced at $0.40 per hour of input audio (or $0.006 per minute, roughly equivalent).\nAnother startup, Hume AI, offers a new model, Octave TTS, with sentence-level and even word-level customization of pronunciation and emotional inflection — based entirely on the user’s instructions, not any pre-set voices. The pricing of Octave TTS isn’t directly comparable, but there is a free tier offering 10 minutes of audio and costs increase from there between\nMeanwhile, more advanced audio and speech models are also coming to the open source community, including one called Orpheus 3B which is available with a permissive Apache 2.0 license, meaning developers don’t have to pay any costs to run it — provided they have the right hardware or cloud servers.\nIndustry adoption and early results\nAccording to testimonials shared by OpenAI with VentureBeat, several companies have already integrated OpenAI’s new audio models into their platforms, reporting significant improvements in voice AI performance.\nEliseAI, a company focused on property management automation, found that OpenAI’s text-to-speech model enabled more natural and emotionally rich interactions with tenants.\nThe enhanced voices made AI-powered leasing, maintenance, and tour scheduling more engaging, leading to higher tenant satisfaction and improved call resolution rates.\nDecagon, which builds AI-powered voice experiences, saw a 30% improvement in transcription accuracy using OpenAI’s speech recognition model.\nThis increase in accuracy has allowed Decagon’s AI agents to perform more reliably in real-world scenarios, even in noisy environments. The integration process was quick, with Decagon incorporating the new model into its system within a day.\nNot all reactions to OpenAI’s latest release have been warm. Dawn AI app analytics software co-founder Ben Hylak (@benhylak), a former Apple human interfaces designer, posted on X that while the models seem promising, the announcement “feels like a retreat from real-time voice,” suggesting a shift away from OpenAI’s previous focus on low-latency conversational AI via ChatGPT.\nAdditionally, the launch was preceded by an early leak on X (formerly Twitter). TestingCatalog News (@testingcatalog) posted details on the new models several minutes before the official announcement, listing the names of gpt-4o-mini-tts, gpt-4o-transcribe, and gpt-4o-mini-transcribe. The leak was credited to @StivenTheDev, and the post quickly gained traction.\nHowever, looking ahead, OpenAI plans to continue refining its audio models and exploring custom voice capabilities while ensuring safety and responsible AI use. Beyond audio, OpenAI is also investing in multimodal AI, including video, to enable more dynamic and interactive agent-based experiences.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Speed is King: How Google’s $32B Wiz play rewrites DevOps security rules",
        "url": "https://venturebeat.com/security/googles-32b-wiz-bet-ai-driven-cnapp-will-finally-eliminate-devsecops-bottlenecks/",
        "content": "The real story: Speed and Security at DevOps scale\nThe real story behind Google acquiring Wiz is how badly the need for speed dominates every enterprise’s DevOps cycles building apps, models and platforms without sacrificing security.\nBy acquiring Wiz, Google gets an AI-infused Cloud Native Application Protection Platform (CNAPP) designed to eliminate DevSecOps bottlenecks, prevent attacks by and on models in development, prevent cloud breaches and scale multi-cloud security in real time. The Wiz CNAPP platform has earned a global reputation by using AI to enhance its threat detection, predictive analytics, automated remediation, and reduction of false positives.  \nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nWiz will integrate Google’s risk detection, threat intelligence and automated remediation, which all are table stakes for protecting every stage of cloud-based app and model development. That’s a solid contribution to Wiz’s graph-based security engine designed to find and contain attack paths instantly, prioritize actual risks and help security teams identify and fix vulnerabilities before they’re exploited.\nGoogle paying $32 billion in cash signals just how urgent the need for speed is across DevOps cycles that have been asking for an AI-driven CNAPP platform that can flex and scale to keep up with more complex DevOps cycles.\n“While Google Cloud Platform (GCP) has been investing in built-in CNAPP capabilities for their own platform’s native security with success, these tools have predominantly focused only on protecting GCP endpoints/assets,” says Andras Cser, VP and Principal Analyst at Forrester.\nCser added, “after Microsoft’s 2021 early acquisition of CloudKnox and development of Defender for Cloud, Google is feeling the pressure to offer a true, multicloud-capable CNAPP tool given that so many organizations are multi-cloud today. Forrester expects that, post-acquisition, most current CNAPP capabilities in GCP (CSPM, CIEM, agentless CWP) will be replaced by Wiz’s offering and remain with multi-cloud support.”\nGoogle just made CNAPP the Formula 1 of Cloud Security  \nIn professional racing, as in DevOps, teams obsess over squeezing the last ounce of speed gains out of their engines or code. Knowing that just a few milliseconds gained by reducing the drag on a Formula 1 car or making slight engine improvements mean the difference between a winning season or not.\nADVERTISEMENT\nCNAPP is one of the engines DevOps and DevSecOps teams rely on to reduce risks, block intrusions and breaches, and provide a 360 view of CI/CD pipelines to make sure they are secure. Having a CNAPP that’s AI-driven delivers more accurate remediation and guidance, contextual threat intelligence and blocks intrusion attempts on CI/CD pipelines protecting code.\n“While Wiz is most focused on CNAPP, the firm’s product offerings bleed into the traditional application security space, with container and Kubernetes security pieces. Recently Wiz expanded into security in the software development phases with software composition analysis (SCA), IAC scanning, and secrets scanning, as well as diving into the software supply chain use case with software bill of materials (SBOM) and CI/CD security posture. These are moves that put Wiz in a position to compete with application security testing vendors and other CNAPP vendors who have ‘shifted left,” explained Forrester Senior Analyst Janet Worthington.\nADVERTISEMENT\nDevOps teams are under constant, growing pressure to deliver. With bonuses often riding on if a delivery date for code is met, security is tacked on to the end of a CI/CD cycle or product schedule. VentureBeat learned that the typical Fortune 1,000 IT department has over 175 active, concurrent DevOps projects running at once, with many having no consistent cloud application security. In other words, those 175 projects are running in a variety of unprotected cloud environments without a common CNAPP platform to protect them. That’s jeopardizing the entire DevOps pipeline which is a move made to reduce time-to-market that leaves dozens of projects at risk.\nWhy Google doubled down on Wiz\nGoogle’s ambitions to grow Google Cloud Platform (GCP) needed a cybersecurity platform that could go end-to-end, protect DevOps and strengthen DevSecOps while leveraging AI to deliver real-time threat detection, automated remediation and full-stack cloud security.\nThe real goal of this acquisition is to have a unified CNAPP solution capable of securing everything from code to cloud to runtime, ensuring that security no longer slows down development but accelerates it. Wiz’s AI-driven risk analysis, attack path visualization and multi-cloud security give GCP a competitive edge, making it a viable competitor in an increasingly crowded market driven by enterprises needing speed, scale and resilience in cloud security.\nThis diagram visually explains how CNAPP integrates security into the entire DevSecOps lifecycle, one of Google’s key motivations in acquiring Wiz to attain an end-to-end, AI-driven security platform. Source: Gartner, 5 Ways CNAPP Will Improve Your Cloud Security, Sept. 21, 2023.\n“Google has invested heavily in application security tooling that protects apps deployed not only in GCP but in other clouds (and on-premises). Google’s investment in its Cloud Armor platform has added web application firewall functionality that is competitive not just with Microsoft and AWS but with other WAF providers. reCaptcha Enterprise has expanded from a Captcha provider into a fuller bot management platform that addresses a range of business logic attacks,” says Forrester Principal Analyst Sandy Carielli.\n“In recent months, Google has begun extending its API management product, Apigee, into broader API security use cases. While there are still gaps to fill, adding Wiz to the combined Cloud Armor, reCaptcha, and Apigee offerings moves Google closer to a holistic defense story for cloud applications,” Carielli continued.  \nGoogle needed a unified AI-driven CNAPP to turbocharge its cybersecurity business. One that brings together security posture management, workload protection, advanced threat detection into a high performance security engine. Challenged by having a siloed approach to security in the past, Google is looking to now have a adaptive, flexible platform that can provide security at the speed of cloud app development.\nPrior to this deal, GCP’s security toolkit was strong, yet siloed as evidenced by its Chronicle SIEM, Mandiant threat intel and a wide variety of partner solutions that created roadblocks across customers’ CI/CD pipeline. Acquiring Wiz closes a major gap in their cybersecurity strategy by providing an integrated AI-driven platform that scans cloud environments in minutes and identify risks in real time.\nCNAPP has a fast track with AI savvy competitors\nThe global CNAPP market was valued at approximately $9.79 billion in 2023 and is projected to reach $38.01 billion by 2030, growing at a compound annual growth rate (CAGR) of about 21.8% during the forecast period. Gartner notes that end-user calls on CNAPPs rose 29% from 2023 to 2024, with an emphasis on Cloud Security Posture Management (CSPM) driven by compliance and easy API deployment, with expectations of runtime visibility and control.\n“Wiz’s key detection and response offering Wiz Defend takes a different approach to cloud detection and response. Instead of relying on built-in detection capabilities in its own cloud protection tools, it offers a unified tool solely for detection and response that takes in alerts and data from other tools and does detection engineering on them,” says Forrester Principal Analyst Allie Mellen.\n“This reduces alert volumes from the cloud at a critical time. With this acquisition, it will put pressure on other vendors to consolidate in a similar way — a big win for security operations teams,” Mellen continued.\nThe CNAPP market is increasingly becoming the Formula 1 of cloud security, with Google, Microsoft, Palo Alto Networks, CrowdStrike and Check Point leading the charge.\nCheck Point CloudGuard: A CNAPP solution designed for multi-cloud security, runtime protection and automated compliance enforcement. CloudGuard’s agent-based and agentless security helps protect workloads, Kubernetes environments, and serverless applications.\nCrowdStrike Falcon Cloud Security: Expanding from endpoint security to cloud, CrowdStrike brings its threat intelligence leadership into CNAPP. Falcon Cloud Security provides code-to-cloud visibility, IaC scanning, and runtime threat detection, reinforcing proactive breach prevention.\nMicrosoft Defender for Cloud: A deeply integrated CNAPP that extends across Azure, AWS, and GCP, offering runtime protection, identity security, and AI-driven threat intelligence. With Security Copilot, Microsoft is leveraging generative AI to automate threat detection and remediation.\nOther CNAPP vendors in the market include Aqua Security, Lacework, Orca Security, Palo Alto Networks, SentinelOne, Sysdig and Trend Micro all offering solutions for cloud security, workload protection and posture management.\nGartner ranks CNAPP vendors based on customer feedback, providing a data-driven comparison of how enterprises perceive the leaders in this market. Source: Gartner, Voice of the Customer for Cloud-Native Application Protection Platforms, Dec. 27, 2024\nThe AI-enabled CNAPP race is just beginning\nGoogle’s decision to make their single largest acquisition in its history says they see the pain of siloed slow processes in enterprises they can quickly turn into a profitable new part of their cybersecurity business. CNAPP is the racing engine their prosects and current customers are looking for.  \nFor CISOs and security leaders, the key takeaway is clear: the future of cloud security belongs to platforms that integrate AI, automate risk detection, and provide full-stack visibility across multi-cloud environments. Whether Google’s Wiz-powered CNAPP takes the lead will depend on how well it integrates with Google’s AI-driven threat intelligence and security operations suite.\nBottom line: Enterprises need AI-powered CNAPP solutions to streamline CI/CD security and reduce the cloud security burden on DevOps teams. The competition among vendors—led by Google’s Wiz-powered push—will be won by those who best integrate AI, automate risk detection, and provide full-stack visibility across multi-cloud environments.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Anthropic just gave Claude a superpower: real-time web search. Here’s why it changes everything",
        "url": "https://venturebeat.com/ai/anthropic-just-gave-claude-a-superpower-real-time-web-search-heres-why-it-changes-everything/",
        "content": "Anthropic announced today that its AI assistant Claude can now search and process information from the internet in real-time, addressing one of users’ most requested features and closing a critical competitive gap with OpenAI’s ChatGPT.\nThe new web search capability, available immediately for paid Claude users in the United States, transforms the AI assistant from a tool limited by its training data cutoff to one that can access and synthesize the latest information across the web.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\n“With web search, Claude has access to the latest events and information, boosting its accuracy on tasks that benefit from the most recent data,” Anthropic said in its announcement. The company emphasized that Claude will provide direct citations to sources, allowing users to fact-check information— a direct response to growing concerns about AI hallucinations and misinformation.\nAI arms race intensifies as Anthropic secures billions in funding\nThis launch comes at an important moment in the rapidly evolving AI sector. Just three weeks ago, Anthropic secured $3.5 billion in Series E funding at a post-money valuation of $61.5 billion, underscoring the high stakes in the AI race. Major backers include Lightspeed Venture Partners, Google (which holds a 14% stake) and Amazon, which has integrated Claude into its Alexa+ service.\nThe web search rollout also follows Anthropic’s recent release of Claude 3.7 Sonnet, which the company claims has set “a new high-water mark in coding abilities.” This focus on programming proficiency appears strategic, especially in light of CEO Dario Amodei’s recent prediction at a Council on Foreign Relations event that “in three to six months, AI will be writing 90% of the code” that software developers currently produce.\nThe timing of this feature launch reveals Anthropic’s determination to challenge OpenAI’s dominance in the consumer AI assistant market. While Claude has gained popularity among technical users for its nuanced reasoning and longer context window, the lack of real-time information access has been a significant handicap in head-to-head comparisons with ChatGPT. This update effectively neutralizes that disadvantage.\nHow Claude’s web search transforms enterprise decision-making\nADVERTISEMENT\nUnlike traditional search engines that return a list of links, Claude processes search results and delivers them in a conversational format. Users simply toggle on web search in their profile settings, and Claude will automatically search the internet when needed to inform its responses.\nAnthropic highlighted several business use cases for the web-enabled Claude: sales teams analyzing industry trends, financial analysts assessing current market data, researchers building grant proposals and shoppers comparing products across multiple sources.\nThis feature fundamentally changes how enterprise users can interact with AI assistants. Previously, professionals needed to toggle between search engines and AI tools, manually feeding information from one to the other. Claude’s integrated approach streamlines this workflow dramatically, potentially saving hours of research time for knowledge workers.\nADVERTISEMENT\nFor financial services firms in particular, the ability to combine historical training data with breaking news creates a powerful analysis tool that could provide genuine competitive advantages. Investment decisions often hinge on connecting disparate pieces of information quickly — exactly the kind of task this integration aims to solve.\nBehind the scenes: The technical infrastructure powering Claude’s new capabilities\nBehind this seemingly straightforward feature lies considerable technical complexity. Anthropic has likely spent months fine-tuning Claude’s ability to search effectively, understand context and determine when web search would improve its responses.\nThe update integrates with other recent technical improvements to the Anthropic API, including cache-aware rate limits, simpler prompt caching, and token-efficient tool use. These enhancements, announced earlier this month, aim to help developers process more requests while reducing costs. For certain applications, these enhancements can reduce token usage by up to 90%.\nAnthropic has also upgraded its developer console to enable collaboration among teams working on AI implementations. The revised console allows developers to share prompts, collaborate on refinements and control extended thinking budgets — features particularly valuable for enterprise customers integrating Claude into their workflows.\nThe investment in these backend capabilities suggests Anthropic is building for scale, anticipating rapid adoption as more companies integrate AI into their operations. By focusing on developer experience alongside user-facing features, Anthropic is creating an ecosystem rather than just a product — a strategy that has served companies like Microsoft well in enterprise markets.\nVoice mode: Anthropic’s next frontier in natural AI interaction\nA web search may be just the beginning of Anthropic’s feature expansion. According to a recent report in the Financial Times, the company is developing voice capabilities for Claude, potentially transforming how users interact with the AI assistant.\nMike Krieger, Anthropic’s chief product officer, told the Financial Times that the company is working on experiences that would allow users to speak directly to Claude. “We are doing some work around how Claude for desktop evolves… if it is going to be operating your computer, a more natural user interface might be to [speak to it],” Krieger said.\nThe company has reportedly held discussions with Amazon and voice-focused AI startup ElevenLabs about potential partnerships, though no deals have been finalized.\nVoice interaction would represent a significant leap forward in making AI assistants more accessible and intuitive. The current text-based interaction model creates friction that voice could eliminate, potentially expanding Claude’s appeal beyond tech-savvy early adopters to a much broader user base.\nHow Anthropic’s safety-first approach shapes regulatory conversations\nAs Anthropic expands Claude’s capabilities, the company continues to emphasize its commitment to responsible AI development. In response to California Governor Gavin Newsom’s Working Group on AI Frontier Models draft report released earlier this week, Anthropic expressed support for “objective standards and evidence-based policy guidance,” particularly highlighting transparency as “a low-cost, high-impact means of growing the evidence base around a new technology.”\n“Many of the report’s recommendations already reflect industry best practices which Anthropic adheres to,” the company stated, noting its Responsible Scaling Policy that outlines how it assesses models for misuse and autonomy risks.\nThis focus on responsible development represents a core differentiator in Anthropic’s brand positioning since its founding in 2021, when Amodei and six colleagues left OpenAI to create an AI company with greater emphasis on safety.\nAnthropic’s approach to regulation appears more collaborative than defensive, positioning the company favorably with policymakers who are increasingly focused on AI oversight. By proactively addressing safety concerns and contributing constructively to regulatory frameworks, Anthropic may be able to shape rules in ways that align with its existing practices while potentially creating compliance hurdles for less cautious competitors.\nThe future of AI assistants: From chatbots to indispensable digital partners\nAdding web search to Claude represents more than just feature parity with competitors — it signals Anthropic’s ambition to create AI systems that can function as comprehensive digital assistants rather than specialized tools.\nThis development marks a significant evolution in AI assistants. First-generation large language models were essentially sophisticated autocomplete systems with impressive but limited capabilities. The integration of real-time information access, combined with Claude’s existing reasoning abilities, creates something qualitatively different: a system that can actively help solve complex problems using up-to-date information.\nClaude’s new capabilities offer compelling advantages for businesses investing in AI integration. Cognition, the maker of the AI software developer assistant Devin, has already leveraged Anthropic’s prompt caching to provide more context about codebases while reducing costs and latency, according to the company’s leadership.\nThe real potential of these systems goes far beyond simple information retrieval. By combining current data with deep contextual understanding, AI assistants like Claude could transform knowledge work by handling substantial portions of research, analysis, and content creation — freeing humans to focus on judgment, creativity, and interpersonal aspects of their roles.\nWhat web search means for Claude users today and tomorrow\nWeb search is available now for all paid Claude users in the United States, with support for free users and international expansion planned “soon,” according to the announcement. Users can activate the feature through their profile settings.\nAs competition in the AI assistant space intensifies, Anthropic’s deliberate approach to expanding Claude’s capabilities while maintaining its focus on safety and transparency suggests a long-term strategy focused on building user trust alongside technical advancement.\nThe race between AI companies is increasingly about balancing capability with reliability and trust. Features like web search with source citations serve both goals simultaneously, providing users with more powerful tools while maintaining transparency about information sources.\nWith Claude now able to tap into the internet’s vast resources while maintaining its characteristic nuanced reasoning, Anthropic has eliminated a key competitive disadvantage. More importantly, the company has taken a significant step toward creating AI systems that don’t just respond to queries but actively help users navigate an increasingly complex information landscape.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Hugging Face submits open-source blueprint, challenging Big Tech in White House AI policy fight",
        "url": "https://venturebeat.com/ai/hugging-face-submits-open-source-blueprint-challenging-big-tech-in-white-house-ai-policy-fight/",
        "content": "In a Washington policy landscape increasingly dominated by calls for minimal AI regulation, Hugging Face is making a distinctly different case to the Trump administration: open-source and collaborative AI development may be America’s strongest competitive advantage.\nThe AI platform company, which hosts more than 1.5 million public models across diverse domains, has submitted its recommendations for the White House AI Action Plan, arguing that recent breakthroughs in open-source models demonstrate they can match or exceed the capabilities of closed commercial systems at a fraction of the cost.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nIn its official submission, Hugging Face highlights recent achievements like OlympicCoder, which outperforms Claude 3.7 on complex coding tasks using just 7 billion parameters, and AI2’s fully open OLMo 2 models that match OpenAI’s o1-mini performance levels.\nThe submission comes as part of a broader effort by the Trump administration to gather input for its upcoming AI Action Plan, mandated by Executive Order 14179, officially titled “Removing Barriers to American Leadership in Artificial Intelligence,” which was issued in January. The Order, which replaced the Biden administration’s more regulation-focused approach, emphasizes U.S. competitiveness and reducing regulatory barriers to development.\nHugging Face’s submission stands in stark contrast to those from commercial AI leaders like OpenAI, which has lobbied heavily for light-touch regulation and “the freedom to innovate in the national interest,” while warning about China’s narrowing lead in AI capabilities. OpenAI’s proposal emphasizes a “voluntary partnership between the federal government and the private sector” rather than what it calls “overly burdensome state laws.”\nHow open source could power America’s AI advantage: Hugging Face’s triple-threat strategy\nHugging Face’s recommendations center on three interconnected pillars that emphasize democratizing AI technology. The company argues that open approaches enhance rather than hinder America’s competitive position.\n“The most advanced AI systems to date all stand on a strong foundation of open research and open source software — which shows the critical value of continued support for openness in sustaining further progress,” the company wrote in its submission.\nADVERTISEMENT\nIts first pillar calls for strengthening open and open-source AI ecosystems through investments in research infrastructure like the National AI Research Resource (NAIRR) and ensuring broad access to trusted datasets. This approach contrasts with OpenAI’s emphasis on copyright exemptions that would allow proprietary models to train on copyrighted material without explicit permission.\n“Investment in systems that can freely be re-used and adapted has also been shown to have a strong economic impact multiplying effect, driving a significant percentage of countries’ GDP,” Hugging Face noted, arguing that open approaches boost rather than hinder economic growth.\nSmaller, faster, better: Why efficient AI models could democratize the technology revolution\nADVERTISEMENT\nThe company’s second pillar focuses on addressing resource constraints faced by AI adopters, particularly smaller organizations that can’t afford the computational demands of large-scale models. By supporting more efficient, specialized models that can run on limited resources, Hugging Face argues the U.S. can enable broader participation in the AI ecosystem.\n“Smaller models that may even be used on edge devices, techniques to reduce computational requirements at inference, and efforts to facilitate mid-scale training for organizations with modest to moderate computational resources all support the development of models that meet the specific needs of their use context,” the submission explains.\nOn security—a major focus of the administration’s policy discussions—Hugging Face makes the counterintuitive case that open and transparent AI systems may be more secure in critical applications. The company suggests that “fully transparent models providing access to their training data and procedures can support the most extensive safety certifications,” while “open-weight models that can be run in air-gapped environments can be a critical component in managing information risks.”\nBig tech vs. little tech: The growing policy battle that could shape AI’s future\nHugging Face’s approach highlights growing policy divisions in the AI industry. While companies like OpenAI and Google emphasize speeding up regulatory processes and reducing government oversight, venture capital firm Andreessen Horowitz (a16z) has advocated for a middle ground, arguing for federal leadership to prevent a patchwork of state regulations while focusing regulation on specific harms rather than model development itself.\n“Little Tech has an important role to play in strengthening America’s ability to compete in AI in the future, just as it has been a driving force of American technological innovation historically,” a16z wrote in its submission, using language that aligns somewhat with Hugging Face’s democratization arguments.\nGoogle’s submission, meanwhile, focused on infrastructure investments, particularly addressing “surging energy needs” for AI deployment—a practical concern shared across industry positions.\nBetween innovation and access: The race to influence America’s AI future\nAs the administration weighs competing visions for American AI leadership, the fundamental tension between commercial advancement and democratic access remains unresolved. OpenAI’s vision of AI development prioritizes speed and competitive advantage through a centralized approach, while Hugging Face presents evidence that distributed, open development can deliver comparable results while spreading benefits more broadly.\nThe economic and security arguments will likely prove decisive. If administration officials accept Hugging Face’s assertion that “a robust AI strategy must leverage open and collaborative development to best drive performance, adoption, and security,” open-source could find a meaningful place in national strategy. But if concerns about China’s AI capabilities dominate, OpenAI’s calls for minimal oversight might prevail.\nWhat’s clear is that the AI Action Plan will set the tone for years of American technological development. As Hugging Face’s submission concludes, both open and proprietary systems have complementary roles to play — suggesting that the wisest policy might be one that harnesses the unique strengths of each approach rather than choosing between them. The question isn’t whether America will lead in AI, but whether that leadership will bring prosperity to the few or innovation for the many.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Nvidia’s Cosmos-Transfer1 makes robot training freakishly realistic—and that changes everything",
        "url": "https://venturebeat.com/ai/nvidias-cosmos-transfer1-makes-robot-training-freakishly-realistic-and-that-changes-everything/",
        "content": "Nvidia has released Cosmos-Transfer1, an innovative AI model that enables developers to create highly realistic simulations for training robots and autonomous vehicles. Available now on Hugging Face, the model addresses a persistent challenge in physical AI development: bridging the gap between simulated training environments and real-world applications.\n“We introduce Cosmos-Transfer1, a conditional world generation model that can generate world simulations based on multiple spatial control inputs of various modalities such as segmentation, depth, and edge,” Nvidia researchers state in a paper published alongside the release. “This enables highly controllable world generation and finds use in various world-to-world transfer use cases, including Sim2Real.”\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nUnlike previous simulation models, Cosmos-Transfer1 introduces an adaptive multimodal control system that allows developers to weight different visual inputs—such as depth information or object boundaries—differently across various scene parts. This breakthrough enables more nuanced control over generated environments, significantly improving their realism and utility.\nHow adaptive multimodal control transforms AI simulation technology\nTraditional approaches to training physical AI systems involve either collecting massive amounts of real-world data — a costly and time-consuming process — or using simulated environments that often lack the complexity and variability of the real world.\nCosmos-Transfer1 addresses this dilemma by allowing developers to use multimodal inputs (like blurred visuals, edge detection, depth maps, and segmentation) to generate photorealistic simulations that preserve crucial aspects of the original scene while adding natural variations.\n“In the design, the spatial conditional scheme is adaptive and customizable,” the researchers explain. “It allows weighting different conditional inputs differently at different spatial locations.”\nThis capability proves particularly valuable in robotics, where a developer might want to maintain precise control over how a robotic arm appears and moves while allowing more creative freedom in generating diverse background environments. For autonomous vehicles, it enables the preservation of road layout and traffic patterns while varying weather conditions, lighting or urban settings.\nPhysical AI applications that could transform robotics and autonomous driving\nADVERTISEMENT\nMing-Yu Liu, one of the core contributors to the project, explained why this technology matters for industry applications.\n“A policy model guides a physical AI system’s behavior, ensuring that the system operates with safety and in accordance with its goals,” Liu and his colleagues note in the paper. “Cosmos-Transfer1 can be post-trained into policy models to generate actions, saving the cost, time, and data needs of manual policy training.”\nADVERTISEMENT\nThe technology has already demonstrated its value in robotics simulation testing. When using Cosmos-Transfer1 to enhance simulated robotics data, Nvidia researchers found the model significantly improves photorealism by “adding more scene details and complex shading and natural illumination” while preserving the physical dynamics of robot movement.\nFor autonomous vehicle development, the model enables developers to “maximize the utility of real-world edge cases,” helping vehicles learn to handle rare but critical situations without needing to encounter them on actual roads.\nInside Nvidia’s strategic AI ecosystem for physical world applications\nCosmos-Transfer1 represents just one component of Nvidia’s broader Cosmos platform, a suite of world foundation models (WFMs) designed specifically for physical AI development. The platform includes Cosmos-Predict1 for general-purpose world generation and Cosmos-Reason1 for physical common sense reasoning.\n“Nvidia Cosmos is a developer-first world foundation model platform designed to help Physical AI developers build their Physical AI systems better and faster,” the company states on its GitHub repository. The platform includes pre-trained models under the Nvidia Open Model License and training scripts under the Apache 2 License.\nThis positions Nvidia to capitalize on the growing market for AI tools that can accelerate autonomous system development, particularly as industries from manufacturing to transportation invest heavily in robotics and autonomous technology.\nReal-time generation: How Nvidia’s hardware powers next-gen AI simulation\nNvidia also demonstrated Cosmos-Transfer1 running in real-time on its latest hardware. “We further demonstrate an inference scaling strategy to achieve real-time world generation with an Nvidia GB200 NVL72 rack,” the researchers note.\nThe team achieved approximately 40x speedup when scaling from one to 64 GPUs, enabling the generation of 5 seconds of high-quality video in just 4.2 seconds — effectively real-time throughput.\nThis performance at scale addresses another critical industry challenge: simulation speed. Fast, realistic simulation enables more rapid testing and iteration cycles, accelerating the development of autonomous systems.\nOpen-source Innovation: Democratizing Advanced AI for Developers Worldwide\nNvidia’s decision to publish both the Cosmos-Transfer1 model and its underlying code on GitHub removes barriers for developers worldwide. This public release gives smaller teams and independent researchers access to simulation technology that previously required substantial resources.\nThe move fits into Nvidia’s broader strategy of building robust developer communities around its hardware and software offerings. By putting these tools in more hands, the company expands its influence while potentially accelerating progress in physical AI development.\nFor robotics and autonomous vehicle engineers, these newly available tools could shorten development cycles through more efficient training environments. The practical impact may be felt first in testing phases, where developers can expose systems to a wider range of scenarios before real-world deployment.\nWhile open source makes the technology available, putting it to effective use still requires expertise and computational resources — a reminder that in AI development, the code itself is just the beginning of the story.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Adobe previews AI generated PowerPoints from raw customer data with ‘Project Slide Wow’",
        "url": "https://venturebeat.com/ai/adobe-previews-ai-generated-powerpoints-from-raw-customer-data-with-project-slide-wow/",
        "content": "Today at Adobe‘s annual digital innovation conference Summit 2024 in Las Vegas, the company is unveiling Project Slide Wow, a generative AI-driven tool designed to streamline the creation of PowerPoint presentations directly from raw customer data.\nPresented as part of the “Adobe Sneaks” program, the innovation aims to solve a common challenge for marketers and analysts—transforming complex data into compelling, easy-to-digest presentations.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nFrom data to presentation—automatically\nProject Slide Wow integrates with Adobe Customer Journey Analytics (CJA) to automatically generate PowerPoint slides populated with relevant data visualizations and speaker notes. This allows marketers and business analysts to quickly build data-backed presentations without manually structuring content or designing slides.\n“It’s analyzing all the charts in this project, generating captions for them, organizing them into a narrative, and creating the presentation slides,” said Jane Hoffswell, a research scientist at Adobe and the creator of Project Slide Wow, in a video call interview with VentureBeat a few days ago. “It figures out the best way to focus on the most important pieces of data.”\nA standout feature of the tool is its interactive AI agent within PowerPoint. Users can ask follow-up questions, request additional visualizations, or dynamically generate new slides on the fly, making it an adaptable solution for data-driven storytelling.\nOne of the biggest advantages of Project Slide Wow is its ability to handle live data updates.\nInstead of static presentations that quickly become outdated, users can refresh their slides to reflect the latest analytics, which is particularly valuable for businesses that rely on real-time data insights.\n“We wanted this technology to be able to keep your data fresh and alive. If you give a presentation six months later, people will want to know how things have changed,” Hoffswell told VentureBeat.\nThe tech under the hood\nADVERTISEMENT\nUnlike many recent AI-powered tools, Project Slide Wow does not rely on large language models (LLMs) like OpenAI’s GPT or Adobe’s Firefly.\nInstead, Adobe’s research team developed a proprietary algorithmic ranking and scoring system to determine which insights are most important for a given dataset.\nThe system prioritizes information based on:\nData Structure in Adobe Customer Journey Analytics (CJA) – Insights that appear higher in the CJA workflow receive more emphasis.\nRelevance & Frequency—In slide generation, data points that appear multiple times across different analyses are given greater weight.\nNarrative Organization – The tool algorithmically arranges insights into a logical storytelling structure to ensure a smooth presentation flow.\n“Our ranking algorithm looks at the layout of the original Customer Journey Analytics project—content higher up is likely more important,” said Hoffswell. “We also prioritize values that frequently appear in the data.”\nADVERTISEMENT\nBecause the system is more rules-based and deterministic rather than relying on probabilistic AI models, it avoids common LLM issues such as hallucinated data or unpredictable outputs. It also allows enterprises to have greater transparency and control over how presentations are structured.\nWhat it means for the enterprise and decision-makers\nFor CTOs, CIOs, team leaders and developer managers, Project Slide Wow represents a potential shift in how teams work with data visualization and presentations. Here’s what it means for enterprise-level decision-making:\nGreater Efficiency for Data Teams – Analysts and marketers can rapidly generate insights in presentation-ready formats, reducing the manual labor involved in building slides from scratch.\nScalability for Large Organizations—By integrating directly into existing workflows, large enterprises can standardize the way customer insights are presented across departments.\nData Integrity & Control – Unlike AI tools that create content based on unpredictable generative models, Project Slide Wow works within the enterprise’s existing datasets in CJA. This ensures data accuracy and minimizes compliance risks.\nEnhanced Collaboration Between Teams—By allowing presentations to be updated dynamically, multiple teams—such as marketing, analytics, and product development—can work with the latest insights in real time without duplicating efforts.\nFuture Integration Potential – If Project Slide Wow becomes a full-fledged Adobe product, enterprise IT leaders may consider integrating it into their existing Microsoft 365 environments through the planned PowerPoint add-on.\nWill it become a full product?\nAdobe Sneaks is an annual showcase of experimental innovations. Around 40% of featured projects eventually become Adobe products.\nThe fate of Project Slide Wow depends on user interest and engagement, as Adobe monitors social media conversations, customer feedback and direct inquiries to gauge demand.\nEric Matisoff, Adobe’s Digital Experience Evangelist and host of Adobe Sneaks, highlighted that the program serves as a testing ground for cutting-edge ideas.\n“We start by scouring the company for hundreds of technologies and ideas…and whittle them down to the seven most exciting, entertaining, and forward-looking innovations,” Matisoff said.\nLooking ahead\nFor businesses that rely on data-driven decision-making, Project Slide Wow could be a major step forward in simplifying the process of building presentations. If the tool gains traction, it may soon be available as an official Adobe product—potentially transforming how companies use customer data to inform strategy.\nUntil then, CTOs, CIOs, team leads, and analysts should stay tuned for Adobe’s Sneaks announcements to see whether Project Slide Wow makes the leap from an experimental demo to a real-world enterprise solution.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Beyond RAG: SEARCH-R1 integrates search engines directly into reasoning models",
        "url": "https://venturebeat.com/ai/beyond-rag-search-r1-integrates-search-engines-directly-into-reasoning-models/",
        "content": "Large language models (LLMs) have seen remarkable advancements in using reasoning capabilities. However, their ability to correctly reference and use external data — information that they weren’t trained on — in conjunction with reasoning has largely lagged behind. \nThis is an issue especially when using LLMs in dynamic, information-intensive scenarios that demand up-to-date data from search engines.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nBut an improvement has arrived: SEARCH-R1, a technique introduced in a paper by researchers at the University of Illinois at Urbana-Champaign and the University of Massachusetts Amherst, trains LLMs to generate search queries and seamlessly integrate search engine retrieval into their reasoning. \nWith enterprises seeking ways to integrate these new models into their applications, techniques such as SEARCH-R1 promise to unlock new reasoning capabilities that rely on external data sources.\nThe challenge of integrating search with LLMs\nSearch engines are crucial for providing LLM applications with up-to-date, external knowledge. The two main methods for integrating search engines with LLMs are Retrieval-Augmented Generation (RAG) and tool use, implemented through prompt engineering or model fine-tuning. \nHowever, both methods have limitations that make them unsuitable for reasoning models. RAG often struggles with retrieval inaccuracies and lacks the ability to perform multi-turn, multi-query retrieval, which is essential for reasoning tasks. \nPrompting-based tool use often struggles with generalization, while training-based approaches require extensive, annotated datasets of search-and-reasoning interactions, which are difficult to produce at scale.\n(In our own experiments with reasoning models, we found that information retrieval remains one of the key challenges.) \nSEARCH-R1\nSEARCH-R1 enables LLMs to interact with search engines during their reasoning process as opposed to having a separate retrieval stage.\nADVERTISEMENT\nSEARCH-R1 defines the search engine as part of the LLM’s environment, enabling the model to integrate its token generation with search engine results seamlessly. \nThe researchers designed SEARCH-R1 to support iterative reasoning and search. The model is trained to generate separate sets of tokens for thinking, search, information, and answer segments. This means that during its reasoning process (marked by <think></think> tags), if the model determines that it needs external information, it generates a <search></search> sequence that contains the search query. The query is then passed on to a search engine and the results are inserted into the context window in an <information></information> segment. The model then continues to reason with the added context and when ready, generates the results in an <answer></answer> segment.\nADVERTISEMENT\nThis structure allows the model to invoke the search engine multiple times as it reasons about the problem and obtains new information (see example below).\nExample of LLM reasoning with SEARCH-R1 (source: arXiv)\nReinforcement learning\nTraining LLMs to interleave search queries with their reasoning chain is challenging. To simplify the process, the researchers designed SEARCH-R1 to train the model through pure reinforcement learning (RL), where the model is left to explore the use of reasoning and search tools without guidance from human-generated data.\nSEARCH-R1 uses an “outcome-based reward model,” in which the model is only evaluated based on the correctness of the final response. This eliminates the need for creating complex reward models that verify the model’s reasoning process.\nThis is the same approach used in DeepSeek-R1-Zero, where the model was given a task and only judged based on the outcome. The use of pure RL obviates the need to create large datasets of manually annotated examples (supervised fine-tuning).\n“SEARCH-R1 can be viewed as an extension of DeepSeek-R1, which primarily focuses on parametric reasoning by introducing search-augmented RL training for enhanced retrieval-driven decision-making,” the researchers write in their paper.\nSEARCH-R1 in action\nThe researchers tested SEARCH-R1 by fine-tuning the base and instruct versions of Qwen-2.5 and Llama-3.2 and evaluating them on seven benchmarks encompassing a diverse range of reasoning tasks requiring single-turn and multi-hop search. They compared SEARCH-R1 against different baselines:‌ direct inference with Chain-of-Thought (CoT) reasoning, inference with RAG, and supervised fine-tuning for tool use.\nSEARCH-R1 consistently outperforms baseline methods by a fair margin. It also outperforms reasoning models trained on RL but without search retrieval. “This aligns with expectations, as incorporating search into LLM reasoning provides access to relevant external knowledge, improving overall performance,” the researchers write.\nSEARCH-R1 is also effective for different model families and both base and instruction-tuned variants, suggesting that RL with outcome-based rewards can be useful beyond pure reasoning scenarios. The researchers have released the code for SEARCH-R1 on GitHub.\nSEARCH-R1’s ability to autonomously generate search queries and integrate real-time information into reasoning can have significant implications for enterprise applications. It can enhance the accuracy and reliability of LLM-driven systems in areas such as customer support, knowledge management, and data analysis. By enabling LLMs to dynamically adapt to changing information, SEARCH-R1 can help enterprises build more intelligent and responsive AI solutions. This capability can be very helpful for applications that require access to constantly changing data, and that require multiple steps to find an answer. \nIt also suggests that we have yet to explore the full potential of the new reinforcement learning paradigm that has emerged since the release of DeepSeek-R1.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Nvidia’s GTC 2025 keynote: 40x AI performance leap, open-source ‘Dynamo’, and a walking Star Wars-inspired ‘Blue’ robot",
        "url": "https://venturebeat.com/ai/nvidias-gtc-2025-keynote-40x-ai-performance-leap-open-source-dynamo-and-a-walking-star-wars-inspired-blue-robot/",
        "content": "Nvidia CEO Jensen Huang took to the stage at the SAP Center on Tuesday morning, leather jacket intact and without a teleprompter, to deliver what has become one of the most anticipated keynotes in the technology industry. The GPU Technology Conference (GTC) 2025, self-described by Huang as the “Super Bowl of AI,” arrives at a critical juncture for Nvidia and the broader artificial intelligence sector.\n“What an amazing year it was, and we have a lot of incredible things to talk about,” Huang told the packed arena, addressing an audience that has grown exponentially as AI has transformed from a niche technology into a fundamental force reshaping entire industries. The stakes were particularly high this year following market turbulence triggered by Chinese startup DeepSeek‘s release of its highly efficient R1 reasoning model, which sent Nvidia’s stock tumbling earlier this year amid concerns about potential reduced demand for its expensive GPUs.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024Navigating AI Regulations in Telecom - AI Impact Tour 2024\nAgainst this backdrop, Huang delivered a comprehensive vision of Nvidia’s future, emphasizing a clear roadmap for data center computing, advancements in AI reasoning capabilities and bold moves into robotics and autonomous vehicles. The presentation painted a picture of a company working to maintain its dominant position in AI infrastructure while expanding into new territories where its technology can create value. Nvidia’s stock traded down throughout the presentation, closing more than 3% lower for the day, suggesting investors may have hoped for even more dramatic announcements.\nBut if Huang’s message was clear, it was this: AI isn’t slowing down, and neither is Nvidia. From groundbreaking chips to a push into physical AI, here are the five most important takeaways from GTC 2025.\nBlackwell platform ramps up production with 40x performance gain over Hopper\nADVERTISEMENT\nThe centerpiece of Nvidia’s AI computing strategy, the Blackwell platform, is now in “full production,” according to Huang, who emphasized that “customer demand is incredible.” This is a significant milestone after what Huang had previously described as a “hiccup” in early production.\nHuang made a striking comparison between Blackwell and its predecessor, Hopper: “Blackwell NVLink 72 with Dynamo is 40 times the AI factory performance of Hopper.” This performance leap is particularly crucial for inference workloads, which Huang positioned as “one of the most important workloads in the next decade as we scale out AI.”\nADVERTISEMENT\nThe performance gains come at a critical time for the industry, as reasoning AI models like DeepSeek‘s R1 require substantially more computation than traditional large language models. Huang illustrated this with a demonstration comparing a traditional large language model (LLM)’s approach to a wedding seating arrangement (439 tokens, but wrong) versus a reasoning model’s approach (nearly 9,000 tokens, but correct).\n“The amount of computation we have to do in AI is so much greater as a result of reasoning AI and the training of reasoning AI systems and agentic systems,” Huang explained, directly addressing the challenge posed by more efficient models like DeepSeek’s. Rather than positioning efficient models as a threat to Nvidia’s business model, Huang framed them as driving increased demand for computation — effectively turning a potential weakness into a strength.\nNext-generation Rubin architecture unveiled with clear multi-year roadmap\nIn a move clearly designed to give enterprise customers and cloud providers confidence in Nvidia’s long-term trajectory, Huang laid out a detailed roadmap for AI computing infrastructure through 2027. This is an unusual level of transparency about future products for a hardware company, but reflects the long planning cycles required for AI infrastructure.\n“We have an annual rhythm of roadmaps that has been laid out for you so that you could plan your AI infrastructure,” Huang stated, emphasizing the importance of predictability for customers making massive capital investments.\nThe roadmap includes Blackwell Ultra in the second half of 2025, offering 1.5 times more AI performance than the current Blackwell chips. This will be followed by Vera Rubin, named after the astronomer who discovered dark matter in the second half of 2026. Rubin will feature a new CPU twice as fast as the current Grace CPU, along with new networking architecture and memory systems.\n“Basically everything is brand new, except for the chassis,” Huang explained about the Vera Rubin platform.\nThe roadmap extends even further to Rubin Ultra in the second half of 2027, which Huang described as an “extreme scale up” offering 14 times more computational power than current systems. “You can see that Rubin is going to drive the cost down tremendously,” he noted, addressing concerns about the economics of AI infrastructure.\nThis detailed roadmap serves as Nvidia’s answer to market concerns about competition and sustainability of AI investments, effectively telling customers and investors that the company has a clear path forward regardless of how AI model efficiency evolves.\nNvidia Dynamo emerges as the ‘operating system’ for AI factories\nOne of the most significant announcements was Nvidia Dynamo, an open-source software system designed to optimize AI inference. Huang described it as “essentially the operating system of an AI factory,” drawing a parallel to how traditional data centers rely on operating systems like VMware to orchestrate enterprise applications.\nDynamo addresses the complex challenge of managing AI workloads across distributed GPU systems, handling tasks like pipeline parallelism, tensor parallelism, expert parallelism, in-flight batching, disaggregated inferencing, and workload management. These technical challenges have become increasingly important as AI models grow more complex and reasoning-based approaches require more computation.\nThe system gets its name from the dynamo, which Huang noted was “the first instrument that started the last Industrial Revolution, the industrial revolution of energy.” The comparison positions Dynamo as a foundational technology for the AI revolution.\nBy making Dynamo open source, Nvidia is attempting to strengthen its ecosystem and ensure its hardware remains the preferred platform for AI workloads, even as software optimization becomes increasingly important for performance and efficiency. Partners including Perplexity are already working with Nvidia on Dynamo implementation.\n“We’re so happy that so many of our partners are working with us on it,” Huang said, specifically highlighting Perplexity as “one of my favorite partners” due to “the revolutionary work that they do.”\nThe open-source approach is a strategic move to maintain Nvidia’s central position in the AI ecosystem while acknowledging the importance of software optimization in addition to raw hardware performance.\nPhysical AI and robotics take center stage with open-source Groot N1 model\nIn what may have been the most visually striking moment of the keynote, Huang unveiled a significant push into robotics and physical AI, culminating with the appearance of “Blue,” a Star Wars-inspired robot that walked onto the stage and interacted with Huang.\n“By the end of this decade, the world is going to be at least 50 million workers short,” Huang explained, positioning robotics as a solution to global labor shortages and a massive market opportunity.\nThe company announced Nvidia Isaac Groot N1, described as “the world’s first open, fully customizable foundation model for generalized humanoid reasoning and skills.” Making this model open-source represents a significant move to accelerate development in the robotics field, similar to how open-source LLMs have accelerated general AI development.\nAlongside Groot N1, Nvidia announced a partnership with Google DeepMind and Disney Research to develop Newton, an open-source physics engine for robotics simulation. Huang explained the need for “a physics engine that is designed for very fine-grain, rigid and soft bodies, designed for being able to train tactile feedback and fine motor skills and actuator controls.”\nThe focus on simulation for robot training follows the same pattern that has proven successful in autonomous driving development, using synthetic data and reinforcement learning to train AI models without the limitations of physical data collection.\n“Using Omniverse to condition Cosmos, and Cosmos to generate an infinite number of environments, allows us to create data that is grounded, controlled by us and yet systematically infinite at the same time,” Huang explained, describing how Nvidia’s simulation technologies enable robot training at scale.\nThese robotics announcements represent Nvidia’s expansion beyond traditional AI computing into the physical world, potentially opening up new markets and applications for its technology.\nGM partnership signals major push into autonomous vehicles and industrial AI\nRounding out Nvidia’s strategy of extending AI from data centers into the physical world, Huang announced a significant partnership with General Motors to “build their future self-driving car fleet.”\n“GM has selected Nvidia to partner with them to build their future self-driving car fleet,” Huang announced. “The time for autonomous vehicles has arrived, and we’re looking forward to building with GM AI in all three areas: AI for manufacturing, so they can revolutionize the way they manufacture; AI for enterprise, so they can revolutionize the way they work, design cars, and simulate cars; and then also AI for in the car.”\nThis partnership is a significant vote of confidence in Nvidia’s autonomous vehicle technology stack from America’s largest automaker. Huang noted that Nvidia has been working on self-driving cars for over a decade, inspired by the breakthrough performance of AlexNet in computer vision competitions.\n“The moment I saw AlexNet was such an inspiring moment, such an exciting moment, it caused us to decide to go all in on building self-driving cars,” Huang recalled.\nAlongside the GM partnership, Nvidia announced Halos, described as “a comprehensive safety system” for autonomous vehicles. Huang emphasized that safety is a priority that “rarely gets any attention” but requires technology “from silicon to systems, the system software, the algorithms, the methodologies.”\nThe automotive announcements extend Nvidia’s reach from data centers to factories and vehicles, positioning the company to capture value throughout the AI stack and across multiple industries.\nThe architect of AI’s second act: Nvidia’s strategic evolution beyond chips\nGTC 2025 revealed Nvidia’s transformation from a GPU manufacturer to an end-to-end AI infrastructure company. Through the Blackwell-to-Rubin roadmap, Huang signaled Nvidia won’t surrender its computational dominance, while its pivot toward open-source software (Dynamo) and models (Groot N1) acknowledges hardware alone can’t secure its future.\nNvidia has cleverly reframed the DeepSeek efficiency challenge, arguing more efficient models will drive greater overall computation as AI reasoning expands—though investors remained skeptical, sending the stock lower despite the comprehensive roadmap.\nWhat sets Nvidia apart is Huang’s vision beyond silicon. The robotics initiative isn’t just about selling chips; it’s about creating new computing paradigms that require massive computational resources. Similarly, the GM partnership positions Nvidia at the center of automotive AI transformation across manufacturing, design, and vehicles themselves.\nHuang’s message was clear: Nvidia competes on vision, not just price. As computation extends from data centers into physical devices, Nvidia bets that controlling the full AI stack—from silicon to simulation—will define computing’s next frontier. In Huang’s world, the AI revolution is just beginning, and this time, it’s stepping out of the server room.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Nvidia will build accelerated quantum computing research center",
        "url": "https://venturebeat.com/games/nvidia-will-build-accelerated-quantum-computing-research-center/",
        "content": "Nvidia said it is building a Boston-based research center to provide cutting-edge technologies to advance quantum computing.\nThe Nvidia Accelerated Quantum Research Center, or NVAQC, will integrate leading quantum hardware with AI supercomputers, enabling what is known as accelerated quantum supercomputing. The NVAQC will help solve quantum computing’s most challenging problems, ranging from tackling qubit noise to the transformation of experimental quantum processors into practical devices.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nBuilding this new center seems to be politically symbolic. The Trump administration has been hostile toward big companies building anything offshore. By creating an important new center in the U.S., Nvidia can show it’s creating American jobs in a technology field that is critical to U.S. competitiveness.\nLeading quantum computing innovators, including Quantinuum, Quantum Machines and QuEra Computing, will tap into NVAQC to drive advancements through collaborations with researchers from leading universities, such as the Harvard Quantum Initiative in Science and Engineering (HQI) and the Engineering Quantum Systems (EQuS) group at the Massachusetts Institute of Technology (MIT).\n“Quantum computing will augment AI supercomputers to tackle some of the world’s most important problems, from drug discovery to materials development,” said Jensen Huang, founder and CEO of Nvidia, in a statement. “Working with the wider quantum research community to advance CUDA-quantum hybrid computing, the Nvidia Accelerated Quantum Research Center is where breakthroughs will be made to create large-scale, useful, accelerated quantum supercomputers.”\nHuang made the announcement at Nvidia GTC 2025, which runs through March 21.\nPropelling quantum innovation\nThrough the NVAQC, commercial and academic partners will work with Nvidia to use Nvidia GB200 NVL72 rack-scale systems, or powerful hardware for quantum computing applications. This enables complex simulations of quantum systems and the deployment of the low-latency quantum hardware control algorithms essential for quantum error correction. Nvidia GB200 NVL72 systems will also accelerate the adoption of AI algorithms in quantum computing research.\nTo address the challenges of integrating GPU and QPU hardware, the NVAQC will employ the Nvidia CUDA-Q™ quantum development platform, enabling researchers to develop new hybrid quantum algorithms and applications.\nADVERTISEMENT\nThe HQI — a community of researchers dedicated to advancing the science and engineering of quantum systems and their applications — will collaborate with the NVAQC to advance their research on next-generation quantum computing technologies.\n“The NVAQC is a very special addition to the unique Boston area quantum ecosystem, including word-leading university groups and startup companies,” said Mikhail Lukin, professor at Harvard and a co-director of HQI, in a statement. “The accelerated quantum and classical computing technologies NVIDIA is bringing together has the potential to advance the research in areas ranging from quantum error correction to applications of quantum computing systems, accelerating quantum computing research and pulling useful quantum computing closer to reality.”\nADVERTISEMENT\nResearchers from the EQuS group, a member of the MIT Center for Quantum Engineering — which serves as a hub for research, education and engagement in support of quantum engineering — will use NVAQC to develop techniques such as quantum error correction.\n“The Nvidia Accelerated Quantum Research Center will provide EQuS group researchers with unprecedented access to technologies and expertise needed to solve the challenges of useful quantum computing,” said William Oliver, professor of electrical engineering and computer science, and of physics, leader of the EQuS group, and director of the MIT Center for Quantum Engineering, in a statement. “We anticipate the future will also include other members of the Center for Quantum Engineering at MIT. Integrating the Nvidia accelerated computing platform with qubits will help tackle core challenges like quantum error correction, hybrid application development and quantum device characterization.”\nThe NVAQC is expected to begin operations later this year.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Nvidia gets industrial software firms to integrate Omniverse to accelerate physical AI",
        "url": "https://venturebeat.com/games/nvidia-gets-industrial-software-firms-to-integrate-omniverse-to-accelerate-physical-ai/",
        "content": "Nvidia said the big industrial software and service providers are integrating Omniverse into their platforms to accelerate industrial digitalization with physical AI.\nThose integrating Nvidia Omniverse include Ansys, Databricks, Dematic, Omron, SAP, Schneider Electric with ETAP, Siemens and more. Nvidia made the announcement during Jensen Huang’s keynote speech during the Nvidia GTC 2025 event today in San Jose, California.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nNew Nvidia Omniverse Blueprints connected to Nvidia Cosmos world foundation models are now available to enable robot-ready facilities and large-scale synthetic data generation for physical AI development.\n“Omniverse is an operating system that connects the world’s physical data to the realm of physical AI,” said Rev Lebaredian, vice president of Omniverse simulation technology at Nvidia, in a statement. “With Omniverse, global industrial software, data and professional services leaders are uniting industrial ecosystems and building new applications that will advance the next generation of AI for industries at\nunprecedented speed.”\nNew Blueprints Enable Robot-Ready Facilities and Large-Scale Synthetic Data Generation\nMega, an Omniverse Blueprint for testing multi-robot fleets at scale in industrial digital twins, is now available in preview on build.nvidia.com. Also available is the Nvidia AI Blueprint for video search and summarization, powered by the Nvidia Metropolis platform, for building AI agents that monitor activity across entire facilities.\nManufacturing leaders are using the blueprints to optimize their industrial operations with physical AI.\nIn automotive manufacturing, Schaeffler and Accenture are starting to adopt Mega to test and simulate fleets of Agility Robotics Digit for material-handling automation. Hyundai Motor Group is using the blueprint to simulate Boston Dynamics Atlas robots on its assembly lines, and Mercedes-Benz is using it to simulate Apptronik’s Apollo humanoid robots to optimize vehicle assembly operations.\nIn electronics manufacturing, Pegatron is using Mega to develop physical AI-based Nvidia Metropolis video analytics agents to improve factory operations and worker safety. Foxconn is using the blueprint to simulate industrial manipulators, humanoids and mobile robots in its manufacturing facilities for the Nvidia Blackwell platform.\nADVERTISEMENT\n“Foxconn is constantly exploring ways to transform our operations as we continue our journey toward building the factories of the future,” said Brand Cheng, CEO of Fii, a core subsidiary of Foxconn, in a statement. “Using NVIDIA Omniverse and Mega, we’re testing and training humanoids to operate in our leading factories as we advance to the next wave of physical AI.”\nFor warehouses and supply chain solutions, KION Group, Dematic and Accenture announced they are integrating Mega to advance next-generation AI-powered automation. Idealworks is integrating Mega into its fleet management software to simulate, test and optimize robotic fleets. SAP customers and partners can use Omniverse to develop their own virtual environments for warehouse management\nscenarios.\nADVERTISEMENT\nA new Omniverse Blueprint for AI factory digital twins lets data center engineers design and simulate AI factory layouts, cooling and electrical to maximize utilization and efficiency. Cadence Reality Digital Twin Platform and Schneider Electric with ETAP are the first to integrate their simulation software into the blueprint, while Vertiv and Schneider Electric are providing Omniverse SimReady 3D models of their\npower and cooling units to accelerate the development of AI factory digital twins.\nThe Nvidia Isaac GR00T Blueprint for synthetic manipulation motion generation is also now available for robotics developers, enabling large-scale synthetic data generation from Omniverse and Cosmos. The blueprint helps humanoid developers reduce data collection time from hours to minutes, fast-tracking robot development.\nOmniverse Physical AI Operating System Expands Across Industries\nDigitalization is challenging for industries grounded in the physical world. Massive amounts of digital and physical world data from legacy systems create silos. Omniverse is an operating system built on the OpenUSD framework that enables developers to unify physical-world data and applications.\nAnsys, Cadence, Hexagon, Omron, Rockwell Automation and Siemens are integrating Omniverse data interoperability and visualization technologies into their leading industrial software, simulation and automation solutions to accelerate product development and optimize manufacturing processes.\nFor physical AI, Intrinsic, an Alphabet company, is enabling Omniverse workflows and Nvidia robotics foundation models to transition from digital twins to hardware deployments using Flowstate. Databricks is integrating Nvidia Omniverse with the Databricks Data Intelligence Platform, which will enable large-scale synthetic data generation for physical AI.\nGeneral Motors, America’s largest auto manufacturer, announced its adoption of Omniverse to enhance its factories and train platforms for operations such as material handling, transportation and precision welding. At the other end of the manufacturing life cycle, Unilever announced its adoption of Omniverse and physically accurate digital twins to streamline and optimize marketing content creation for its products.\nOmniverse in Every Cloud\nOmniverse AI factory blueprint.\nTo simplify development, deployment and scale-out of OpenUSD-based applications, Nvidia Omniverse is now available as virtual desktop images on EC2 G6e instances with NVIDIA L40S GPUs in AWS Marketplace. The Microsoft Azure Marketplace now features preconfigured Omniverse instances and Omniverse Kit App Streaming on Nvidia A10 GPUs, allowing developers to easily develop and\nstream their custom Omniverse applications.\nThese cloud-based Nvidia Omniverse developer tools and services are expected to be available later this year on Oracle Cloud Infrastructure compute bare-metal instances with Nvidia L40S GPUs, as well as the newly announced Nvidia RTX Pro 6000 Blackwell Server Edition on Google Cloud.\nOpenUSD Unifies Robotics Workflows\nAt GTC, NVIDIA introduced the OpenUSD Asset Structure Pipeline for Robotics with Disney Research and Intrinsic. This new structure and data pipeline uses today’s best practices within OpenESt to work toward unifying robotic workflows, providing a common language for all data sources.\nGTC 2025 runs through Friday March 21.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Nvidia launches Blackwell RTX Pro for workstations and servers",
        "url": "https://venturebeat.com/games/nvidia-launches-blackwell-rtx-pro-for-workstations-and-servers/",
        "content": "Nvidia today announced the Nvidia RTX Pro Blackwell series — a new generation of workstation and server graphics processing units.\nNvidia said the GPUs redefine workflows for AI, technical, creative, engineering and design professionals with accelerated computing, AI inference, ray tracing and neural rendering technologies. The company unveiled the news during the keynote speech of Jensen Huang, CEO of Nvidia, at the GTC 2025 event.\nbringing_brands_&_celebrities_into_the_new_world_of_gaming (1080p).mp4\nFor everything from agentic AI, simulation, extended reality, 3D design and complex visual effects to developing physical AI powering autonomous robots, vehicles and smart spaces, the RTX Pro Blackwell series provides professionals across industries with compute power, memory capacity and data throughput right at their fingertips — from their desktop, on the go with mobile workstations or powered by data center GPUs.\nThe new lineup includes:\nData center GPU: Nvidia RTX PRO 6000 Blackwell Server Edition\nDesktop GPUs: Nvidia RTX PRO 6000 Blackwell Workstation Edition, Nvidia RTX\nPRO 6000 Blackwell Max-Q Workstation Edition, Nvidia RTX PRO 5000 Blackwell,\nNvidia RTX PRO 4500 Blackwell and Nvidia RTX PRO 4000 Blackwell\nLaptop GPUs: Nvidia RTX PRO 5000 Blackwell, Nvidia RTX PRO 4000 Blackwell,\nNvidia RTX PRO 3000 Blackwell, Nvidia RTX PRO 2000 Blackwell, Nvidia RTX PRO\n1000 Blackwell and Nvidia RTX PRO 500 Blackwell\n“Software developers, data scientists, artists, designers and engineers need powerful AI and graphics performance to push the boundaries of visual computing and simulation, helping tackle incredible industry challenges,” said Bob Pette, vice president of enterprise platforms at Nvidia, in a statement. “Bringing Nvidia Blackwell to workstations and servers will take productivity, performance and speed to new heights, accelerating AI inference serving, data science, visualization and content creation.”\nNvidia Blackwell technology comes to workstations and data centers\nNvidia RTX PRO Blackwell Desktop GPUs.\nRTX Pro Blackwell GPUs unlock the potential of generative, agentic and physical AI by delivering exceptional performance, efficiency and scale.\nNvidia RTX Pro Blackwell GPUs feature:\nNvidia Streaming Multiprocessor: Offers up to 1.5 times faster throughput and new neural shaders that integrate AI inside of programmable shaders to drive the next decade of AI-augmented graphics innovations.\nFourth-Generation RT Cores: Delivers up to two times the performance of the previous generation to create photoreal, physically accurate scenes and complex 3D designs with optimizations for Nvidia RTX Mega Geometry.\nFifth-Generation Tensor Cores: Delivers up to 4,000 AI trillion operations per second and adds support for FP4 precision and Nvidia DLSS 4 Multi Frame Generation, enabling a new era of AI-powered graphics and the ability to run and prototype larger AI models faster.\nLarger, Faster GDDR7 Memory: Boosts bandwidth and capacity — up to 96GB for workstations and servers and up to 24GB on laptops. This enables applications to run faster and work with larger, more complex datasets for everything from tackling massive 3D and AI projects to exploring large-scale virtual reality environments.\nNinth-Generation Nvidia NVENC: Accelerates video encoding speed and improves quality for professional video applications with added support for 4:2:2 encoding.\nSixth-Generation Nvidia NVDEC: Provides up to double the H.264 decoding throughput and offers support for 4:2:2 H.264 and HEVC decode. Professionals can benefit from high-quality video playback, accelerate video data ingestion and use advanced AI-powered video editing features.\nFifth-Generation PCIe: Support for fifth-generation PCI Express provides double the bandwidth over the previous generation, improving data transfer speeds from CPU memory and unlocking faster performance for data-intensive tasks.\nDisplayPort 2.1: Drives high-resolution displays at up to 4K at 480Hz and 8K at 165Hz. Increased bandwidth enables seamless multi-monitor setups, while high dynamic range and higher color depth support deliver more precise color accuracy for tasks like video editing, 3D design and live broadcasting.\nMulti-Instance GPU (MIG): The RTX PRO 6000 data center and desktop GPUs and 5000 series desktop GPUs feature MIG technology, enabling secure partitioning of a single GPU into up to four instances (6000 series) or two instances (5000 series).\nNvidia said Fault isolation is designed to prevent workload interference for secure, efficient resource allocation for diverse workloads, maximizing performance and flexibility.\nADVERTISEMENT\nThe new laptop GPUs also support the latest Nvidia Blackwell Max-Q technologies, which intelligently and continually optimize laptop performance and power efficiency with AI.\nNvidia RTX PRO Blackwell Laptop GPUs.\nWith neural rendering and AI-augmented tools, Nvidia RTX PRO Blackwell GPUs enable the creation of stunning visuals, digital twins of real-world environments and immersive experiences with unprecedented speed and efficiency. The GPUs are built to elevate 3D computer-aided design and building information model workflows, offering designers and engineers exceptional performance for complex modeling, rendering and visualization.\nADVERTISEMENT\nDesigned for enterprise data center deployments, the RTX PRO 6000 Blackwell Server Edition features a passively cooled thermal design and can be configured with up to eight GPUs per server. For workloads that require the compute density and scale that data centers offer, the RTX PRO 6000 Blackwell Server Edition delivers powerful performance for next-generation AI, scientific and visual computing applications across industries such as healthcare, manufacturing, retail and media and entertainment.\nIn addition, this powerful data center GPU can be combined with Nvidia vGPU software to power AI workloads across virtualized environments and deliver high-performance virtual workstation instances to remote users. Nvidia vGPU support for the Nvidia RTX PRO 6000 Blackwell Server Edition GPU is expected in the latter half of this year.\n“Foster + Partners has tested the Nvidia RTX PRO 6000 Blackwell Max-Q Workstation Edition GPU on Cyclops, our GPU-based ray-tracing product,” said Martha Tsigkari, head of applied research and development and senior partner at Foster + Partners, in a statement. “The new Nvidia Blackwell GPU has managed to outperform everything we have tested before. For example, when using it with Cyclops, it has performed at 5x the speed of Nvidia RTX A6000 GPUs. Rendering speeds also increased five times, allowing tools like Cyclops to provide feedback on how well our design solutions perform in real time as we design them and resulting in intuitive yet informed decision-making from early conceptual stages.”\n“Nvidia RTX PRO 6000 Blackwell Workstation Edition GPUs enable incredibly sharp and photorealistic graphics,” said Jeff Hammoud, chief design officer at Rivian. “In conjunction with a Varjo XR4 headset and Autodesk VRED, the system delivered the level of crispness necessary for immersive automotive design reviews. With Nvidia Blackwell support for PCIe Gen 5, we used two powerful 600W GPUs via VR SLI, allowing us to achieve the highest pixel density and the most stunning visuals we have ever experienced in VR.”\nRTX PRO GPUs run on the Nvidia AI platform and feature larger memory capacity and the latest Tensor Cores to accelerate a deep ecosystem of AI-accelerated applications built on Nvidia CUDA and RTX technology. With everything from the latest AI-based content creation tools and new reasoning models, such as the Nvidia Llama Nemotron Reason family of models and Nvidia NIM microservices unveiled today, inferencing is faster than ever. And with over 400 Nvidia CUDA-X libraries, developers can easily build, optimize, deploy and scale new AI applications, from workstations to the data center or cloud.\nEnterprises can fast-track their AI development and deployments by prototyping locally with an Nvidia RTX PRO GPU and the Nvidia Omniverse and Nvidia AI Enterprise platforms, Nvidia Blueprints and Nvidia NIM, which gives access to easy-to-use inference microservices backed by enterprise-level support. They can also run these applications at scale on the ultimate universal data center GPU for AI and visual computing, delivering breakthrough acceleration for the most demanding compute-intensive enterprise workloads with the RTX Pro 6000 Blackwell Server Edition.\nAvailability\nThe Nvidia RTX Pro 6000 Blackwell Server Edition will soon be available in server configurations from leading data center system partners including Cisco, Dell Technologies, Hewlett Packard Enterprise, Lenovo and Supermicro.\nCloud service providers and GPU cloud providers including AWS, Google Cloud, Microsoft Azure and CoreWeave will be among the first to offer instances powered by the Nvidia RTX Pro 6000 Blackwell Server Edition later this year. In addition, the server edition GPU will be available in data center platforms from ASUS, GIGABYTE, Ingrasys, Quanta Cloud Technology (QCT) and other global system partners.\nThe Nvidia RTX PRO 6000 Blackwell Workstation Edition and Nvidia RTX PRO 6000 Blackwell Max-Q Workstation Edition will be available through global distribution partners such as PNY and TD Synnex starting in April, with availability from manufacturers, such as BOXX, Dell, HP Inc., Lambda and Lenovo, starting in May.\nThe Nvidia RTX PRO 5000, RTX PRO 4500 and RTX PRO 4000 Blackwell GPUs will be available in the summer from Boxx, Dell, HP and Lenovo and through global distribution partners.\nNvidia RTX PRO Blackwell laptop GPUs will be available from Dell, HP, Lenovo and Razer starting later this year.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Nvidia Omniverse Blueprint enables better Earth-2 weather forecasting",
        "url": "https://venturebeat.com/game-development/nvidia-omniverse-blueprint-enables-better-earth-2-weather-forecasting/",
        "content": "Nvidia announced the Nvidia Omniverse Blueprint for Earth-2 weather analytics to accelerate the development of more accurate weather forecasting.\nClimate-related weather events have had a $2 trillion impact on the global economy over the last decade, the company said. The new Omniverse Blueprint equips users with the latest technologies to help global organizations improve risk management and disaster preparedness.\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nThe Nvidia Omniverse Blueprint for Earth-2 weather analytics offers reference workflows — including Nvidia GPU acceleration libraries, a physics-AI framework, development tools and microservices — to help enterprises go from prototyping to production with weather forecast models.\nThe company unveiled the news during the keynote speech of Jensen Huang, CEO of Nvidia, at the GTC 2025 event. Earth-2 is a big technological achievement when it comes to predicting the weather, but it’s also a representation of the kind of resources it would take to create an Earth-size metaverse, or simulation environment that is just like the real world. In that way, it interesting to more than just weather people.\nIn his speech, Huang said, “This is one of the biggest things that has happened in the last year. We have made it possible for every important CAE software to be accelerated. This is just a sample of the libraries made possible by CUDA.”\nEasy-to-deploy Nvidia NIM microservices for Nvidia Earth-2 are also part of the blueprint, including CorrDiff for downscaling and FourCastNet for predicting global atmospheric dynamics of various weather and climate variables. These are already being used by weather technology companies, researchers and government agencies to derive insights and mitigate risk from extreme weather events.\n“We’re seeing more extreme weather events and natural disasters than ever, threatening lives and property,” said Huang. “The Nvidia Omiverse Blueprint for Earth-2 will help industries around the world prepare for — and mitigate — climate change and weather-related disasters.”\nEcosystem support\nEarth-2 at Nvidia’s GTC 2024 event.\nIndustry-leading climate tech companies including AI company G42, JBA Risk Management, Spire and others are using the blueprint to develop unique AI-augmented solutions.\nWhen combined with proprietary enterprise data in the $20 billion climate tech industry, the Nvidia Earth-2 platform helps developers build solutions that deliver warnings and updated forecasts in seconds rather than minutes or hours with traditional CPU-driven modeling.\nADVERTISEMENT\nG42 is integrating various components of the Omniverse Blueprint with its own AI-driven forecasting models for Earth-2 to provide the UAE’s National Center of Meteorology with AI technologies for advanced weather forecasting and disaster management.\n“G42 is advancing AI-powered forecasting to help governments and enterprises strengthen resilience against extreme weather in a rapidly changing world,” said Andrew Jackson, CEO of Inception, a G42 company. “Using high-resolution weather and climate modeling, we are transforming how organizations anticipate and respond to severe weather conditions with precision and speed. Building on Nvidia’s CorrDiff model, we have developed a custom AI-driven system that downscales coarse weather data into hyper-local forecasts, enabling faster predictions at an unprecedented scale. Combined with the Earth-2 Blueprint, this technology equips decision-makers with actionable intelligence to protect communities, safeguard infrastructure and plan for a more resilient future.”\nADVERTISEMENT\nSpire Global used AI components from the blueprint as reference to develop new AI products that integrate its proprietary satellite data and deliver medium-range and sub-seasonal forecasts out to 45 days. Powered by Nvidia GPUs and the Omniverse Blueprint for Earth-2, Spire’s models run 1,000 times faster than traditional physics-based models, enabling large ensemble forecasts that capture the full range of possible weather outcomes.\nIn addition to the Central Weather Administration of Taiwan and The Weather Company, other companies adopting or exploring Earth-2 include 3D mapping company Ecopia AI, spatial analytics company ESRI, green-energy company GCL Power, flood risk management company JBA Risk Management, aerospace company OroraTech, and Tomorrow.io, a leading resilience platform powered by proprietary space data and weather intelligence.\nGroundbreaking generative AI for climate tech\nNvidia’s Earth-2 software will simulate the planet’s climate.\nThe Earth-2 platform offers tools, microservices and an array of state-of-the-art AI weather models for visualizing and simulating the globe.\nCorrDiff, part of the Omniverse Blueprint, is available as an Nvidia NIM microservice. Compared to CPUs, it can be 500 times faster and 10,000 times more energy-efficient in delivering high-resolution numerical weather prediction methods, Nvidia said.\nThe Omniverse Blueprint for Earth-2 allows independent software vendors to develop and deploy AI-augmented solutions and use observational data to make their solutions faster and more accurate.\nEsri, the market leader in geospatial software, is collaborating with NVIDIA to connect its ArcGIS platform to Earth-2 through the blueprint. OroraTech is exploring connecting its data platform to the Omniverse Blueprint for Earth-2.\nTomorrow.io contributed its near-real-time proprietary satellite data to help create an NVIDIA digital twin of Earth for next-generation AI model training, inference and reinforcement.\nA key component of the new blueprint is Nvidia Omniverse, a platform for developing OpenUSD-based 3D workflows and applications. The Omniverse Blueprint for Earth-2 showcases how developers can use Omniverse software development kits and microservices to build Nvidia RTX-powered visualization pipelines for rendering geospatial and weather data.\nNvidia DGX cloud-powered compute\nNvidia’s Earth-2 will be a digital twin of the planet.\nThe Omniverse Blueprint for Earth-2 taps into the Nvidia DGX Cloud platform to demonstrate full-stack acceleration for AI-augmented weather forecasting.\nRunning on Nvidia DGX GB200, Nvidia HGX B200 and Nvidia OVX supercomputers, the blueprint provides a path to simulating and visualizing the global climate simulations at exceptional speed and scale.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Nvidia Blackwell accelerates computer-aided engineering software by 50X",
        "url": "https://venturebeat.com/games/nvidia-blackwell-accelerates-computer-aided-engineering-software-by-50x/",
        "content": "Nvidia announced that Nvidia Blackwell hardware will accelerate by 50 times the big computer-aided software engineering firms’ software for digital twins.\nThe vendors include Ansys, Altair, Cadence, Siemens and Synopsys. With such accelerated software, along with Nvidia CUDA-X libraries and blueprints to optimize performance such as automotive, aerospace, energy, manufacturing and life sciences can significantly reduce product development time, cut costs and increase design accuracy while maintaining energy efficiency.\nbringing_brands_&_celebrities_into_the_new_world_of_gaming (1080p).mp4\n“CUDA-accelerated physical simulation on Nvidia Blackwell has enhanced real-time digital twins and is reimagining the entire engineering process,” said Jensen Huang, founder and CEO of Nvidia, in a statement. “The day is coming when virtually all products will be created and brought to life as a digital twin long before it is realized physically.”\nThe company unveiled the news during Huang’s keynote at the GTC 2025 event. As noted in my recent Q&A with Ansys CTO Prith Banerjee, the gap between simulation and reality is closing, not just in things like video games but also with the similar technology used for engineering simulations.\nEcosystem support for Nvidia Blackwell\nSoftware providers can help their customers develop digital twins with real-time interactivity and now accelerate them with NVIDIA Blackwell technologies.\nThe growing ecosystem integrating Blackwell into its software includes Altair, Ansys, BeyondMath, Cadence, COMSOL, ENGYS, Flexcompute, Hexagon, Luminary Cloud, M-Star, NAVASTO, an Autodesk company, Neural Concept, nTop, Rescale, Siemens, Simscale, Synopsys and Volcano Platforms.\nCadence is using Nvidia Grace Blackwell-accelerated systems to help solve one of computational fluid dynamics’ biggest challenges — the simulation of an entire aircraft during takeoff and landing. Using the Cadence Fidelity CFD solver, Cadence successfully ran multibillion cell simulations on a single NVIDIA GB200 NVL72 server in under 24 hours, which would have previously required a CPU cluster with hundreds of thousands of cores and several days to complete.\nThis breakthrough will help the aerospace industry move toward designing safer, more efficient aircrafts while reducing the amount of expensive wind-tunnel testing required, speeding time to market.\nAnirudh Devgan, president and CEO of Cadence, said in a statement, “Nvidia Blackwell’s acceleration of\nthe Cadence.AI portfolio delivers increased productivity and quality of results for intelligent system design — reducing engineering tasks that took hours to minutes and unlocking simulations not possible before. Our collaboration with Nvidia drives innovation across semiconductors, data centers, physical AI and sciences.”\nSassine Ghazi, president and CEO of Synopsys, said, “At GTC, we’re unveiling the latest performance results observed across our leading portfolio when optimizing Synopsys solutions for NVIDIA Blackwell to accelerate computationally intensive chip design workflows. Synopsys technology is mission-critical to the productivity and capabilities of engineering teams, from silicon to systems. By harnessing the power of Nvidia accelerated computing, we can help customers unlock new levels of performance and deliver their innovations even faster.”\nADVERTISEMENT\nAjei Gopal, president and CEO of Ansys, said, in a statement: “The close collaboration between Ansys and NVIDIA is accelerating innovation at an unprecedented pace. By harnessing the computational performance of NVIDIA Blackwell GPUs, we at Ansys are empowering engineers at Volvo Cars to tackle the most complex computational fluid dynamics challenges with exceptional speed and accuracy — enabling more optimization studies and delivering more performant vehicles.”\nJames Scapa, founder and CEO of Altair, said in a statement, “The Nvidia Blackwell platform’s computing power, combined with Altair’s cutting-edge simulation tools, gives users transformative capabilities. This combination makes GPU-based simulations up to 1.6 times faster compared with the previous generation, helping engineers rapidly solve design challenges and giving industries the power to create safer, more sustainable products through real-time digital twins and physics-informed AI.”\nADVERTISEMENT\nRoland Busch, President and CEO of Siemens said: “The combination of Nvidia’s groundbreaking Blackwell architecture with Siemens’ physics-based digital twins will enable engineers to drastically reduce development times and costs through using photo-realistic, interactive digital twins. This collaboration will allow us to help customers like BMW innovate faster, optimize processes, and achieve remarkable levels of efficiency in design and manufacturing.”\nRescale CAE Hub With Nvidia Blackwell\nRescale’s newly launched CAE Hub enables customers to streamline their access to Nvidia technologies and CUDA®-accelerated software developed by leading independent software vendors. Rescale CAE Hub provides flexible, high-performance computing and AI technologies in the cloud powered by Nvidia GPUs and Nvidia DGX Cloud.\nBoom Supersonic, the company building the world’s fastest airliner, will use the Nvidia Omniverse Blueprint for real-time digital twins and Blackwell-accelerated CFD solvers on Rescale CAE Hub to design and optimize its new supersonic passenger jet.\nThe company’s product development cycle, which is almost entirely simulation-driven, will use the Rescale platform accelerated by Blackwell GPUs to test different flight conditions and refine requirements in a continuous loop with simulation.\nThe adoption of the Rescale CAE Hub powered by Blackwell GPUs expands Boom Supersonic’s collaboration with Nvidia. Through the Nvidia PhysicsNeMo framework and the Rescale AI Physics platform, Boom Supersonic can unlock 4x more design explorations for its supersonic airliner, speeding iteration to improve performance and time to market.\nThe Nvidia Omniverse Blueprint for real-time digital twins, now generally available, is also part of the Rescale CAE Hub. The blueprint brings together Nvidia CUDA-X libraries, Nvidia PhysicsNeMo AI and the Nvidia Omniverse platform — and is also adding the first Nvidia NIM microservice for external aerodynamics, the study of how air moves around objects.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Rune Aero uses Nvidia GPUs to cut virtual wind tunnel costs by 80%",
        "url": "https://venturebeat.com/games/rune-aero-uses-nvidia-gpus-to-cut-virtual-wind-tunnel-costs-by-80/",
        "content": "Rune Aero has revolutionized its aircraft development process using an interactive virtual wind tunnel powered by Physics AI, achieving an 80% reduction in early design costs while doubling payload capacity and cutting fuel consumption by 50%.  \nThe groundbreaking digital twin technology combines Luminary Cloud’s Nvidia CUDA-X-accelerated CFD simulations with an Nvidia Omniverse Blueprint for real time computer-aided engineering digital twins. Ultimately engineers can observe aerodynamic  effects instantly as they modify designs. This transformative approach has compressed what  traditionally required months of iterative physical testing into a continuous, real-time optimization  process for the autonomous cargo aircraft startup. \n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nThe company made the announcement at Nvidia’s GTC 2025 in San Jose, California.\nAI Physics Transforms Aircraft Economics with Real-Time Design Optimization \nFounded in 2023, Rune Aero is developing a sustainable and autonomous aircraft for the  growing middle-mile air cargo market. As a new startup, they are embracing Physics AI, a  modern approach to rapid design exploration. Rune Aero’s adoption of the virtual wind tunnel  demonstrates how aerospace design engineers can quickly analyze many aircraft design  prototypes, iterate designs, and visualize the impact in real time.\nBy performing virtual wind  tunnel testing earlier in the design process, Rune Aero dramatically reduces future technical risk  and costs compared to relying solely on conventional physical wind tunnel tests.  \nThe company said the results are transformative: 70% reduction in operating costs for cargo operators through optimized aerodynamics and fuel efficiency, doubled payload capacity, and 50% lower fuel  consumption. For operators, this translates to double-digit profitability increases that  fundamentally change air freight economics. \nLuminary Cloud developed the interactive virtual wind tunnel by combining its GPU-native CFD  solvers leveraging Nvidia CUDA-X libraries, the PhysicsNeMo AI training framework, and  Omniverse visualization API to create digital twin environments that deliver immediate design  feedback. This technology stack makes other digital twin environments possible, with the ability  to incorporate real-world data into the Physics AI platform.\n“AI Physics models are essential for real-time interactive engineering analysis and design, but require tremendous amounts of data,” said Tim Costa, senior director CAE EDA and quantum at Nvidia, in a statement. “Luminary Cloud’s accelerated CFD solver allows customers to generate the high-fidelity data needed to train the underlying AI physics model in Nvidia PhysicsNeMo in a matter of hours.” \nADVERTISEMENT\n“It is exciting to work with aerospace companies like Rune Aero that are willing to adopt new  design optimization techniques. By quickly running a large database of simulations and creating  Physics AI models, Rune Aero engineers can arrive at a feasible design faster to solve  important regional air transportation problems,” said Juan J. Alonso, CTO Luminary Cloud, in a statement.  \n“At the early stages of aircraft development, getting fast, accurate, and cost-effective design  feedback is essential. Luminary’s virtual wind tunnel allows Rune Aero to test configurations  earlier in the process, reducing our early development costs by over 80% compared to  traditional wind tunnels and enabling faster, smarter design decisions,” said Nadine Auda, cofounder of Rune Aero, in a statement. “Rune’s optimized aircraft configuration resulting from these tests  improves aerodynamic lift-to-drag ratio, combined with advanced propulsion, this doubles  payload and range while cutting fuel burn by 50%, ultimately lowering operating costs for our customers.” \nADVERTISEMENT\nLuminary Cloud and nTop today also announced a new integration with Nvidia PhysicsNeMo that reduces physics-based AI design optimization from weeks or months to mere hours. By seamlessly connecting nTop’s parametric geometry generation, Luminary’s GPU-native simulation and simulation management platform, and Nvidia PhysicsNeMo via NIM microservices, engineers can now create and analyze hundreds of design variations in a single day—a process that previously took weeks to months of manual effort across disconnected systems.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Nvidia debuts Llama Nemotron open reasoning models in a bid to advance agentic AI",
        "url": "https://venturebeat.com/ai/nvidia-debuts-llama-nemotron-open-reasoning-models-in-a-bid-to-advance-agentic-ai/",
        "content": "Nvidia is getting into the open source reasoning model market.\nAt the Nvidia GTC event today, the AI giant made a series of hardware and software announcements. Buried amidst the big silicon announcements, the company announced a new set of open source Llama Nemotron reasoning models to help accelerate agentic AI workloads. The new models are an extension of the Nvidia Nemotron models that were first announced in January at the Consumer Electronics Show (CES).\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nThe new Llama Nemotron reasoning models are in part a response to the dramatic rise of reasoning models in 2025. Nvidia (and its stock price) were rocked to the core earlier this year when DeepSeek R1 came out, offering the promise of an open source reasoning model and superior performance.\nThe Llama Nemotron family models are competitive with DeepSeek offering business-ready AI reasoning models for advanced agents. \n“Agents are autonomous software systems designed to reason, plan, act and critique their work,” Kari Briski, vice president of Generative AI Software Product Managements at Nvidia said during a GTC pre-briefing with press. “Just like humans, agents need to understand context to breakdown complex requests, understand the user’s intent, and adapt in real time.”\nWhat’s inside Llama Nemotron for agentic AI\nAs the name implies Llama Nemotron is based on Meta’s open source Llama models.\nWith Llama as the foundation, Briski said that Nvidia algorithmically pruned the model to optimize compute requirements while maintaining accuracy.\nNvidia also applied sophisticated post-training techniques using synthetic data. The training involved 360,000 H100 inference hours and 45,000 human annotation hours to enhance reasoning capabilities. All that training results in models that have exceptional reasoning capabilities across key benchmarks for math, tool calling, instruction following and conversational tasks, according to Nvidia.\nThe Llama Nemotron family has three different models\nADVERTISEMENT\nThe family includes three models targeting different deployment scenarios:\nNemotron Nano: Optimized for edge and smaller deployments while maintaining high reasoning accuracy.\nNemotron Super: Balanced for optimal throughput and accuracy on single data center GPUs.\nNemotron Ultra: Designed for maximum “agentic accuracy” in multi-GPU data center environments.\nFor availability, Nano and Super are now available at NIM micro services and can be downloaded at AI.NVIDIA.com. Ultra is coming soon.\nHybrid reasoning helps to advance agentic AI workloads\nADVERTISEMENT\nOne of the key features in Nvidia Llama Nemotron is the ability to toggle reasoning on or off.\nThe ability to toggle reasoning is an emerging capability in the AI market. Anthropic Claude 3.7 has a somewhat similar functionality, though that model is a closed proprietary model. In the open source space IBM Granite 3.2 also has a reasoning toggle that IBM refers to as – conditional reasoning.\nThe promise of hybrid or conditional reasoning is that it allows systems to bypass computationally expensive reasoning steps for simple queries. In a demonstration, Nvidia showed how the model could engage complex reasoning when solving a combinatorial problem but switch to direct response mode for simple factual queries.\nNvidia Agent AI-Q blueprint provides an enterprise integration layer\nRecognizing that models alone aren’t sufficient for enterprise deployment, Nvidia also  announced the Agent AI-Q blueprint, an open-source framework for connecting AI agents to enterprise systems and data sources.\n“AI-Q is a new blueprint that enables agents to query multiple data types—text, images, video—and leverage external tools like web search and other agents,” Briski said. “For teams of connected agents, the blueprint provides observability and transparency into agent activity, allowing developers to improve the system over time.”\nThe AI-Q blueprint is set to become available in April\nWhy this matters for enterprise AI adoption\nFor enterprises considering advanced AI agent deployments, Nvidia’s announcements address several key challenges.\nThe open nature of Llama Nemotron models allows businesses to deploy reasoning-capable AI within their own infrastructure. That’s important as it can address data sovereignty and privacy concerns that can have limited adoption of cloud-only solutions. By building the new models as NIMs, Nvidia is also making it easier for organizations to deploy and manage deployments, whether on-premises or in the cloud.\nThe hybrid, conditional reasoning approach is also important to note as it provides organizations with another option to choose from for this type of emerging capability. Hybrid reasoning allows enterprises to optimize for either thoroughness or speed, saving on latency and compute for simpler tasks while still enabling complex reasoning when needed.\nAs enterprise AI moves beyond simple applications to more complex reasoning tasks, Nvidia’s combined offering of efficient reasoning models and integration frameworks positions companies to deploy more sophisticated AI agents that can handle multi-step logical problems while maintaining deployment flexibility and cost efficiency.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Nvidia will supercharge humanoid robot development with Isaac GR00T N1 foundation model for human-like reasoning",
        "url": "https://venturebeat.com/games/nvidia-will-supercharge-humanoid-robot-development-with-isaac-gr00t-n1-foundation-model-for-humanoid-reasoning/",
        "content": "OK, this is not a drill. The robots are coming.\nNvidia announced a portfolio of technologies to supercharge humanoid robot development, including Nvidia Isaac GR00T N1, the world’s first open, fully customizable foundation model for generalized humanoid reasoning and skills.\nbringing_brands_&_celebrities_into_the_new_world_of_gaming (1080p).mp4\nThe other technologies include simulation frameworks and blueprints such as the Nvidia Isaac GR00T Blueprint for generating synthetic data, as well as Newton, an open-source physics engine — under development with Google DeepMind and Disney Research — purpose-built for developing robots.\nAvailable now, GR00T N1 is the first of a family of fully customizable models that Nvidia will pretrain and release to worldwide robotics developers — accelerating the transformation of industries challenged by global labor shortages estimated at more than 50 million people.\n“The age of generalist robotics is here,” said Jensen Huang, founder and CEO of Nvidia, in a statement. “With Nvidia Isaac GR00T N1 and new data-generation and robot-learning frameworks, robotics developers everywhere will open the next frontier in the age of AI.”\nThe company unveiled the news during Huang’s keynote speech at the GTC 2025 event.\n“This could be the biggest industry of all,” Huang said.\nHe noted the reinforcement learning and verifiable rewards (in the form of physics) will drive the robot technology forward.\n“We need a physics engine designed for fine-grained soft and rigid bodies,” he said. “We need it to be GPU accelerated so these virtuals can live in super linear time.”\nGR00T N1 Advances Humanoid Developer Community\nNvidia GR00T generates synthetic data for robots.\nThe GR00T N1 foundation model features a dual-system architecture, inspired by principles of human cognition. “System 1” is a fast-thinking action model, mirroring human reflexes or intuition. “System 2” is a slow-thinking model for deliberate, methodical decision-making.\nPowered by a vision language model, System 2 reasons about its environment and the instructions it has received to plan actions. System 1 then translates these plans into precise, continuous robot movements. System 1 is trained on human demonstration data and a massive amount of synthetic data generated by the Nvidia Omniverse platform.\nADVERTISEMENT\nGR00T N1 can easily generalize across common tasks — such as grasping, moving objects with one or both arms, and transferring items from one arm to another — or perform multistep tasks that require long context and combinations of general skills. These capabilities can be applied across use cases such as material handling, packaging and inspection.\nDevelopers and researchers can post-train GR00T N1 with real or synthetic data for their specific humanoid robot or task.\nIn his GTC keynote, Huang demonstrated 1X’s humanoid robot autonomously performing domestic tidying tasks using a post-trained policy built on GR00T N1. The robot’s autonomous capabilities are the result of an AI training collaboration between 1X and Nvidia.\n“The future of humanoids is about adaptability and learning,” said Bernt Børnich, CEO of 1X Technologies, in a statement. “Nvidia’s GR00T N1 model provides a major breakthrough for robot reasoning and skills. With a minimal amount of post-training data, we were able to fully deploy on NEO Gamma — furthering our mission of creating robots that are not tools, but companions that can assist humans in meaningful, immeasurable ways.”\nADVERTISEMENT\nAmong the additional leading humanoid developers worldwide with early access to GR00T N1 are Agility Robotics, Boston Dynamics, Mentee Robotics and Neura Robotics.\nNvidia, Google DeepMind and Disney Research Focus on Physics\nNvidia Isaac GR00T makes it easier to design humanoid robots.\nNvidia announced a collaboration with Google DeepMind and Disney Research to develop Newton, an open-source physics engine that lets robots learn how to handle complex tasks with greater precision.\nBuilt on the Nvidia Warp framework, Newton will be optimized for robot learning and compatible with simulation frameworks such as Google DeepMind’s MuJoCo and Nvidia Isaac Lab. Additionally, the three companies plan to enable Newton to use Disney’s physics engine.\nGoogle DeepMind and Nvidia are collaborating to develop MuJoCo-Warp, which is expected to accelerate robotics machine learning workloads by more than 70 times and will be available to developers through Google DeepMind’s MJX open-source library, as well as through Newton.\nDisney Research will be one of the first to use Newton to advance its robotic\ncharacter platform that powers next-generation entertainment robots, such as the\nexpressive Star Wars-inspired BDX droids that joined Huang on stage during his GTC\nkeynote.\n“The BDX droids are just the beginning. We’re committed to bringing more characters to life in ways the world hasn’t seen before, and this collaboration with Disney Research, Nvidia and Google DeepMind is a key part of that vision,” said Kyle Laughlin, senior vice president at Walt Disney Imagineering Research &\nDevelopment, in a statement. “This collaboration will allow us to create a new generation of robotic characters that are more expressive and engaging than ever before — and connect with our guests in ways that only Disney can.”\nNvidia and Disney Research, along with Intrinsic, announced an additional collaboration to build OpenUSD pipelines and best practices for robotics data workflows.\nMore Data to Advance Robotics Post-Training\nLarge, diverse, high-quality datasets are critical for robot development but costly to capture. For humanoids, real-world human demonstration data is limited by a person’s 24-hour day.\nAnnounced today, the Nvidia Isaac GR00T Blueprint for synthetic manipulation motion generation helps address this challenge. Built on Omniverse and Nvidia Cosmos Transfer world foundation models, the blueprint lets developers generate exponentially large amounts of synthetic motion data for manipulation tasks from a small number of human demonstrations.\nUsing the first components available for the blueprint, Nvidia generated 780,000 synthetic trajectories — the equivalent of 6,500 hours, or nine continuous months, of human demonstration data — in just 11 hours. Then, combining the synthetic data with real data, Nvidia improved GR00T N1’s performance by 40%, compared with using only real data.\nTo further equip the developer community with valuable training data, Nvidia is releasing the GR00T N1 dataset as part of a larger open-source physical AI dataset — also announced at GTC and now available on Hugging Face.\nAvailability\nNvidia Isaac Lab Project GR00T Models\nNvidia GR00T N1 training data and task evaluation scenarios are now available for download from Hugging Face and GitHub. The Nvidia Isaac GR00T Blueprint for synthetic manipulation motion generation is also now available as an interactive demo on build.nvidia.com or to download from GitHub.\nThe Nvidia DGX Spark personal AI supercomputer, also announced today at GTC, provides developers a turnkey system to expand GR00T N1’s capabilities for new robots, tasks and environments without extensive custom programming. The Newton physics engine is expected to be available later this year.\nAt GTC 2025, Nvidia will hold Humanoid Developer Day sessions, including:\n● “An Introduction to Building Humanoid Robots” for a deep dive into Nvidia Isaac GR00T;\n● “Insights Into Disney’s Robotic Character Platform” to learn how Disney Research redefines entertainment robotics with BDX droids;\n● “Announcing Mujoco-Warp and Newton: How Google DeepMind and Nvidia are Supercharging Robotics Development” for a deeper look into these new technologies and how Google deploys AI models to train AI-powered humanoids for real-world tasks.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "GPUs go biological: BBB unveils Bionode, lab-grown, living neuron compute for AI applications",
        "url": "https://venturebeat.com/ai/gpus-go-biological-bbb-unveils-bionode-lab-grown-living-neuron-compute-for-ai-applications/",
        "content": "Graphics processing units (GPUs), the expensive computer chips made by companies like Nvidia, AMD, and Sima.ai, are no longer the only way to train and deploy artificial intelligence.\nBiological Black Box (BBB), a Baltimore-founded startup developing a new class of AI hardware, has emerged from stealth with its Bionode platform—a computing system that integrates living, lab-grown neurons with traditional processors.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nThe company, which has been operating quietly while filing patents and refining its technology, believes its biological computing approach — growing new neurons specifically to act as computer chips using donor human stem cells and rat-derived cells — could offer a low-power, adaptive alternative to conventional GPUs.\n“Over the last 20 years, three independent fields—biology, hardware, and computational tools—have advanced to the point where biological computing is now possible,” said Alex Ksendzovsky, BBB’s co-founder and CEO, in a video call interview with VentureBeat.\nA member of Nvidia’s Inception incubator, BBB is positioning itself as an advancement and augmentation to the dominant silicon-based AI chips that Nvidia and others produce.\nBy leveraging neurons’ ability to physically rewire themselves, the company aims to reduce energy costs, improve processing efficiency, and accelerate AI model training—challenges that have become increasingly urgent as AI adoption expands.\nThis isn’t sci-fi, despite the incredible premise: BBB’s neural chips are already powering computer vision and LLMs for customers. The company has entered talks with two partners to license its tech for computer vision apps—though the company declined to name its customers and partners specifically, citing confidentiality agreements. It is also accepting inquiries from prospective partners and clients on its website.\nBlending biology and hardware\nAt the core of BBB’s approach is the Bionode platform, which uses lab-grown neurons wired into computing systems.\n“We have multiple models that we use,” Ksendzovsky told me. “One of those models is from rat cells. One of those models is from actually human stem cells that are converted into neurons.”\nADVERTISEMENT\nThe co-founder said that “hundreds of thousands of them” are integrated into a dish containing 4,096 electrodes, which forms the basis of one Bionode chip. He also said they live for over a year before needing to be replaced.\nThe idea is to harness neurons’ natural adaptability for AI processing, creating a hybrid computing system that differs fundamentally from today’s rigid, transistor-based chips.\nMicroscopic image of BBB neural compute cell with information flowing through it. Credit: BBB\nADVERTISEMENT\nKsendzovsky, who has been working with neurons on electrodes since 2005, originally considered using them to predict the stock market. His mentor, Steve Potter, dismissed the idea at the time.\n“Why aren’t we using neurons to predict the stock market so we can all be rich?” Ksendzovsky recalled asking Potter, who laughed it off as impractical. “At the time, he was right,” Ksendzovsky admitted.\nSince then, improvements in electrode technology, computational tools, and neuron longevity have made biological computing viable. “The biological network has evolved over hundreds of millions of years into the most efficient computing system ever created,” Ksendzovsky explained.\nThis setup offers two immediate advantages:\n• More Efficient Computer Vision: Bionode has been tested as a pre-processing layer for AI classification tasks, reducing both inference times and GPU power consumption.\n• Accelerated Large Language Model (LLM) Training: Unlike GPUs, which require frequent retraining cycles, neurons adapt on the fly. This could significantly reduce the time and energy needed to update large language models (LLMs), addressing a key bottleneck in AI scaling.\n“One of our biggest breakthroughs is using biological networks to train LLM’s more efficiently, reducing the massive energy consumption required today,” Ksendzovsky said.\nBuilding a viable, living GPU with Nvidia’s help\nNvidia’s GPUs have been instrumental in AI’s rapid advancement, but their high energy consumption and increasing cost have raised concerns about scalability.\nBBB sees an opportunity to introduce a more power-efficient alternative while still operating within Nvidia’s ecosystem.\n“We don’t see ourselves as direct competitors to Nvidia, at least in the near future,” Ksendzovsky noted. “Biological computing and silicon computing will coexist. We still need GPUs and CPUs to process the data coming from neurons.”\nIn fact, according to the co-founder, “we can use our biological networks to augment and improve silicon-based AI models, making them more accurate and more energy-efficient.”\nHe argued that the long-term vision for AI hardware will be a modular ecosystem in which biological computing, silicon chips, and even quantum computing each play a role.\n“The future of computing will be a modular ecosystem where traditional silicon, biological computing, and quantum computing each play a role based on their strengths,” he said.\nAlthough BBB has yet to disclose a commercial launch date, the company is relocating from Baltimore, Maryland, to the Bay Area as it prepares to scale its technology.\nThe future of hybrid AI processing\nWhile silicon-based GPUs remain the industry standard, BBB’s brain-on-a-chip concept presents a glimpse into a future where AI hardware is no longer limited to transistors and circuits.\nThe ability of neurons to reconfigure themselves dynamically could enable AI systems that are more energy-efficient, adaptive, and capable of continuous learning.\n“We’re already applying biological computing to computer vision. We can encode images into a biological network, let neurons process them, and then decode the neural response to improve classification accuracy,” Ksendzovsky said.\nBeyond efficiency gains, BBB also believes that its biological approach can provide deeper insight into how AI models process data.\n“We’ve built a closed-loop system that allows neurons to rewire themselves, increasing efficiency and accuracy for AI tasks,” he explained.\nDespite the potential, Ksendzovsky acknowledges that ethical considerations will be an ongoing discussion. BBB is already working with ethicists and regulatory experts to ensure its technology is developed responsibly.\n“We don’t need millions of neurons to process the entire environment like a brain does. We use only what’s necessary for specific tasks, keeping ethical considerations in mind,” he emphasized.\nBBB is betting that living tissue, not just silicon, could be the key to AI’s next leap forward.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Adobe’s new AI agents can make personal websites for your customers",
        "url": "https://venturebeat.com/ai/adobes-new-ai-agents-can-make-personal-websites-for-your-customers/",
        "content": "Adobe first made its mark in generative AI with its Firefly image generation model in 2023 and its generative fill feature on Photoshop.\nWith enterprise customers turning their attention from exploring AI-powered creation tools to agents, Adobe is throwing its hat in the agentic ring and adding more personalization features to everyday customer experience tasks. \nNavigating AI Regulations in Financial Services - AI Impact Tour 2024Navigating AI Regulations in Financial Services - AI Impact Tour 2024\nAdobe announced the launch of 10 agents and an orchestration tool on its Adobe Experience platform. These tools target specific needs such as customer channel engagement, content production, data management and site optimization. \nThe company also debuted Brand Concierge, a way for organizations to personalize their websites for customers based on their previous interactions with the brand. \nLoni Stark, vice president of strategy and product for Adobe, told VentureBeat in an interview that agents would change the customer experience for both the enterprise and their clients.\n“We see that agents can scale up the capacity of experience makers. It’s not just because of the hype out there, but because when we have delivered our tools to the customers we work with, we see that as their trust in the AI capabilities we deliver increases, they start to think, oh, can I make them autonomous,” Stark said. \nShe added the idea is to let these agents work ambiently, meaning the agents and the orchestrator continue to work in the background to provide information or solve issues for enterprises proactively. \nOrchestration and agents for customer experience\nThe new agents launching on AEP are:\nAccount qualification agent to evaluate new sales pipelines\nAudience agent, which analyzes cross-channel engagement data to \nContent production agent that helps marketers and creatives scale by generating and assembling content  \nData insights agent simplifies and expands the process of deriving insights from signals  \nData engineering agent  \nExperimentation agent helps stimulate new ideas and conduct impact analysis  \nJourney agent can orchestrate cross-channel experiences \nProduct advisor agent recommends experience and product engagement experiments  \nSite optimization agent manages and detects traffic and engagement in a website \nWorkflow optimization agent for cross-team collaboration and monitoring ongoing projects\nADVERTISEMENT\nStark highlighted the Site Optimization agent during a demo with VentureBeat. The agent would check for broken links or proactively examine a brand’s website for traffic and bounce rates and suggest fixes. \n“Most companies don’t have people that spend all of their days looking at broken links, for example, especially if they have tens of thousands of pages, or can’t check on these daily,” Stark said. “What’s happening is that there’s lost opportunity both if you think about the bounce rate. This agent is pre-trained, so out of the box, it already comes with skills like looking for broken backlinks.”\nStark said enterprises using the Experience Platform can fine-tune how much agents access their data through the orchestrator.\nADVERTISEMENT\nAdobe joins companies like Salesforce and ServiceNow in providing users with pre-built agents for specific tasks and teams.  Source: Adobe\nA customized brand website\nAnother new feature for the Adobe Experience Platform is Brand Concierge, which will help enterprises build websites that offer customized customer visits. Organizations can create a website for their company or product that greets customers by name and provides a query box asking them what information they want. \nSource: Adobe\nSay a company has a website for a hotel chain. A customer can ask the chat function or click on premade prompts to ask about amenities specific to one location, Brand Concierge helps the company push the appropriate information to the front page of the site and also customize all other assets and experiences to that location. Stark said customers can still browse the site as usual, but Brand Concierge pushes customer engagement further by remembering how particular customers have interacted with the enterprise before. \nBrand Concierge is a separate offering from the agents that sits on top of the AEP, but Stark said, “It’ll leverage agents such as the Product Advisor Agent, which is already built into the Concierge app.” The company also understands its customers’ past interactions and preferences. \nStark said Adobe customers increasingly find their clients more comfortable using AI chatbots, making it easier to transition them to more personalized, prompt-based website experiences. \n“I think what we’re seeing is that consumers are increasingly comfortable with an AI-powered conversational experience. New Adobe Analytics data shows a 1,200% surge in U.S. retail sites and a 1,700% surge in U.S. travel sites (July 2024 to Feb. 2025) from generative AI sources. Companies can surface this on high-traffic properties (like their website) with an increasingly familiar form factor that is gaining traction,” Stark said. \nThe company launched the Adobe Experience Platform in 2019, but the real-time customer experience management solution saw a massive update last year, including an AI assistant for users.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Nvidia partners with telecom leaders to develop AI-native 6G wireless networks",
        "url": "https://venturebeat.com/games/nvidia-partners-with-telecom-leaders-to-develop-ai-native-6g-wireless-networks/",
        "content": "Nvidia said 6G wireless networks are coming, with help from artificial intelligence processing.\nAt GTC 2025, Nvidia today unveiled partnerships with industry leaders T-Mobile, Mitre, Cisco, ODC, a portfolio company of Cereberus Capital Management, and Booz Allen Hamilton on the research and development of AI-native wireless network hardware, software and architecture for 6G.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nNext-generation wireless networks must be fundamentally integrated with AI to seamlessly connect hundreds of billions of phones, sensors, cameras, robots and autonomous vehicles. AI-native wireless networks will provide enhanced services for billions of users and set new standards in spectral efficiency — the rate at which data can be transmitted over a given bandwidth.\nThey will also offer groundbreaking performance and resource utilization while creating new revenue\nstreams for telecommunications companies.\n“Next-generation wireless networks will be revolutionary, and we have an unprecedented opportunity to ensure AI is woven in from the start,” said Jensen Huang, founder and CEO of Nvidia, in a statement. “Working with leaders in the field, we’re building an AI-enhanced 6G network that achieves extreme spectral efficiency.”\nIn his speech, Huang added, “The next generation of radio networks will have AI deeply inserted into it.”\nOpen ecosystems drive innovation\nResearch-driven breakthroughs harnessing the power of AI are necessary to maximize the performance and benefits of AI-native wireless networks. To drive innovation, Nvidia is collaborating with telco and research leaders to develop an AI native wireless network stack based on the Nvidia AI Aerial platform, which provides software-defined radio access networks on the Nvidia accelerated computing platform.\nDevelopers across the globe are building AI-RAN as a precursor to AI-native 6G wireless networks. AI-RAN is a technology that brings AI and RAN workloads together on one platform and embeds AI into radio signal processing.\nTo deliver enhanced spectral efficiency and lower operational complexity and costs, AI will be fully embedded into the network stack’s software and hosted over a unified accelerated infrastructure, capable of running both network and AI workloads. Also at the solution’s core will be end-to-end security and an open architecture to foster rapid innovation.\nT-Mobile and Nvidia will expand their AI-RAN Innovation Center collaboration announced last year with the goal of providing additional research-based concepts for AI-native 6G network capabilities, working alongside these new industry collaborators.\nADVERTISEMENT\n“This is an exciting next step to the AI-RAN Innovation Center efforts we began last September at our Capital Markets Day in partnership with Nvidia,” Mike Sievert, CEO of T-Mobile. “Working with these additional industry leaders on research to natively integrate AI into the network as we begin the journey to 6G will enable the network performance, efficiency and scale to power the next generation of\nexperiences that customers and businesses expect.”\nIn a press briefing, Nvidia’s Ronnie Vasishta said 95% of telecom companies are engaging with Nvidia. And he noted 6G networks would likely have hundreds of billions of devices connected to them.\nADVERTISEMENT\n“AI is set to transform wireless networks. The countdown to 6G has already begun. Funding and research has moved to next gen cycle. The opportunity is to see AI is woven in from the start. We believe the next generation will be AI native,” he said.\nAs the founding research partner, Mitre, a not-for-profit research and development organization, will research, prototype and contribute open, AI-driven services and applications, such as for agentic network orchestration and security, dynamic spectrum sharing and 6G-integrated sensing and communications.\n“Mitre is working with Nvidia to help make AI-native 6G a reality,” said Mark Peters, president and CEO of MITRE. “By integrating AI into 6G in the beginning, we can solve a wide range of problems, from enhancing service delivery to unlocking required spectrum availability to fuel wireless growth. Through all of our collaborations with Nvidia, we look forward to creating impact in 6G, AI, simulation, transportation and more.”\nCisco plans to take a lead position in this collaboration as the provider of mobile core and network technologies and will tap into its existing service provider reach and expertise.\n“With 6G on the horizon, it’s critical for the industry to work together to build AI\nnative networks for the future,” said Chuck Robbins, chair and CEO of Cisco. “Cisco is at the forefront of developing secure infrastructure technology for AI, and we are proud to work with NVIDIA and the broader ecosystem to create an AI-enhanced network that improves performance, reliability and security for our customers.”\nODC, a portfolio company of Cerberus Capital Management, L.P., will deliver cutting-edge layer 2 and layer 3 software for distributed and centralized units of virtual RAN as part of the AI-native radio access stack. Tapping into decades of experience in large-scale mobile systems, ODC is pioneering next-generation AI native 5G open RAN (ORAN), surpassing existing networks and seamlessly paving\nthe way for 6G evolution.\n“The mobile industry has always taken advantage of advances in other technology fields, and today, no technology is more central than AI,” said Shaygan Kheradpir, chairman of the advisory board of ODC, in a statement. “ODC is at the forefront of developing and deploying AI-native ORAN 2.0 networks, enabling service providers to on-ramp seamlessly from 5G to 6G by taking advantage of the vast AI ecosystem to redefine the future of connectivity.”\nAs a leader in AI and cybersecurity to the federal government, Booz Allen will develop AI RAN algorithms and secure the AI-native 6G wireless platform. Its NextG lab will conduct functional, performance integration and security testing to ensure the resiliency and security of the platform against the most sophisticated adversaries. The company will lead field trials for advanced use cases such as autonomy and robotics.\n“The future of wireless communications starts today, and it’s all about AI,” said Horacio Rozanski, chairman and CEO of Booz Allen. “Booz Allen has the technologies to make AI-native 6G networks a reality and revolutionize secure communications for an entirely new generation of intelligent platforms and applications.”\nExpanded aerial research portfolio\nThese collaborations build on Nvidia’s AI-RAN and 6G research ecosystem, supported by advancements in the NVIDIA Aerial™ research portfolio for developing, training, simulating and deploying groundbreaking AI-native wireless innovations.\nNew additions to the Nvidia Aerial Research portfolio, also announced today, include the Aerial Omniverse Digital Twin Service, the Aerial Commercial Test Bed on Nvidia MGX, Nvidia Sionna 1.0 — building on the open-source Sionna library, which has nearly 150,000 downloads since its launch in 2022 — and the Sionna Research Kit on the Nvidia Jetson accelerated computing platform.\nThe Nvidia Aerial Research portfolio serves over 2,000 members through the Nvidia 6G Developer Program. Industry leaders and more than 150 higher education and research institutions from the U.S. and around the world are harnessing the platform to accelerate 6G and AI-RAN innovation — paving the way for AI-native wireless networks.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Why smarter ERP data is the key to AI-powered growth",
        "url": "https://venturebeat.com/ai/why-smarter-erp-data-is-the-key-to-ai-powered-growth/",
        "content": "Presented by SAP\nSometimes two contradictory forces can be true at the same time.\nIn the past 20 years, we’ve seen an explosion of innovative new technologies for businesses but slowing business productivity growth. Between 1995 and 2005, productivity in the U.S. grew at 2.6%. But in the decades since 2005, it grew at just 1.4% — a drop of nearly 50%, according to McKinsey. And this happened during a time when the internet matured, smartphones and mobile apps became pervasive and there were big advancements in ERP technologies.\nIt is clear from the data that innovation alone doesn’t lead to broad-based productivity gains. And that is a problem because when businesses are more productive, everyone benefits. This is especially important to take note of right now as new data and business AI innovations are reaching the market with breakneck speed. But these innovations are also bringing more complexity. What really matters is how you use data and AI together. Clean and meaningful data is the foundation for relevant AI. At the center of this is ERP data.\nWhy is ERP data so valuable? Because it is structured, describes processes, and has a sematic meaning.\nExamples include purchase orders, invoices, financial postings in a general ledger and supplier delivery schedules. ERP data has business semantics that connects individual data sets and that’s what makes it possible to truly understand how businesses run. When you add in industry-specific data — like energy prices, interest rates and consumer trends and preferences, it becomes even richer. Considering that over 80% of the business data generated in the world runs through an SAP system, this is knowledge that is unique to SAP. We know how businesses run best across any industry and geography.\nEmerging AI applications as a force multiplier\nThe cloud has changed the game as how this data is accessible. On top of this data, a new class of AI-driven applications can be built that is relevant to a business and their industry and is useful through an intuitive user interface. That’s what creates a force multiplier for business productivity and business value. That’s exactly what SAP is focused on right now.\nIn the past few weeks, SAP has announced a series of data and AI innovations to make businesses more productive. This includes Joule for Developers, which is an AI-powered assistant that understands SAP’s development framework and empowers developers of all skill levels to be more productive, creative and proficient in accelerating ABAP, Java and JavaScript-based application development and automation. Last month, we announced our Business Data Cloud, as our solution to unify all SAP and third-party data for our customers, providing the trusted data foundation organizations need to make more impactful decisions. In combination with our partner DataBricks, BDC combines structured data with unstructured data such as customer feedback, industry trends, etc., leading to better decision making.\nUnifying AI across the organization\nThe source that is generating the necessary operational data is the SAP Business Suite. It represents a comprehensive set of integrated applications that seamlessly connects every part of their business. And that really speaks to the elegance of the SAP system — that everything works together seamlessly and provides customers a 360 view on their business.\nThink of it like an orchestra. Instead of individual musicians playing different instruments, an orchestra creates music and harmony because they play seamlessly together. In other words, the whole becomes greater than the sum of its parts. That’s what the SAP Business Suite does for business data.\nA great example is KIND, which is the health and wellness division of Mars, Inc. Their recent growth created lots of complexity. They needed to rethink their decentralized ways of working. So they implemented SAP S/4HANA Cloud Public Edition as their new business technology foundation while partnering with implementation experts from Accenture. As a result, they can better access and leverage their ERP data in the public cloud, leading to operational efficiencies and better use of analytics to drive better decisions for efficient business operations.\nIt’s clear that the next wave of AI is agentic — agents or services that come as software. For example, think of a service to enter and process travel expenses. What is still needed is software that stores the data in a structured way, a system of record, etc. But what is changing is the user experience. In the case of SAP, our UX is Joule and it is becoming the orchestrator of services.\nThere isn’t a single business technology conversation today that doesn’t start and end with the impact of data and AI. But just like an orchestra, it can either be noise or music depending on how it all comes together. Companies that embrace an active approach of data management supercharged by generative AI will stay ahead of what’s coming next and create a blueprint for a new era of business productivity growth.\nJan Gilg is Chief Revenue Officer, Americas and SAP Global Business Suite.\nSponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact sales@venturebeat.com."
    },
    {
        "title": "Orion Security emerges from stealth using LLMs to track your enterprise’s data flow and stop it from leaking out",
        "url": "https://venturebeat.com/security/orion-security-emerges-from-stealth-using-llms-to-track-your-enterprises-data-flow-and-stop-it-from-leaking-out/",
        "content": "If you pay attention at all to cybersecurity news, there’s a strong chance you’ve heard scary reports of firms hiring remote contractors that turn out to be hackers or North Korean spies making off with sensitive, proprietary data.\nBut even without that cloak-and-dagger, international espionage veneer, the truth is that all organizations have reasons to be concerned about their data security and the prospect of “exfiltration,” or the movement of data without authorization. IBM’s 2024 Cost of a Data Breach Report found that incidents involving data exfiltration are now on the rise extortion now average around $5.21 million per incident.\nNavigating AI Regulations in Financial Services - AI Impact Tour 2024\nCredit: IBM, “Cost of a Data Breach Report 2024”\nIn an age when data has never been more important or valuable to an organization — yet is also moving around between siloes more than ever before — how can enterprises best protect their sensitive information without breaking the bank?\nA new firm, Orion Security, believes generative AI large language models are the key. Today, the company announced its emergence from stealth with $6 million in seed funding led by Pico Partners and FXP, with participation from Underscore VC and prominent cybersecurity leaders, such as the founders of Perimeter 81 and the CISO of Elastic.\nOrion Security, founded by Nitay Milner (CEO) and Yonatan Kreiner (CTO), is already working with leading technology companies to help them safeguard sensitive business data from insider threats, according to an interview VentureBeat conducted with Milner over video call last week.\nOrion’s co-founders Nitay Milner (CEO) and Yonatan Kreiner (CTO).\nADVERTISEMENT\n“I spent a lot of years as a product leader in several companies solving very complicated challenges around observability and security in cloud environments, helping T-Mobile and BlackRock to get ahold of, and better understand, their very complex system stacks,” Milner said. “I experienced firsthand that the main problem in data security is understanding the business context of how sensitive data is being used in a company.”\nAI-powered Contextual Data Protection (AI CDP)\nUnlike traditional data protection tools that rely on rigid rules and manual policies, Orion Security’s platform dynamically learns and maps an organization’s business processes.\nBy understanding how data typically moves within an organization, Orion can distinguish between legitimate workflows and potential threats, whether intentional or accidental.\nADVERTISEMENT\n“Orion revolutionizes data protection by understanding business processes and data flows in the company and automating data loss prevention with the power of AI,” Milner explains.\nThis approach is a departure from conventional manual policy-based security models, which Milner believes are fundamentally flawed.\n“Most security solutions rely on manual policies, but policies don’t scale. There are new applications and workflows that make them obsolete pretty often.”\nHe further emphasized how security teams struggle with outdated methods: “Security teams are stuck writing endless policies over and over again, getting hit by false positives, and still, data keeps leaking from enterprises. It’s a really bad situation.”\nOrion Security employs a combination of proprietary AI models and fine-tuned open-source LLMs to automate data protection.\n“All our AI is something that we developed… we’re not using a third party, like ChatGPT or something like that. We developed our AI internally, so it’s all our IP,” he told VentureBeat.\nThe platform relies on two core models: one for classification, which identifies how sensitive data is based on context, and another for business reasoning, which assesses user roles, workflows, and typical data movement to detect anomalies.\nOrion’s AI is further fine-tuned on industry-specific and organization-specific data to improve accuracy, ensuring it adapts to each company’s unique operations.\nWhile they leverage fine-tuned open-source LLMs, Milner notes their surprising effectiveness even without extensive pre-training, saying, “LLMs that are open source… have a lot of context, and you wouldn’t believe the level they give you just by throwing sensitive data on them.”\nHow Orion’s solution works\nThe platform connects to an organization’s cloud services, browsers, and devices to map data flows comprehensively.\nAt the core of its detection capabilities is its Indicators of Leakage (IOL) engine, which leverages proprietary reasoning models and large language model (LLM) classification to analyze data movement patterns.\nKey features include:\nReal-time risk assessment: The platform continuously evaluates business processes, assigning risk scores based on observed behavior.\nSensitive data detection: Orion identifies and classifies data types, including personally identifiable information (PII), trade secrets, payroll details, and intellectual property (IP).\nMinimal manual configuration: Unlike traditional DLP tools that require extensive setup, Orion automates detection and response with minimal user intervention.\nReduced false alerts: By incorporating business context, Orion ensures that security teams are only alerted to genuinely suspicious activity, cutting down on noise and unnecessary investigations.\nMilner compares Orion’s approach to endpoint detection and response (EDR) solutions, but for data protection. “We act as an EDR for data—think of it like a CrowdStrike for your data. If something anomalous happens, we catch and prevent it in real-time, even if there wasn’t a predefined policy.”\nBeyond catching malicious insiders, Orion also distinguishes between human errors and external attackers. “The three main vectors for data leaks are malicious insiders, human errors, and external attackers. We detect and differentiate between all of them,” Milner says.\nEnterprise leaders can see the flow of their firm’s data at a glance\nOrion Security provides users with a dashboard-driven experience, offering real-time insights into business data flows. The interface categorizes risk by severity, allowing security teams to quickly identify and address high-risk activities.\nSome notable elements of Orion’s UI include:\nTop Data Types Monitored: The system classifies and tracks PII, marketing materials, product-related data, and source code.\nRisk Score Distribution: A visual breakdown of critical, high, medium, and low-risk activities helps prioritize security responses.\nTop Outbound Sources: Displays the most common platforms where data is being transferred, helping security teams detect unusual exfiltration patterns.\nBusiness Flow Risk Scores: Each monitored business process is assigned a risk score, with specific activities (e.g., “Engineering teams moving data before leaving the company”) flagged based on severity.\nThis intuitive approach to data security allows security teams to quickly assess potential threats and take immediate action when necessary.\nMilner described the platform’s visibility capabilities thusly: “Imagine having a dynamic map of all the sensitive data movement in your company—between people, devices, and applications—and making sure it doesn’t leave your organization.”\nHigh investor confidence\nBacking from cybersecurity veterans further reinforces Orion’s approach. Gil Zimmermann, Partner at FXP, who previously co-founded CloudLock (acquired by Cisco), sees Orion’s technology as a long-overdue evolution in data protection:\n“AI is creating a watershed moment for data protection, and Orion Security is at the forefront of this transformation,” he wrote in a prepared statement in a press release provided to VentureBeat. “Orion’s AI-powered approach solves the core challenges we faced for years — the lack of business context and overwhelming manual work. This is the future of data security we envisioned but which couldn’t be built a decade ago.”\nBeyond detection, Orion offers flexibility in response mechanisms, letting companies customize their approach.\n“Some companies want us to block data exfiltration in real-time, while others prefer just getting notifications or educating employees on security policies. We let them decide how aggressive the approach should be,” Milner said.\nWhat’s next for Orion Security and its tech?\nOrion Security is already working with leading technology companies (confidential due to business agreements) and plans to further refine its AI models to stay ahead of evolving insider threats.\nThe company’s onboarding process ensures customers see immediate value. “We take three months of historical data when onboarding a new customer, so our AI delivers value from day one,” Milner explains.\nAdditionally, Orion emphasizes privacy-first security architecture. “We don’t store any sensitive data—only metadata. If a company prefers, they can even install our classifier in their own environment so nothing leaves their systems,” Milner says.\nWith an AI-driven approach that reduces manual workload, false positives, and security blind spots, Orion Security is well-positioned to shape the next generation of context-aware data protection solutions.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Halliday raises $20 million to build AI agents that operate safely on blockchain",
        "url": "https://venturebeat.com/ai/halliday-raises-20-million-to-build-ai-agents-that-operate-safely-on-blockchain/",
        "content": "Halliday has raised $20 million in Series A funding to develop AI agents that can safely operate on blockchain networks without requiring traditional smart contract development. The funding round, led by Andreessen Horowitz’s crypto arm (a16z crypto), brings the company’s total funding to over $26 million.\nThe investment signals growing confidence in Halliday’s approach to solving one of AI’s thorniest challenges: safely deploying autonomous agents in decentralized environments where mistakes can be costly and irreversible.\nNavigating AI Regulations in Financial Services - AI Impact Tour 2024\n“AI on blockchain has remained inaccessible due to compliance and safety concerns,” said Griffin Dunaif, CEO of Halliday, in an exclusive interview with VentureBeat. “Typically with smart contract development for AI agents, any minor mistake can cause a breach, leaving them vulnerable. For AI to operate onchain, there needs to be robust safety infrastructure where businesses have oversight of AI-enabled automations.”\nHow Halliday’s safety protocol makes AI agents viable for enterprise blockchain applications\nHalliday’s core innovation, the Agentic Workflow Protocol (AWP), creates what the company calls “immutable guardrails” for AI agents operating on blockchain networks. These guardrails ensure that autonomous systems can execute tasks within strictly defined parameters without the risk of exploits or unintended actions.\n“We’ve built the proper tooling with our workflow protocol to make safe AI possible with immutable guardrails onchain,” explained Dunaif. “The protocol ensures that no party, not even Halliday, can circumvent the guardrails placed on tasks.”\nThis approach could solve a fundamental challenge in AI development: how to allow autonomous agents to interact with financial systems and digital assets while maintaining security and compliance. By embedding safety constraints directly into the protocol layer, Halliday aims to make AI agents trustworthy enough for enterprise applications.\nAI-powered treasury management and B2B automation already in production\nADVERTISEMENT\nUnlike many AI startups announcing funding before their technology reaches production, Halliday has already deployed its AI-enabled workflow engine with several high-profile partners, including DeFi Kingdoms, Core Wallet by Ava Labs, and ApeChain.\nThe company’s workflows enable AI agents to automate complex, multi-step processes like treasury management, recurring payments, and business-to-business transactions. These autonomous systems can operate across multiple blockchain networks while maintaining compliance with predefined rules.\n“Halliday handles all the heavy lifting: protocol integrations like bridges and DEXs, data translation, and reliable multi-chain execution,” said Dunaif. “With workflows, teams can create breakthrough applications in hours, not years.”\nEnterprise AI adoption accelerated by removing blockchain programming barriers\nADVERTISEMENT\nFor enterprise customers, Halliday’s technology offers a path to leveraging blockchain networks and AI agents without specialized development teams or the security risks typically associated with smart contracts.\nThis could dramatically accelerate adoption of AI in financial services, where institutions have shown interest in autonomous systems but have been deterred by security and compliance concerns. According to Dunaif, Halliday offers a “10,000x development cost reduction,” potentially transforming AI-blockchain integration from a capital-intensive project to an operational expense.\n“There’s opportunity to expand beyond web3 and bring our Workflow Protocol to fintechs and banks looking to enter the crypto arena, whether through stablecoin subscriptions, global yield products, or programmable treasury management,” Dunaif noted.\nWill safe AI agents unlock the next wave of enterprise blockchain adoption?\nThe intersection of AI and blockchain has long promised powerful new business models. Autonomous agents that can execute complex financial transactions without human intervention could dramatically reduce costs and enable new services. However, the risk of rogue AI agents or exploitable code has limited enterprise adoption.\nHalliday’s approach aims to solve this problem by creating a middleware layer that handles security and execution while allowing businesses to focus on defining the logic of their AI agents. This separation of concerns could make AI-blockchain integration more accessible to mainstream enterprises.\n“We’re extending Stripe’s technology to be utilized by developers who are creating their own blockchains,” explained Dunaif when discussing how Halliday works with traditional financial infrastructure.\nThe future of enterprise AI: Automated workflows replace manual processes\nWith the new funding, Halliday plans to accelerate development of its AI workflow protocol and expand its team. The company has already attracted talent from leading technology companies, including Alphabet, Meta, Netflix, Stripe, and Compound.\nHalliday Payments, a first-party application built on the protocol, demonstrates how AI agents can simplify complex processes. It offers an end-to-end payments solution that uses AI to handle the complexities of blockchain transactions, lowering the barrier for new users.\n“By safely delegating workflows to autonomous systems, such as agents or software, teams can create breakthrough applications in hours, not years,” Dunaif said, describing his vision for how AI will transform enterprise operations.\nAs businesses continue to explore AI’s potential to automate complex workflows, solutions like Halliday’s that address the fundamental challenges of safety and compliance could play a crucial role in bringing these technologies to mainstream adoption.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Muse unveils wearable that combines brainwave tracking and brain oxygen measurements",
        "url": "https://venturebeat.com/games/muse-unveils-wearable-that-combines-brainwave-tracking-and-brain-oxygen-measurements/",
        "content": "Muse by Interaxon, a neurotechnology startup, unveiled Muse S Athena, a consumer wearable that can deliver advanced cognitive fitness insights. It does so by combining two powerful brain sensors: Electroencephalogram (EEG) to track brain activity and Functional Near-Infrared Spectroscopy (fNIRS), a technology that measures oxygen levels in the brain’s frontal cortex.\nAthena transforms real-time brain activity into actionable insights, personalized training, and measurable progress, the company said. It’s powered by Muse’s AI-driven Foundational Brain Model (FBM), which is trained on over 80,000 sessions from the world’s largest EEG database.\n212_Does marketing D2C work outside your mobile game.mp4\nExpanding beyond sleep and meditation, Athena marks Toronto, Canada-based Muse’s transformation into a full-spectrum brain health brand, empowering users to strengthen their minds just as they do their bodies.\nBy integrating dual-brain sensor neurofeedback with AI-powered analysis, Athena measures and trains mental strength, clarity and recovery by analyzing brain activity and oxygenation levels in real time. This breakthrough redefines mental fitness and cognitive health, giving users the ability to optimize focus, endurance, and resilience while proactively addressing cognitive decline.\nWith Athena, users train their minds by wearing a lightweight sensor-equipped headband and engaging with real-time neurofeedback in the Muse App. In addition to the classic Muse neurofeedback experiences using EEG alone, Athena users will now have access to a new challenge: control an owl’s flight using only mental effort.\nUnlike previous models, Athena supports both eyes-open and eyes-closed training, helping users build cognitive endurance in dynamic tasks or deepen focus in meditation. As effort increases, the owl soars; as mental strain decreases (measured by brain blood oxygenation levels), it slows—providing real-time biofeedback to strengthen focus, resilience, and concentration.\nBringing advanced brain training to everyday life\nMuse is a headband with sensors.\nWith dementia affecting six million Americans and projected to double by 2060, the need for proactive brain health solutions has never been greater. Research shows that lifestyle, habits, and early intervention can help slow or prevent cognitive decline, yet most people focus primarily on physical health when it comes to aging. It measures:\nADVERTISEMENT\nEEG (The Skill of the Mind): Tracks brainwave activity and measures how the mind shifts between brain states like focus or relaxation.\nfNIRS (The Strength and Endurance of Mental Fitness): Monitors brain oxygenation, showing how well the brain is fueled to handle stress, sustain attention, and recovery.\n“True health isn’t just physical—it starts with your brain.” said Jean-Michel Fournier, CEO of Interaxon, in a statement. “Athena is designed to tackle the growing challenge of cognitive decline while marking Muse’s evolution from meditation to full-spectrum brain health. Just as physical fitness requires strength, endurance, and skill, mental fitness demands the same targeted training. Athena provides the tools to enhance cognitive performance, build resilience, and support lifelong brain health.”\nADVERTISEMENT\nAthena’s dual-brain sensor technology unlocks a deeper, multidimensional view of mental effort and resilience—surpassing what EEG or fNIRS alone can measure. Powered by AI-driven insights from Muse’s Foundational Brain Model, Athena personalizes neurofeedback training with unmatched precision, making advanced brain health insights more accessible than ever.\nHow EEG + fNIRS is advancing neurotechnology:\nMuse will help address the rise of dementia.\nAI-powered holistic insights: EEG captures rapid neural oscillations, while fNIRS tracks cerebral blood oxygenation. Together with Muse’s Foundational Brain Model, they provide a holistic understanding of mental effort, focus, and resilience.\nTargeted cognitive training: fNIRS enables precise monitoring of the prefrontal cortex, a region critical for executive function, allowing Athena to guide interventions that improve memory, attention, and problem-solving skills.\nNeurovascular coupling & accuracy: By linking electrical activity (EEG) and blood flow (fNIRS), Athena offers deeper insights into how the brain adapts during cognitive tasks, enhancing training programs and neuroplasticity.\nAthena’s impact also extends beyond just personal use:\nWorkplace performance (Neuroergonomics): Helps optimize mental load and productivity in high-pressure environments.\nSleep health & diagnostics: Muse S is the most accurate sleep tracker outside of the lab and now can monitor brain oxygenation and SpO₂ levels more effectively than wrist or finger measurements to detect early signs of sleep apnea.\nCognitive aging research: Supports interventions for age-related cognitive decline with real-time brain data.\nMuse is a new kind of consumer sleep wearable.\n“Muse headbands are becoming a game changer in the landscape of sleep wearables. Their accessibility and ease of use, combined with remote access to both raw and scored EEG data after each recording, open up exciting new opportunities to explore the sleeping brain at scale in the field,” said Rebecca Robillard, a sleep researcher with the Canadian Sleep Research Consortium, in a statement. “I can’t wait to see the upcoming science and clinical applications that will leverage the new fNIRS and SpO₂ features of Athena.”\nAthena makes research-grade neuroscience accessible and actionable, setting a new standard for brain health by bridging cutting-edge research with everyday cognitive fitness. Muse S “Athena” is now available at choosemuse.com for $475.\nMuse’s AI-driven neurotech tools empower researchers, developers, and consumers worldwide, and are underpinned by over 200 third-party-led research studies from renowned institutions including the Mayo Clinic, MIT, and Harvard. Muse has collected and decoded over a billion minutes of brain data to date, comprising one of the largest EEG collections in the world.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Minecraft Education launches replica of St. Peter’s Basilica in the Vatican",
        "url": "https://venturebeat.com/games/minecraft-education-launches-replica-of-st-peters-basilica-in-the-vatican/",
        "content": "For centuries, St. Peter’s Basilica in the Vatican has stood as one of the world’s most awe-inspiring landmarks—home to breathtaking art, architecture, and stories that have shaped history.\nNow, for the first time ever, students can step inside this legendary building and take on the role of its caretakers in Peter Is Here: AI for Cultural Heritage, a brand-new Minecraft Education learning experience created in collaboration with Microsoft and the Vatican. Part of the hands-on restoration used AI.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nPeter is Here invites students to explore St. Peter’s Basilica through interactive gameplay that blends history, artificial intelligence (AI) tools, and creative problem-solving. Learners will travel through time to uncover the rich history of St. Peter’s Basilica while taking on hands-on restoration tasks as part of the Sanpietrini, the dedicated team responsible for maintaining St. Peter’s.\nThey’ll meet historical figures, investigate key artifacts, and use simulated AI tools. Educators can bring this world into the classroom with supplementary materials that explain the history of St. Peter’s Basilica and how it’s being digitally preserved with the help of AI. Whether repairing ancient mosaics, reinforcing Renaissance architecture, or ensuring Bernini’s iconic Baldachin stands tall, students will actively participate in the preservation of cultural heritage— all within the blocky world of Minecraft.\nThis project shows us how technology—especially AI—can help us keep historic places intact for future generations. This Minecraft experience is part of a collaboration between the Vatican, Iconem, and Microsoft to make St. Peter’s Basilica accessible to anyone, preserving the iconic church using digital technology.\nThe project started with the creation of an AI-enhanced digital twin using advanced photogrammetry techniques and AI. Microsoft’s AI for Good Lab supplied the tech needed to process and analyze the photogrammetry data they collected. To extend this immersive learning experience into classrooms, Microsoft and the Vatican teamed up with Minecraft Education to create Peter Is Here.\nHands-On Restoration Using AI\nPeter Is Here offers two distinct gameplay paths, making it perfect for educators looking to integrate history, art, STEM, and digital literacy into their lessons. They begin in ‘Restoration Mode’ where they take on the role of the Sanpietrini, the dedicated team responsible for preserving St. Peter’s Basilica.\nBy beginning with restoration, students gain a hands-on understanding of cultural preservation, then see how these efforts shape St. Peter’s Basilica as they venture through its grand halls. This learning path challenges players to repair key sections of the Basilica across four distinct historical eras:\nADVERTISEMENT\nThe Vatican Obelisk (75 AD)\nSt. Peter’s Tomb (400 AD)\nThe Rotunda Column (1546 AD)\nBernini’s Baldachin (1626 AD)\nIn Restoration Mode, students actively restore St. Peter’s Basilica using AI tools, making real-time decisions that provide historical insights and unlock new areas for exploration, transforming them into active preservers of cultural heritage.\nHands-On Problem Solving: Students use simulated AI tools like block scanners and\nretrievers to assess damage and identify which parts of the structure need repair.\nReal-Time Decision Making: Every restoration decision has a visible impact on the\nBasilica’s appearance. As players replace broken blocks or reinforce aging structures,\nthey see firsthand how their actions help restore this historic landmark.\nHistorical Context: Each restoration task is set within a specific era, providing insights\ninto the architectural styles and construction methods of the time. This context enriches\nthe restoration process and highlights the evolution of cultural heritage over centuries.\nUnlocking Exploration: Once a restoration task is completed, it not only revitalizes the\nBasilica but also unlocks further areas for exploration. This seamless transition from\nrestoration to discovery reinforces the connection between preserving the past and\nengaging with its history.\nExploring History in an Immersive World\nAfter completing restoration, students can enter the ‘Exploration Mode’ path where they become inquisitive historians exploring the grand halls and hidden secrets of the fully restored St. Peter’s Basilica. Students navigate the building using tools like the Smart Compass to fast-travel and set waypoints, engaging with interactive collectibles and in-world NPCs that reveal detailed historical narratives. Their journey transforms them into active historians and curious explorers.\nADVERTISEMENT\nImmersive Discovery: Students roam freely, interacting with intricate architectural\ndetails and unlocking secret areas that reveal hidden artifacts and cultural insights.\nInteractive Encounters: Engaging dialogues with figures like Michelangelo and Bernini\nprovide contextual perspectives that bring the Basilica’s past to life.\nContextual Learning: Each explored area reflects the architectural styles and cultural\ninfluences of its era, deepening students’ understanding of the building’s evolution.\nUnlocking Secrets: As players gather collectibles and interact with the environment,\nadditional areas become accessible, seamlessly connecting restoration with discovery.\nStudents are invited to document their learnings using the Book and Quill tool in Minecraft Education, draw parallels with other historic landmarks, and share their findings with peers, ultimately transforming them from passive observers into active protectors of cultural heritage.\nBringing Peter Is Here to Your Classroom\nThe Peter is Here lesson and immersive world connect social studies with modern technology. Students explore simulated AI tools like the scanner that mirror real-world digital imaging and 3D modeling techniques used by preservationists and in fields such as medicine, environmental science, and urban planning.\nIn addition to the in-game experience, educators can download a classroom-ready PowerPoint for structured instruction and student workbooks that deepen engagement and contextual understanding of each restoration site. These resources ensure educators can seamlessly integrate historical insights, AI concepts, and reflective prompts into their lesson—making every step of the Basilica’s restoration meaningful and aligned with classroom objectives.\nUpon completing the entire journey, students receive an official certificate celebrating their\naccomplishments, underscoring the real-world impact of their newly acquired skills and recognizing them as honorary preservers of cultural heritage.\nBy placing restoration work at the heart of Peter is Here, students become active stewards and conservationists, offering a glimpse into our shared responsibility to safeguard cultural treasures and the opportunity of AI tools to support this ongoing work. Watch your students discover how AI-driven restoration can illuminate centuries of human creativity—one block at a time.\nPeter is Here is available for all licensed users in the Minecraft Education lesson library. Anyone can access a trial version of Minecraft Education for free by downloading the application and logging in with your Office 365 or Microsoft 365 Education accounts.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Operative Games unveils AI-driven interactive storytelling platform",
        "url": "https://venturebeat.com/games/operative-games-unveils-ai-driven-interactive-storytelling-platform/",
        "content": "Operative Games, an AI-based interactive storytelling company, emerged today from stealth with backing from investors 1AM Gaming, Samsung Next, and LongJourney.vc.\nThe company is creating immersive games and experiences where players can converse with genuinely thoughtful, lifelike AI characters that forge lasting emotional connections in the context of solving problems and navigating intricate stories together.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nFounded by Jon Snoddy, former head of Walt Disney research and development, and Jon Kraft, founding CEO of Pandora Media, along with game industry veteran Pegi Bryant, Operative Games combines decades of entertainment industry innovation with an entirely new application of artificial intelligence to create a unique category of interactive experiences.\nDaniel in The Operative.\n“So we’re this collection of storytellers. They’re artists, writers, engineers, scientists, but the thing that kind of holds us all together is this love of story. And so we set about finding the stories we wanted to tell, and the StoryEngine is really our embodiment of that. It’s our technology, the embodiment of the thing we’re trying to do, which is to tell these stories that that have the emotional resonance, the emotional fidelity of stories you’re used to seeing in film and TV, that same level of emotional connection with characters.”\nThe company’s secret sauce lies in its proprietary StoryEngine, which enables game developers to imbue their characters with a built-in sense of narrative and the understanding of how to take players through that narrative while allowing for full player agency.\nPlayers can call a game character’s phone number and talk, text or video chat with them on a standard phone. The StoryEngine is completely server-based, requiring no downloads. This allows for lifelike character experiences in interactive stories and games that leverage both Operative’s IP and the IP of partners.\n“We’re not just making games – we’re creating relationships, and in some sense – real memories and shared experiences,” said Jon Snoddy, CEO of Operative Games. “Our technology enables characters who understand context and form genuine emotional bonds with players. This marks the beginning of truly personalized storytelling.”\nThe Operative\nA scene in The Operative.\nADVERTISEMENT\nLos Angeles-based Operative Games debuted a small glimpse of its first story experience, The Operative, at the Game Developers Conference (GDC) in San Francisco. In this experience, players call a character named “Enya” who draws them into her world with a request for help.\nWhat seems at first like a simple ask will ultimately unfold into a complex spy thriller in which the player has a pivotal role. Enya reacts to the players’ actions and will even show real human emotions – such as\nannoyance – if she is contacted late at night (just like a real person would).\nInitial projects in development span multiple genres and feature collaborations with renowned Hollywood writers, as well as major players in traditional and interactive entertainment. These experiences showcase the platform’s ability to deliver emotionally resonant storytelling that adapts to each player’s choices and personality, creating uniquely personal narratives that can unfold indefinitely.\nADVERTISEMENT\n“We’ve crossed a line here,” added Snoddy. “With the emerging ability to bring truly believable characters to life and hang out with them in story-driven worlds.”\n“What sets Operative Games apart is their unique ability to create characters that feel genuinely alive,” said Gregory Milken, Managing Partner at 1AM Gaming. “The combination of their technological innovation and their demonstrated ability to turn complex emerging technologies into popular entertainment forms positions them to define an entirely new category of interactive entertainment.”\nOrigins\nJon Snoddy is CEO of Operative Games.\nSnoddy worked at Disney for 15 years as head of R&D. He spent a lot of time thinking about telling stories with emerging technology.\n“We have been working on technologies that would allow us to do something that feels like you’re in a story. And yet the players have an agency. The players are people who are in the story, are able to participate in the story, to be a character in the story,” Snoddy said. “How do you give people agency and yet have something that builds to a climax? There’s a crisis, it leads to that climax, and then a resolution that feels satisfying. And so we set about trying to do that.”\nWhen large language models (LLM) started to work with generative AI, the company saw the final piece in its platform click into place. They formally started their work in 2022 and are coming out of stealth today.\nA scene in The Operative.\nAlong the way, they met with writers, game developers, technologists and investors to shape the company in the right way. The team has 20 people and it has developed what it calls the StoryEngine, a proprietary platform.\nThe StoryEngine details\nThe StoryEngine will drive games like The Operative.\nSnoddy said, “The StoryEngine is really the pieces necessary to tell those stories. We work with the very top Hollywood screenwriters, people that have had their own series, have a long history of drawing and holding a giant audience. We take people like that and and they create the stories and the character. And the character.”\nHe added, “We’re artist driven. And so these screenwriters create these amazing characters and create these amazing scenarios, and then we have to take that linear story and those characters and their histories and their wants and deeds and hopes and fears and turn that into instructions for AI to create the dialog and create those characters so that you can interact with them. So the story engine is the family of technologies that makes that possible.”\nAs a production tool, it takes the prompts, creates the story, and then the AI executes it as the story you want to tell at runtime.\n“It monitors the player’s mood, their emotional level, how the story is progressing, where we need to get to next, to the milesstones of the story that we need to hit and gently moves the story along that that storyline,” Snoddy said.\nThe game adapts to the player’s actions, whether they are aggressive or more passive, and moves the story along as needed.\nThe Operative details\nThe story of The Operative plays out on your phone.\nThe Operative is the first story on the platform. All the characters live in the world. They’re fairly ordinary people, and they may just contact you on your phone.\nThere’s no software to download. There’s no client to install. You just literally pick up your phone, dial their phone number, call them up, and they’ll talk to you, Snoddy said.\n“You’re having a conversation with them, just like you and I might have a conversation. You can text them and they’ll text you back. You can schedule a zoom call, like we’re doing, and do a video conference with them, and so that is how the game is played,” Snoddy said. “It all lives on a server, and so you just live your life. You can be driving down the road and get a text from one of our characters, and the story progresses as you’re talking to them.”\nThe way the story works is that that you meet these characters, and you’re drawn into their lives. You become friends with them.\nThe Operative will have believable characters.\nSnoddy said, “It’s amazing how storytelling works. You know Toy Story’s Buzz Lightyear. You know he’s not real, and yet you feel real emotions for him. You celebrate his successes, you agonize over his failures and great storytellers are able to do that. They tell things in a way that draws us in. And similarly, these characters are interesting ‘people’ and you’re drawn into their lives. And you start talking to them about all sorts of things. They have a lot of wide-ranging interests. You’ll likely find a lot of stuff in common with them that you’ll talk about. And as you get to know them and then you discover that problems are happening. Things are happening in their lives, and they need your help.”\nAs you’re drawn into the story, it starts as you become friends with people. But then you find something bad is happening, and you’re drawn in further as this character gets into trouble and she’s kidnapped. Then you have to find her before something terrible happens to her.\nAnother character winds up getting in trouble with the law and he winds up having to run. He contacts you with a burner phone and you need to follow a number of quests to get the story to move forward. He helps you do things like analyze photos and you have to decode encrypted messages. It’s a kind of detective experience, and there are moments when you interrogate people.\n“People that have information you need and they don’t want to give that information to you, and you have to find a way to break through,” Snoddy said. “And there’s a ticking clock where someone is in real trouble, and you need to get this information now before something bad happens to her. So it’s a whole new kind of storytelling.”\nAll of this comes from the LLM, which has been fed the backstory, history, and characters. It has to understand the world and then assemble the individual stories that will entertain players.\nAsked about more to come, Snoddy said, “We have a second, more traditional game in production for which we are building game worlds. AI is definitely streamlining that process both in design and in production. In design we are able to explore environments at a much more complete level than we could in the past which leads to quicker, more confident decision making. In production we are finding lots of opportunities to move faster and expect that to continue to improve as technology matures.”\nWhat it all means\nThe Operative will have dramatic real world integration.\nIn reminds me of Electronic Arts’ Majestic game, which also involved using phones and came out in 2001. One of the interesting byproducts of using phone conversations is that there won’t be as much need to create high-end 3D graphics for this kind of game. Generally speaking, AI is not so good at that now.\nAfter talking to Snoddy, I had lots of questions, but I presume the answers would give away some of the story experience. His team of writers has to create backstories and history, and the LLM has to know about all of that and understand the characters.\n“Moving forward, our characters always feel alive. That’s kind of who we are as humans. We’re always thinking about where we’re going in the future. And similarly, our characters live in this ‘reality’ that we’ve created that hopefully is complete enough that they’re believable and they make sense,” Snoddy said. “For us, it’s a balance between what the player brings to the table and what the writers have brought to the table. It’s a pretty magical, exciting time.”\nHe added, “One of the powers of our company is that everything we’re doing is in service of that thing, that storytelling thing. And so we will do two things. We’re creating this StoryEngine that allows us to do the real thing we want to do, which is tell stories. And so the output of our company will be these games, these story games that that are already in production and are moving forward.”\nSnoddy estimates it might take something like a month to do a single episode, and more for an entire series. Some collaborations with game companies are already under way, Snoddy said.\nFor the first ones, Operative Games will use its own intellectual property as it gives them the freedom to invent. But in the future, it could do this with IPs from other companies that fans already love.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Baidu delivers new LLMs ERNIE 4.5 and ERNIE X1 undercutting DeepSeek, OpenAI on cost — but they’re not open source (yet)",
        "url": "https://venturebeat.com/ai/baidu-delivers-new-llms-ernie-4-5-and-ernie-x1-undercutting-deepseek-openai-on-cost-but-theyre-not-open-source-yet/",
        "content": "Over the weekend, Chinese web search giant Baidu announced the launch of two new AI models, ERNIE 4.5 and ERNIE X1, a multimodal language model and reasoning model, respectively.\nBaidu claims they offer state-of-the-art performance on a variety of metrics, besting DeepSeek’s non-reasoning V3 and OpenAI’s GPT-4.5 (how do you like the close name match Baidu chose as well?) on several third-party benchmark tests such as the C-Eval (assessing Chinese LLM performance on knowledge and reasoning across 52 subjects), CMMLU (massive multitask language understanding in Chinese), and GSM8K (math word problems).\nNavigating AI Regulations in Telecom - AI Impact Tour 2024\nIt also claims to undercut the cost of both fellow Chinese wunderkind’s DeepSeek’s R1 reasoning model with ERNIE X1 by 50% and US AI juggernaut OpenAI’s GPT-4.5 with ERNIE 4.5 by 99%, respectively.\nYet both have some important limitations, including a lack of open source licensing in the former case (which DeepSeek R1 offers) and a far reduced context compared to the latter (8,000 tokens instead of 128,000, frankly an astonishingly low amount in this age of million-token-plus context windows. Tokens are how a large AI model represents information, with more meaning more information. A 128,000-token window is akin to a 250-page novel).\nAs X user @claudeglass noted in a post, the small context window makes it perhaps only suitable for customer service chatbots.\nBaidu posted on X that it did plan to make the ERNIE 4.5 model family open source on June 30th, 2025.\nBaidu has enabled access to the models through its application programming interface (API) and Chinese-language chatbot rival to ChatGPT, known as “ERNIE Bot” — it answers questions, generates text, produces creative writing, and interacts conversationally with users — and made ERNIE Bot free to access.\nERNIE 4.5: A new generation of multimodal AI\nADVERTISEMENT\nERNIE 4.5 is Baidu’s latest foundation model, designed as a native multimodal system capable of processing and understanding text, images, audio, and video, and is a clear competitor to OpenAI’s GPT-4.5 model released back in February 2025.\nThe model has been optimized for better comprehension, generation, reasoning, and memory. Enhancements include improved hallucination prevention, logical reasoning, and coding capabilities.\nAccording to Baidu, ERNIE 4.5 outperforms GPT-4.5 in multiple benchmarks while maintaining a significantly lower cost.\nThe model’s advancements stem from several key technologies, including FlashMask Dynamic Attention Masking, Heterogeneous Multimodal Mixture-of-Experts, and Self-feedback Enhanced Post-Training.\nERNIE X1: A deep-thinking reasoning model with tool use\nADVERTISEMENT\nERNIE X1 introduces advanced deep-thinking reasoning capabilities, emphasizing understanding, planning, reflection, and evolution.\nUnlike standard multimodal AI models, ERNIE X1 is specifically designed for complex reasoning and tool use, enabling it to perform tasks such as advanced search, document-based Q&A, AI-generated image interpretation, code execution, and web page analysis.\nThe model supports a range of tools, including Baidu’s academic search, business information search, and franchise research tools. Its development is based on Progressive Reinforcement Learning, End-to-End Training integrating Chains of Thought and Action, and a Unified Multi-Faceted Reward System.\nAccess and API availability\nUsers can now access both ERNIE 4.5 and ERNIE X1 via the official ERNIE Bot website.\nFor enterprise users and developers, ERNIE 4.5 is now available through Baidu AI Cloud’s Qianfan platform via API access. ERNIE X1 is expected to be available soon.\nPricing for API Access:\nERNIE 4.5:\nInput: $0.55 USD per 1 million tokens\nOutput: $2.2 per 1M tokens\nERNIE X1:\nInput: $0.28 per 1M tokens\nOutput: $1.1 per 1M tokens\nCompare that to:\nGPT-4.5, which has a noticeably astonishingly high price through the OpenAI API of:\nInput: $75.00 per 1M tokens\nOutput: $150.00 per 1M tokens\nDeepSeek R1\nInput: $0.55 per 1M tokens\nOutput: $2.19 per 1M tokens\nBaidu has also announced plans to integrate ERNIE 4.5 and ERNIE X1 into its broader ecosystem, including Baidu Search and the Wenxiaoyan app.\nConsiderations for enterprise decision-makers\nFor CIOs, CTOs, IT leaders, and DevOps teams, the launch of ERNIE 4.5 and ERNIE X1 presents both opportunities and considerations:\nPerformance vs. Cost – With pricing significantly lower than competing models, organizations evaluating AI solutions may see cost savings by integrating ERNIE models via API. However, further benchmarking and real-world testing may be necessary to assess performance for specific business applications.\nMultimodal and Reasoning Capabilities – The ability to process and understand text, images, audio, and video could be valuable for businesses in industries such as customer support, content generation, legal tech, and finance.\nTool Integration – ERNIE X1’s ability to work with tools like advanced search, document-based Q&A, and code interpretation could provide automation and efficiency gains in enterprise environments.\nEcosystem and Localization – As Baidu’s AI models are optimized for Chinese-language processing and regional knowledge, enterprises working in China or targeting Chinese-speaking markets may find ERNIE models more effective than global alternatives.\nLicensing and Data Privacy – While Baidu has indicated that GPT-4.5 will be made open source later this summer, June 30, 2025, that’s still three months away, so enterprises should at least wait until that time to assess whether it’s worth deploying locally or on US-hosted cloud services. Enterprise users should review Baidu’s policies regarding data privacy, compliance, and model usage before integrating these AI solutions.\nAI expansion and future outlook\nAs AI development accelerates in 2025, Baidu is positioning itself as a leader in multimodal and reasoning-based AI technologies.\nThe company plans to continue investing in artificial intelligence, data centers, and cloud infrastructure to enhance the capabilities of its foundation models.\nBy offering a combination of powerful performance and lower costs, Baidu’s latest AI models aim to provide businesses and individual users with more accessible and advanced AI tools.\nFor more details, visit ERNIE Bot’s official website.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Mistral AI drops new open-source model that outperforms GPT-4o Mini with fraction of parameters",
        "url": "https://venturebeat.com/ai/mistral-ai-drops-new-open-source-model-that-outperforms-gpt-4o-mini-with-fraction-of-parameters/",
        "content": "French artificial-intelligence startup Mistral AI unveiled a new open-source model today that the company says outperforms similar offerings from Google and OpenAI, setting the stage for increased competition in a market dominated by U.S. tech giants.\nThe model, called Mistral Small 3.1, processes both text and images with just 24 billion parameters—a fraction of the size of leading proprietary models—while matching or exceeding their performance, according to the company.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\n“This new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens,” Mistral said in a company blog post announcing the release. The firm claims the model processes information at speeds of 150 tokens per second, making it suitable for applications requiring rapid response times.\nBy releasing the model under the permissive Apache 2.0 license, Mistral is pursuing a markedly different strategy than its larger competitors, which have increasingly restricted access to their most powerful AI systems. The approach highlights a growing divide in the AI industry between closed, proprietary systems and open, accessible alternatives.\nHow a $6 billion European startup is taking on Silicon Valley’s AI giants\nFounded in 2023 by former researchers from Google DeepMind and Meta, Mistral AI has rapidly established itself as Europe’s leading AI startup, with a valuation of approximately $6 billion after raising around $1.04 billion in capital. This valuation, while impressive for a European startup, remains a fraction of OpenAI’s reported $80 billion or the resources available to tech giants like Google and Microsoft.\nMistral has achieved notable traction, particularly in its home region. Its chat assistant Le Chat recently reached one million downloads in just two weeks following its mobile release, bolstered by vocal support from French President Emmanuel Macron, who urged citizens to “download Le Chat, which is made by Mistral, rather than ChatGPT by OpenAI — or something else” during a television interview.\nThe company strategically positions itself as “the world’s greenest and leading independent AI lab,” emphasizing European digital sovereignty as a key differentiator from American competitors.\nSmall but mighty: How Mistral’s 24 billion parameter model punches above its weight class\nADVERTISEMENT\nMistral Small 3.1 stands out for its remarkable efficiency. With just 24 billion parameters—a fraction of models like GPT-4—the system delivers multimodal capabilities, multilingual support, and handles long-context windows of up to 128,000 tokens.\nThis efficiency represents a significant technical achievement. While the AI industry has generally pursued ever-larger models requiring massive computational resources, Mistral has focused on algorithmic improvements and training optimizations to extract maximum capability from smaller architectures.\nADVERTISEMENT\nThe approach addresses one of the most pressing challenges in AI deployment: the enormous computational and energy costs associated with state-of-the-art systems. By creating models that run on relatively modest hardware—including a single RTX 4090 graphics card or a Mac with 32GB of RAM—Mistral makes advanced AI accessible for on-device applications where larger models prove impractical.\nThis emphasis on efficiency may ultimately prove more sustainable than the brute-force scaling pursued by larger competitors. As climate concerns and energy costs increasingly constrain AI deployment, Mistral’s lightweight approach could transition from alternative to industry standard.\nWhy Europe’s AI champion could benefit from growing geopolitical tensions\nMistral’s latest release emerges amid growing concerns about Europe’s ability to compete in the global AI race, traditionally dominated by American and Chinese companies.\n“Not being American or Chinese may now be a help, not a hindrance,” The Economist reported in a recent analysis of Mistral’s position, suggesting that as geopolitical tensions rise, a European alternative may become increasingly attractive for certain markets and governments.\nArthur Mensch, Mistral’s CEO, has advocated forcefully for European digital sovereignty. At the Mobile World Congress in Barcelona this month, he urged European telecoms to “get into the hyperscaler game” by investing in data center infrastructure.\n“We would welcome more domestic effort in making more data centers,” Mensch said, suggesting that “the AI revolution is also bringing opportunities to decentralize the cloud.”\nThe company’s European identity provides significant regulatory advantages. As the EU’s AI Act takes effect, Mistral enters the market with systems designed from inception to align with European values and regulatory expectations. This contrasts sharply with American and Chinese competitors who must retrofit their technologies and business practices to comply with an increasingly complex global regulatory landscape.\nBeyond text: Mistral’s expanding portfolio of specialized AI models\nMistral Small 3.1 joins a rapidly expanding suite of AI products from the company. In February, Mistral released Saba, a model focused specifically on Arabic language and culture, demonstrating an understanding that AI development has concentrated excessively on Western languages and contexts.\nEarlier this month, the company introduced Mistral OCR, an optical character recognition API that converts PDF documents into AI-ready Markdown files—addressing a critical need for enterprises seeking to make document repositories accessible to AI systems.\nThese specialized tools complement Mistral’s broader portfolio, which includes Mistral Large 2 (their flagship large language model), Pixtral (for multimodal applications), Codestral (for code generation), and “Les Ministraux,” a family of models optimized for edge devices.\nThis diversified portfolio reveals a sophisticated product strategy that balances innovation with market demands. Rather than pursuing a single monolithic model, Mistral creates purpose-built systems for specific contexts and requirements — an approach that may prove more adaptable to the rapidly evolving AI landscape.\nFrom Microsoft to military: How strategic partnerships are fueling Mistral’s growth\nMistral’s rise has accelerated through strategic partnerships, including a deal with Microsoft that includes distribution of its AI models through Microsoft’s Azure platform and a $16.3 million investment.\nThe company has also secured partnerships with France’s army and job agency, German defense tech startup Helsing, IBM, Orange, and Stellantis, positioning itself as a key player in Europe’s AI ecosystem.\nIn January, Mistral signed a deal with press agency Agence France-Presse (AFP) to allow its chat assistant to query AFP’s entire text archive dating back to 1983, enriching its knowledge base with high-quality journalistic content.\nThese partnerships reveal a pragmatic approach to growth. Despite positioning itself as an alternative to American tech giants, Mistral recognizes the necessity of working within existing technological ecosystems while building the foundation for greater independence.\nThe open source advantage: Why Mistral is betting against big tech’s closed AI systems\nMistral’s continued commitment to open source represents its most distinctive strategic choice in an industry increasingly dominated by closed, proprietary systems.\nWhile Mistral maintains some premier models for commercial purposes, its strategy of releasing powerful models like Mistral Small 3.1 under permissive licenses challenges conventional wisdom about intellectual property in AI development.\nThis approach has already produced tangible benefits. The company noted that “several excellent reasoning models” have been built on top of its previous Mistral Small 3, such as DeepHermes 24B by Nous Research—evidence that open collaboration can accelerate innovation beyond what any single organization might achieve independently.\nThe open-source strategy also serves as a force multiplier for a company with limited resources compared to its competitors. By enabling a global community of developers to build upon and extend its models, Mistral effectively expands its research and development capacity far beyond its direct headcount.\nThis approach represents a fundamentally different vision for AI’s future — one where foundational technologies function more like digital infrastructure than proprietary products. As large language models become increasingly commoditized, the true value may shift to specialized applications, industry-specific implementations, and service delivery rather than the base models themselves.\nThe strategy carries significant risks. If core AI capabilities become widely available commodities, Mistral will need to develop compelling differentiation in other areas. Yet it also protects the company from becoming trapped in an escalating arms race with vastly better-funded competitors — a contest few European startups could hope to win through conventional means.\nBy positioning itself at the center of an open ecosystem rather than attempting to control it entirely, Mistral may ultimately build something more resilient than what any single organization could create alone.\nThe $6 billion question: Can Mistral’s business model support its ambitious vision?\nMistral faces significant challenges despite its technical achievements and strategic vision. The company’s revenue reportedly remains in the “eight-digit range,” according to multiple sources—a fraction of what might be expected for its nearly $6 billion valuation.\nMensch has ruled out selling the company, stating at the World Economic Forum in Davos that Mistral is “not for sale” and that “of course, [an IPO is] the plan.” However, the path to sufficient revenue growth remains unclear in an industry where deep-pocketed competitors can afford to operate at a loss for extended periods.\nThe company’s open-source strategy, while innovative, introduces its own challenges. If base models become commoditized as Lample predicts, Mistral must develop additional revenue streams through specialized services, enterprise deployments, or unique applications that leverage but extend beyond their foundational technologies.\nMistral’s European identity, while providing regulatory advantages and appeal to sovereignty-conscious customers, also potentially limits its immediate growth potential compared to American and Chinese markets where AI adoption typically moves faster.\nNevertheless, Mistral Small 3.1 represents a compelling technical achievement and strategic statement. By demonstrating that advanced AI capabilities can be delivered in smaller, more efficient packages under open licenses, Mistral challenges fundamental assumptions about how AI development and commercialization should proceed.\nFor a technology industry increasingly concerned about concentration of power among a handful of American tech giants, Mistral’s European-led, open-source alternative offers a vision of a more distributed, accessible AI future—provided it can build a sustainable business model to support its ambitious technical agenda.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Inworld AI showcases AI case studies as they move to production",
        "url": "https://venturebeat.com/gaming-business/inworld-ai-showcases-ai-case-studies-as-they-move-to-production/",
        "content": "The current AI ecosystem wasn’t built with game developers in mind. While impressive in controlled demos, today’s AI technologies expose critical limitations when transitioning to production-ready games, said Kylan Gibbs, CEO of Inworld AI, in an interview with GamesBeat.\nRight now, AI deployment is being slowed because game developers are dependent on black-box APIs with unpredictable pricing and shifting terms, leading to a loss of autonomy and stalled innovation, he said. Players are left with disposable “AI-flavored” demos instead of sustained, evolving experiences.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nAt the Game Developers Conference 2025, Inworld isn’t going to showcase technology for technology’s sake. Gibbs said the company is demonstrating how developers have overcome these structural barriers to ship AI-powered games that millions of players are enjoying right now. Their experiences highlight why so many AI projects fail before launch and more importantly, how to overcome these challenges.\n“We’ve seen a transition over the last few years at GDC. Overall, it’s a transition from demos and prototypes to production,” Gibbs said. “When we started out, it was really a proof of concept. ‘How does this work?’ The use case is pretty narrow. It was really just characters and non-player characters (NPCs), and it was a lot of focus on demos.”\nNow, Gibbs said, the company is focused on production with partners and large scale deployments and actually solving problems.\nGetting AI to work in production\nInworld AI is working with partners like Nvidia and Streamlabs on AI.\nEarlier large language models (LLMs) were too costly to put in games. That’s because it could cost a lot of money to send a user’s query to AI out across the web to a datacenter, using valuable graphics processing unit (GPU) time. It sent the answer back, often so slowly that the user noticed the delay.\nOne of the things that has helped with AI costs now is that the AI processing has been restructured, with tasks moving from the server to the client-side logic. However, that can only really happen if the user has a good machine with a good AI processor/GPU. Inference tasks can be done on the local machines, while harder machine learning problems may have to be done in the cloud, Gibbs said.\nADVERTISEMENT\n“Where I think we’re at today is we actually have proof that the stuff works at huge scale in production, and we have the right tools to be able to do that. And that’s been a great and exciting transition at the same time, because we’ve now been focusing on that we’ve been able to actually uncover regarding the root challenges in the AI ecosystem,” Gibbs said. “When you’re in the prototyping demo mindset, a lot of things work really well, right? A lot of these tools like OpenAI, Anthropic are great for demos but they do not work when you go into massive, multi-million users at scale.”\nGibbs said Inworld AI is focusing on solving the bigger problems at GDC. Inworld AI is sharing the real challenges it has encountered and showing what can work in production.\nADVERTISEMENT\n“There are some very real challenges to making that work, and we can’t solve it all on our own. We need to solve it as an ecosystem,” Gibbs said. “We need to accept and stop promoting AI as this panacea, a plug and play solution. We have solved the problems with a few partners.”\nGibbs is looking forward to the proliferation of AI PCs.\n“If you bring all the processing onto onto the local machine, then a lot of that AI becomes much more affordable,” Gibbs said.\nThe company is providing all the backend models and efforts to contain costs. I noted that Mighty Bear Games, headed by Simon Davis, is creating games with AI agents, where the agents play the game and humans help craft the perfect agents.\n“Companions are super cool. You’ll see multi-agent simulation experiences, like doing dynamic crowds. If you’re if you are focused on a character based experience, you can have primary characters or background characters,” Gibbs said. “And actually getting background characters to work efficiently is really hard because when people look at things like the Stanford paper, it’s about simulating 1,000 agents at once. We all know that games are not built like that. How do you give a sense of millions of characters at scale, while also doing a level-of-detail system, so you’re maximizing the depth of each agent as you get closer to it.”\nAI skeptics?\nAI livestreams\nI asked Gibbs what he thought about the stat in the GDC 2025 survey, which showed that more game developers are skeptical about AI in this year’s survey compared to a year ago. The numbers showed 30% had a negative sentiment on AI, compared to 18% the year before. That’s going in the wrong direction.\n“I think that we’ve got to this point where everybody realizes that the future of their careers will have AI in it. And we are at a point before where everybody was happy just to follow along with OpenAI’s announcements and whatever their friends were doing on LinkedIn,” Gibbs said.\nPeople were likely turned off after they took tools like image generators with text prompts and these didn’t work so well in prodction. Now, as they move into production, they’re finding that it doesn’t work at scale. And so it takes better tools geared to specific users for developers, Gibbs said.\n“We should be skeptical, because there are real challenges that no one is solving. And unless we voice that skepticism and start really pressuring the ecosystem, it’s not going to change,” Gibbs said.\nThe problems include cloud lock-in and unpredictable costs; performance and reliability issues; and a non-evolving AI. Another problem is controlling AI agents effectively so they don’t go off the rails.\nWhen players are playing in a game like Fortnite, getting a response in milliseconds is critical, Gibbs said. AI in games can be a compelling experience, but making it work with cost efficiency at scale requires solving a lot of problems, Gibbs said.\nAs for the changes AI is bringing, Gibbs said, “There’s going to be a fundamental architecture change in how we build user-facing AI apps.”\nGibbs said, “What happens is studios are building with tools and then they get a few months from production and they’re like, ‘Holy crap! This doesn’t work. We need to completely change our architecture.'”\nThat’s what Inworld AI is working on and it will be announced in the future. Gibbs predicts that many AI tools will be quickly outdated within a matter of months. That’s going to make planning difficult. He also predicts that the capacity of third-party cloud providers will break under the strain.\n“Will that code actually work when you have four million users funneling through it?,” Gibbs said. “What we’re seeing is a lot of people having to go back and rework their entire code base from Python to C++ as they get closer to production.”\nSummary of partner demos\nStreamlabs’ architecture for bringing AI into workflow.\nAt GDC, Inworld will be showcasing several key partner demos that highlight how studios of all sizes are successfully implementing AI. These include:\nStreamlabs: Intelligent Streaming Agent provides real-time commentary and production assistance.\nWishroll: Showing off Status, a social media simulation game with unique AI-driven personalities.\nLittle Umbrella: The Last Show, a web-based party game with witty AI hosting.\nNanobit: Winked, a mobile chat game with persistent, evolving relationship building.\nVirtuos: Giving developers full control over AI character behaviors for a more immersive storytelling experience.\nAdditionally, Inworld will feature two Inworld-developed technology showcases:\nOn-device Demo: A cooperative game running seamlessly on-device across multiple hardware platforms.\nRealistic Multi-agent Simulation: Multi-agent simulation demonstrating realistic social behaviors and interactions.\nThe critical barriers blocking AI games from production and real dev solutions \nKylan Gibbs is cofounder of Inworld AI and a speaker at our recent GamesBeat Next event.\nBelow are seven of the key challenges that consistently prevent AI-powered games from making the leap from promising prototype to shipped product. Here’s how studios of all sizes used Inworld to break through these barriers and deliver experiences enjoyed by millions. \nThe real-time wall: Streamlabs Intelligent Agent\nThe developer problem: Non-production ready cloud AI introduces response delays that break player immersion. Unoptimized cloud dependencies result in AI response times of 800 milliseconds to 1,200 milliseconds, making even the simplest interactions feel sluggish.\nAll intelligence remains server-side, creating single points of failure and preventing true ownership, yet most developers can find few alternatives beyond this cloud-API-only AI workflow that locks them into perpetual dependency architectures.\nThe Inworld solution: The Logitech G’s Streamlabs Intelligent Streaming Agent is an AI-driven co-host, producer, and technical sidekick that observes game events in real time, providing commentary during key moments, assisting with scene transitions, and driving audience engagement—letting creators focus on content without getting bogged down in production tasks.\n“We tried building this with standard cloud APIs, but the 1-2 second delay made the assistant feel disconnected from the action,” said the Streamlabs team. “Working with Inworld, we achieved 200 millisecond response times that make the assistant feel present in the moment.”\nBehind the scenes, the Inworld Framework orchestrates the assistant’s multimodal input processing, contextual reasoning, and adaptive output. By integrating seamlessly with third-party models and the Streamlabs API, Inworld makes it easy to interpret gameplay, chat, and voice commands, then deliver real-time actions—like switching scenes or clipping highlights. This approach saves developers from writing custom pipelines for every new AI model or event trigger.\nThis isn’t just faster—it’s the difference between an assistant that feels alive versus one that always seems a step behind the action.\nThe success tax: The Last Show\nThe Last Show\nThe developer problem: Success should be a cause for celebration, not a financial crisis. Yet, for AI-powered games, linear or even increasing unit costs mean expenses can quickly spiral out of control as user numbers grow. Instead of scaling smoothly, developers are forced to make emergency architecture changes, when they should be doubling down on success.\nThe Inworld solution: Little Umbrella, the studio behind Death by AI, was no exception. While the game was an instant hit–reaching 20 million players in just two months – the success nearly bankrupted the studio.\n“Our cloud API costs went from $5K to $250K in two weeks,” shares their technical director. “We had to throttle user acquisition—literally turning away players—until we partnered with Inworld to restructure our AI architecture.”\nFor their next game, they decided to flip the script, building with cost predictability and scalability in mind from day one. Introducing The Last Show, a web-based party game where an AI host generates hilarious questions based on topics chosen or customized by players. Players submit answers, vote for their favorites, and the least popular response leads to elimination – all while the AI host delivers witty roasts.\nThe Last Show marks their comeback, engineered from the ground up to maintain both quality and cost predictability at scale. The result? A business model that thrives from success rather than being threatened by it.\nThe quality-cost paradox: Status\nHow can you be popular? Status knows.\nThe developer problem: Better AI quality often correlates with higher costs, forcing developers into an impossible decision: deliver a subpar player experience or face unsustainable costs. AI should enhance gameplay, not become an economic roadblock.\nThe Inworld solution: Wishroll’s Status (ranking as high as No. 4 in the App Store Lifestyle category) immerses players in a fictional world where they can roleplay as anyone they imagine—whether a world-famous pop star, a fictional character, or even a personified ChatGPT. Their goal is to amass followers, develop relationships with other celebrities, and complete unique milestones.\nThe concept struck a chord with gamers and by the time the limited access beta launched in October 2024, Status had taken off. TikTok buzz drove over 100,000 downloads with many gamers getting turned away, while the game’s Discord community ballooned from a modest 100 users to 60,000 within a few days. Only two weeks after their public beta launch in February 2025, Status surpassed a million users. \n“We were spending $12 to $15 per daily active user with top-tier models,” said CEO Fai Nur, in a statement. “That’s completely unsustainable. But when we tried cheaper alternatives, our users immediately noticed the quality drop and engagement plummeted.”\nWorking with Inworld’s ML Optimization services, Wishroll was able to cut AI costs by 90% while improving quality metrics. “We saw how Inworld solved similar problems for other AI games and thought, ‘This is exactly what we need,'” explained Fai. “We could tell Inworld had a lot of experience and knowledge on exactly what our problem was – which was optimizing models and reducing costs.”\n“If we had launched with our original architecture, we’d be broke in days,” Fai explained. “Even raising tens of millions wouldn’t have sustained us beyond a month. Now we have a path to profitability.”\nThe agent control problem: Partnership with Virtuos\nThe developer problem: Even with sustainable performance benchmarks met, complex narrative games still require sophisticated control over AI agents’ behaviors, memories, and personalities to deliver deeply immersive and engaging experiences to gamers. Traditional approaches either lead to unpredictable interactions or require prohibitively complex scripting, making it nearly impossible to create believable characters with consistent personalities.\nThe Inworld solution: Inworld is partnering with Virtuos, a global game development powerhouse known for co-developing some of the biggest triple-A titles in the industry like Marvel’s Midnight Suns and Metal Gear Solid Delta: Snake Eater. With deep expertise in world-building and character development, Virtuos immediately saw the need for providing developers with precise control over the personalities, behaviors, and memories of AI-driven NPCs. This ensures storytelling consistency and players’ choices to dynamically influence the narrative’s direction and outcome.\nInworld’s suite of generative AI tools provides the cognitive core that brings these characters to life while equipping developers with full customization capabilities. Teams can fine-tune AI-driven characters to stay true to their narrative arcs, ensuring they evolve logically and consistently within the game world. With Inworld’s tools, Virtuos can focus on what they do best–creating rich, immersive experiences.\n“At Virtuos, we see AI as a way to enhance the artistry of game developers and accurately bring their visions to life,” said Piotr Chrzanowski, CTO at Virtuos, in a statement. “By integrating AI, we enable developers to add new dimensions to their creations, enriching the gaming experience without compromising quality. Our partnership with Inworld opens the door to gameplay experiences that weren’t possible before.”\nA prototype showcasing the best of both teams is in the works, and interested media are invited to stop by the Virtuos booth at C1515 for a private demo.\nThe immersive dialogue challenge: Winked\nThe developer problem: Nanobit’s Winked is a mobile interactive narrative experience where players build relationships through dynamic, evolving conversations, including direct messages with core characters. To meet player expectations, the player-facing AI-driven dialogue had to exceed what was possible even with frontier models — offering more personal, emotionally nuanced, and stylistically unique interactions. Yet, achieving the level of quality was beyond the capabilities of off-the-shelf models, and the high costs of premium AI solutions made scalability a challenge. \nThe Inworld solution: Using Inworld Cloud, Nanobit trained and distilled a custom AI model tailored specifically for Winked. This model delivered superior dialogue quality–more organic, personal, and contextually aware than off-the-shelf solutions—while keeping costs a fraction of traditional cloud APIs. The AI integrated seamlessly into Winked’s core game loops, enhancing user engagement while maintaining financial viability.\nBeyond improving player immersion, this AI-driven dialogue system remembers past conversations and carries the storyline forward, providing the player with relationships that evolve as chats progress. This in turn encourages players to engage in longer conversations and return more frequently as they grow closer to characters.\nThe multi-agent orchestration challenge: Realistic multi-agent simulation\nThe developer problem: Creating living, believable worlds requires coordinating multiple AI agents to interact naturally with each other and the player. Developers struggle to create social dynamics that feel organic rather than mechanical, especially at scale.\nThe Inworld solution: Our Realistic Multi-agent Simulation demonstrates how to effectively orchestrate multiple AI agents into cohesive, living worlds using Inworld. By implementing sophisticated agent coordination systems, contextual awareness, and shared environmental knowledge, this simulation creates believable social dynamics that emerge naturally rather than through scripted behaviors.\nWhether forming spontaneous crowds around exciting in-game events, reacting to shared group emotes, or engaging in multi-character conversations, these autonomous agents showcase how proper agent orchestration enables emergent, lifelike behaviors at scale. This technical demonstration underscores the potential for deep player immersion and sustained engagement by bringing social hubs to life—where multiple characters interact with consistent personalities, mutual awareness, and collective response patterns that create the feeling of a truly living world.\nThe hardware fragmentation challenge: On-device Demo\nThe developer problem: AI features optimized for high-end devices fail on mainstream hardware, forcing developers to either limit their audience or compromise their vision. AI vendors also obscure critical capabilities required for on-device inference (distilled models, deep fine-tuning and distillation, runtime model adaptation) to maintain control and protect recurring revenue.\nThe Inworld solution: While on-device is the key to a more scalable future of AI and games, AI hardware in gaming doesn’t have a one-size-fits-all solution. Ensuring consistent performance and accessibility for users on various devices can easily drive up complexity and cost. To achieve scalability, AI solutions must adapt seamlessly across diverse hardware configurations.\nOur on-device demo showcases an AI-powered cooperative gameplay running seamlessly across three hardware configurations:\nNvidia GeForce RTX 5090\nAMD Radeon RX 7900 XTX\nTenstorrent Quietbox\nThis demo isn’t about theoretical compatibility; it’s about achieving consistent performance across diverse hardware, allowing developers to target the full spectrum of gaming devices without sacrificing quality.\nThe development difference: Going beyond prototypes\nThe gap between prototype and production is where most AI game projects collapse. While out-of-the-box plugins are useful for prototyping, they break under real-world conditions:\nLatency collapse: Cloud-dependent tools see response times balloon under load, breaking immersion and even gameplay\nCost explosion: Per-token pricing creates financial cliff edges that make scaling unpredictable\nReliability bottlenecks: Each external API call introduces a new potential point of failure\nQuality consistency: AI performance varies dramatically between test and production environments\n“We’ve watched incredible AI game prototypes die in the transition to production for four years now,” says Evgenii Shingarev, VP of Engineering at Inworld, in a statement. “The pattern is always the same: impressive demo, enthusiastic investment, then the slow realization that the economics and technical architecture don’t support real-world deployment.”\nAt Inworld, we’ve worked relentlessly to close this prototype-to-production gap, developing solutions that address the real-world challenges of shipping and scaling AI-powered games—not just showcasing impressive demos. At GDC, Inworld is excited to share experiences that don’t just make it to launch, but thrive at scale, said Gibbs. The company’s booth is at C1615.\nInstead of talking about the future of gaming with AI, we’ll show the real systems solving real problems, developed by teams who have faced the same challenges you’re encountering, Gibbs said.\nThe path from AI prototype to production is challenging, but with the right approach and partners who understand what it takes to ship AI experiences that players love, it’s absolutely achievable, Gibbs said.\nSession with Jim Keller of Tenstorrent: Breaking down AI’s unsustainable economics:\nJim Keller, now head of Tenstorrent, is a legendary hardware engineer who headed important processor projects at companies such as Apple, AMD and Intel. He will be on a GDC panel with Inworld CEO Kylan Gibbs for a candid examination of AI’s broken economic model in gaming and the practical path forward:\n“Current AI infrastructure is economically unsustainable for games at scale,” said Keller, in a statement. “We’re seeing studios adopt impressive AI features in development, only to strip them back before launch once they calculate the true cloud costs at scale.”\nGibbs said he is looking forward to talking with Keller on stage about Tenstorrent, which aims to serve AI applications at scale for less than 100 times the cost.\nThe session will explore concrete solutions to these economic barriers:\nDramatically cheaper model and hardware options\nLocal inference strategies that eliminate API dependency\nPractical hybridization approaches that optimize for cost, performance, and quality\nActive learning systems that improve ROI over time\nDrawing on Keller’s deep hardware expertise from Tenstorrent, AMD, Apple, Intel, and Tesla and Inworld’s expertise in real-time, user-facing AI, we’ll explore how to blend on-device compute with large-scale cloud resources under one architectural umbrella. Attendees will gain candid insights into what actually matters when bringing AI from theory into practice, and how to build a sustainable AI pipeline that keeps costs low without sacrificing creativity or performance.\nSession details:\nThursday, March 20, 9:30 a.m. – 10:30 a.m.\nWest Hall, Room #2000\nFor more details, visit the GDC page\nSession with Microsoft: AI innovation for game experiences\nGibbs will also join Microsoft’s Haiyan Zhang and Katja Hofmann to explore how AI can drive the next wave of dynamic game experiences. This panel bridges research and practical implementation, addressing the critical challenges developers face when moving from prototypes to production.\nThe session showcases how our collaborative approach solves industry-wide barriers preventing AI games from reaching players – focusing on proven patterns that overcome the reliability, quality, and cost challenges most games never survive.\nI asked how Gibbs could convince a game developer that AI is a train they can get on, and that it’s not a train coming right at them.\n“Unfortunately, there’s lots of other partners that we weren’t able to share publicly. A lot of the triple-A’s [are quiet]. It’s happening, but it requires a lot of work. We’re starting to engage with developers where the requirements are being creative. If they have a game that they’re planning on launching in the next year or two years, and they don’t have a clear line of sight on how to do that efficiently at scale or cost, we can work with them on that,” Gibbs said. “There is a fundamentally different ways that it can be structured and integrated into games. And we’re going to have a lot more announcements this year as we’re trying to make them more self serve.”\nSession details:\nMonday, March 17, 10:50 a.m. to 11:50 a.m.\nWest Hall, Room #3011\nFor more details, visit the GDC page\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Visa’s AI edge: How RAG-as-a-service and deep learning are strengthening security and speeding up data retrieval",
        "url": "https://venturebeat.com/ai/visas-ai-edge-how-rag-as-a-service-and-deep-learning-are-strengthening-security-and-speeding-up-data-retrieval/",
        "content": "Global payments giant Visa operates in 200-plus countries and territories, all with their own unique, complex rules and regulations. \nIts client services team must understand those nuances when policy-related questions come up — like ‘are we allowed to process this type of payment in this country?’ — but it’s simply not humanly possible to know all those answers top-of-mind. \nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nThis means they’ve typically had to track down relevant information manually — an exhaustive process that can take days depending on how accessible it is. \nWhen generative AI emerged, Visa saw this as a perfect use case, applying retrieval-augmented generation (RAG) to not only pull out information up to 1,000X faster, but cite it back to its sources. \n“First of all, it’s better quality results,” Sam Hamilton, Visa’s SVP of data and AI, told VentureBeat. “It’s also latency, right? They can handle a lot more cases than they were able to before.”\nThis is just one way Visa is using gen AI to enhance its operations — supported by a deliberately-built, tiered tech stack — while managing risk and keeping fraud at bay. \nSecure ChatGPT: Visa’s protected models\nNovember 30, 2022, the day ChatGPT was introduced to the world, will go down in history as a pivotal moment for AI. \nNot long thereafter, Hamilton noted, “employees at Visa were all asking, ‘Where is my chatGPT?’ ‘Can I use ChatGPT?’ ‘I don’t have access to ChatGPT.’ ‘I want ChatGPT.’”\nHowever, as one of the world’s largest digital payments providers, Visa naturally had concerns about its customers’ sensitive data — specifically, that it remained secure, out of the public domain and wouldn’t be used for future model training.  \nTo meet employee demand while balancing these concerns, Visa introduced what it calls ‘Secure ChatGPT,’ which sits behind a firewall and runs internally on Microsoft Azure. The company can control input and output via data loss prevention (DLP) screening to ensure no sensitive data is leaving Visa’s systems. \n“All the hundreds of petabytes of data, everything is encrypted, everything is secure at rest and also in transport,” Hamilton explained.\nADVERTISEMENT\nDespite the name, Secure ChatGPT is a multi-model interface offering six different options: GPT (and its various iterations), Mistral, Anthropic’s Claude, Meta’s Llama, Google’s Gemini and IBM’s Granite. Hamilton described this as model-as-a-service or RAG-as-a-service. \n“Think of that as a kind of a layer where we can provide an abstraction,” he said.\nInstead of people building their own vector databases, they can pick and choose the API that best fits their particular use case. For instance, if they just need a little bit of fine-tuning, they’ll typically choose a smaller open-source model like Mistral; by contrast, if they’re looking for more of a sophisticated reasoning model, they can choose something like OpenAI o1 or o3. \nADVERTISEMENT\nThis way, people don’t feel constrained or as if they’re missing out on what’s readily available in the public domain (which can lead to ‘shadow AI,’ or the use of unapproved models). Secure GPT is “nothing more than a shell on top of the model,” Hamilton explained. “Now they can pick the model they want on top of that.” \nBeyond Secure ChatGPT, all Visa developers are given access to GitHub Copilot to assist in their day to day coding and testing. Developers use Copilot and plugins for various integrated development environments (IDEs) to understand code, enhance code and perform unit testing (determining that code runs as intended), Hamilton noted.\n“So the code coverage [identifying areas where proper testing is lacking] increases significantly because we have this assistant,” he said. \nRAG-as-a-service in action\nOne of the most potent use cases for Secure ChatGPT is the handling of policy-related questions specific to a given region. \n“As you can imagine, being in 200 countries with different regulations, documents could be thousands and thousands, hundreds of thousands,” Hamilton noted. “That gets really complicated. You need to nail that, right? And it needs to be an exhaustive search.” \nNot to mention, local policy changes over time, so Visa’s experts must be up-to-date. \nNow with a robust RAG grounded in reliable, up-to-date data, Visa’s AI not only quickly retrieves answers, but provides citations and source materials. “It tells you what you can do or cannot do, and says, ‘Here is the document that you want, I’m giving an answer based on that,’” Hamilton explained. “We have narrowed answers with the knowledge that we have built into the RAG.” \nNormally, the exhaustive process would take “if not hours, days” to draw concrete conclusions. “Now I can get that in five minutes, two minutes,” said Hamilton. \nVisa’s four-layer ‘birthday cake’ data infrastructure\nThese capabilities are the result of Visa’s heavy investment in data infrastructure over the last 10 years: The finance giant has spent around $3 billion on its tech stack, according to Hamilton. \nHe describes that stack as a “birthday cake with 4 layers”: The foundation is a ‘data-platform-as-a-service layer, with ‘data-as-a-service,’ an AI and machine learning (ML) ecosystem and data services and products layers built on top. \nData-platform-as-a-service essentially serves as an operating system built on a data lake that aggregates “hundreds of petabytes of data,” Hamilton explained. The layer above, data-as-a-service, serves as a sort of “data highway” with multiple lanes going at different speeds to power hundreds of applications. \nLayer three, the AI/ML ecosystem, is where Visa continuously tests models to ensure they are performing the way they should be, and are not susceptible to bias and drift. Finally, the fourth layer is where Visa builds products for employees and clients. \nBlocking $40 billion in fraud\nBeing a trusted payment provider, one of Visa’s top priorities is fraud prevention, and AI is playing an increased role here, as well. Hamilton explained that the company has invested more than $10 billion to help reduce fraud and increase network security. Ultimately, this helped the company block $40 billion in attempted fraud in 2024 alone.\nFor instance, a new Visa deep authorization tool provides transaction risk scoring to help manage card-not-present (CNP) payments (such as when users pay via web or mobile app, as is everyday practice for all of us). This is powered by a deep learning recurrent neural network (RNN) model based on petabytes of contextual data. Similarly, real-time, account-to-account payment protection (think via digital wallets or instant payment systems) — is enabled by deep learning AI models that produce instant risk scores and automatically block bad transactions. \nHamilton explained that Visa used a transformer-based model — a neural network that learns context and meaning by tracking relationships in data — to enhance these tools and quickly identify and thwart fraud. “We wanted to do that in line with the transactions,” he said. “That means we have less than a second, I should say milliseconds, response times.” \nSynthetic data provides value in fraud prevention, as well: Hamilton’s team augments existing data with synthetic data to perform simulations around newer enumerations of fraud. “That helps us learn what’s happening now and what could happen in the short term and long term, so we can simulate and train the model to catch the data,” he said.  \nHe noted that fraud is an arms race — and there’s a very low barrier to entry for threat actors. “We need to be a step ahead of that and anticipate and block them,” Hamilton emphasized. \nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Roblox unveils Roblox Cube GenAI and other game dev tools for GDC",
        "url": "https://venturebeat.com/game-development/roblox-unveils-roblox-cube-genai-and-other-game-dev-tools-for-gdc/",
        "content": "Roblox introduced its Roblox Cube AI tools, the core generative AI system for building 3D objects and scenes in time for the Game Developers Conference.\nAnd the company said that this week it is open sourcing the first release of its 3D foundational model for GenAI dubbed Cube 3D. Anyone can use Cube 3D on or off the Roblox platform, said Roblox execs Anupam Singh and Nick Tornow in a blog post.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024Why Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nLast fall, Roblox announced the ambitious project to build an open-source 3D foundational model to\ncreate 3D objects and scenes on Roblox. Now it’s followed up with Cube 3D’s release.\nCube will underpin many of the AI tools Roblox will develop in the years to come, including highly complex scene-generation tools. It will ultimately be a multimodal model, trained on text, images, video, and other types of input—and will integrate with our existing AI creation tools.\nMore details on Cube 3D\nCube 3D mesh generation.\nCube 3D generates 3D models and environments directly from text and, in the future, image inputs. Today, state-of-the-art 3D generation uses images and a reconstruction approach to build 3D objects. This is a good option when there isn’t sufficient 3D training data.\nHowever, thanks to the nature of Roblox’s platform, the company trains on native 3D data. The generated object is fully compatible with game engines today and can be extended to make objects functional. The difference here is similar to a racetrack movie set.\nOn TV, you might see what looks like a fully functional racetrack, with stands, garages, and a victory lane. But if you were to walk around on that set, you’d quickly realize that the structures were actually flat. Building a truly immersive 3D world requires complete, functional structures, with garages you can drive into, stands you can sit in, and a victory lane with a functional podium.\nTo achieve this, Roblox has taken inspiration from state-of-the-art models trained on text tokens (or sets of characters) so they can predict the next token to form a sentence. The innovation builds on the same core idea. Roblox has built the ability to tokenize 3D objects and understand shapes as tokens and trained Cube 3D to predict the next shape token to build a complete 3D object.\nADVERTISEMENT\nWhen Roblox extends this to full scene generation, Cube 3D then predicts the layout and recursively predicts the shape to complete that layout.\nAnyone can fine-tune, develop plug-ins for, or train Cube 3D on their own data to suit their needs. Roblox believes that AI tools should be built on openness and transparency, which is why the company said it is a committed partner in the open-source AI community.\nRoblox released one of its AI safety models because it believes sharing advancements in AI safety helps the entire industry accelerate innovation and technical advancements. For this reason, the company also helped found ROOST, a new nonprofit dedicated to tackling important areas in digital safety with open-source safety tools. In open-sourcing Cube 3D, the goal is to enable researchers, developers, and the broader AI community to learn, augment, and advance 3D generation industry-wide.\nADVERTISEMENT\nCube 3D for Creation\nRoblox mesh generations.\nRoblox previously talked about how AI can accelerate the creation of 3D assets, accessories, and experiences. Ultimately AI will enable even more immersive and personalized play and connections, the company said.\n“We invest in infrastructure to support AI at every stage of the creation cycle—for both the developers of these experiences and the users who spend time in them. We envision a future where developers will give their users new ways to create by enabling AI in their experiences. This puts the power of AI in the hands of more than 85 million daily active users as part of their gameplay,” the post said.\nIn the past year, Roblox introduced several new features through our AI-powered Assistant within Roblox Studio to provide developers with the tools and capabilities they need to create and eliminate hours of manual work. With Cube, we intend to make 3D creation more efficient. With 3D mesh generation, developers can quickly explore new creative directions and increase their productivity by deciding rapidly which to move forward with.\nImagine building a racetrack game. Today, you could use the Mesh Generation API within Assistant by typing in a quick prompt, like “/generate a motorcycle” or “/generate orange safety cone.” Within seconds, the API would generate a mesh version of these objects. They could then be fleshed out with texture, color, etc.\nWith this API, developers can model props or design their space much faster—no need to spend hours modeling simple objects. It lets devs focus on the fun stuff, like designing the track layout and fine-tuning the car handling. This API saves hours on each object created and gives you back that time to experiment with new ideas without worrying about spending too much time or effort. Longer term, Roblox plans to enable more complex and functional objects, even scenes.\nThis technology extends to the tens of millions of creative people who play and connect on Roblox every day. Roblox sees a future where developers enable their users to become creators using AI. With the Mesh Generation API enabled, players can bring to life anything they can imagine. If a player wants a futuristic car, they can just type “red car of the future with side wings” or “black leather motorcycle jacket” and see it generated. This kind of in-game AI generation is going to unlock a whole new level of creativity. Players can personalize their experience in ways developers never imagined, and that’s going to make their games even more engaging.\nUnder the Hood: Cross Attention Between 3D and Text/Image Tokens\nThe key technical challenge was to connect text and images with 3D shapes. The core technical breakthrough is 3D tokenization, which allows us to represent 3D objects as tokens in the same way that text can be represented as tokens. This gives us the ability to predict the next shape just as language models predict the next word in a sentence.\nTo achieve 3D generation, Roblox designed a unified architecture for autoregressive generation of single object, shape completion, and multiobject/scene layout generation. Autoregressive transformers are neural networks that use previous inputs to predict the next component. This architecture provides both scalability and multimodal compatibility so that as Roblox expands the model, it will work with many different kinds of input (text, visual, audio, and 3D). Roblox is open-sourcing this model. In this initial stage, creators will be able to generate 3D objects based on text prompts. Down the road, Roblox intends for creators to be able to generate entire scenes based on multimodal inputs.\nTo train a generative pretrained transformer (GPT) for shape generation, Roblox uses discrete 3D shape tokens and align them with text prompts. This novel approach sets us up for the world of 3D scene generation that’s playable.\nWhere Cube Is Heading\nDynamic ranking on Home.\nToday, much of the world uses AI for text, to predict words in a sentence. Many also use it for images, to predict pixels. This gets much more complex when creating scenes, where all of these elements come together and need to work in context with one another. For example, imagine an experience with a simple scene that can be described as “an avatar on a motorcycle in front of a racetrack with trees.”\nMany elements go into building this experience. The trees are a combination of two 3D meshes, the motorcycle is a dense mesh with details and triangles, and the buildings are made up of Roblox parts. The avatar on the motorbike has more complex geometric features for its body, limbs, and head. Finally, Roblox needs a way to tie it all together with a layout. For that, the company needs bounding boxes, which outline an object to define its size and location, to know how to arrange this geometry. This is a painstaking process, but AI is capable of helping with each step. With AI, creators can get to the first version faster and have more time to test new ideas or refine their scene.\nWhen Roblox gets there, it wants the 3D objects and scenes created to be fully functional. It calls this 4D creation, where the fourth dimension is interaction between objects, environments, and people. Achieving this requires the ability not only to build immersive 3D objects and scenes, but also to understand the contexts and relationships between those objects. This is where Roblox is heading with Cube.\nBeyond this first use case of mesh generation, Roblox plans to extend to scene generation and understanding. The company will be able to serve users the experiences they’re most interested in and to\naugment scenes by adding objects in context. For example, in an experience with a forest scene, a developer could ask Assistant to replace all the lush green leaves on the trees with fall foliage to indicate the change of season.\nThe AI Assistant tools react to requests from the developer, helping them rapidly create, adapt, and scale their experiences. Roblox will share updates and new functionality as it continues improving and expanding the foundational model. The Cube 3D model will be available later this week.\nMore news\nScript iteration using AI.\nMeanwhile, Roblox said it also has updates to Roblox Studio’s performance and reliability, UI, and Asset Manager, several new APIs, and a more personalized homepage.\nRoblox believes developer teams of any size can produce some of the biggest games on the platform because it provides the tools to publish once to almost any device, anywhere in the world.\nAt last year’s Roblox Developers Conference (RDC), the company announced its overarching goal: to have 10% of all gaming market content revenue flow through Roblox and be distributed within the creator community. To achieve that, Roblox said it is committed to giving developers creation, discovery, and monetization tools that make it easy for them to start and build businesses on the platform.\nLast year, creators on Roblox earned $923 million, up 25% from $741 million in 2023. The number of daily active users on Roblox grew 21% in 2024, while the gaming industry grew at a single-digit pace in the same time frame.\nDuring the fourth quarter of 2024, 85.3 million daily active users spent an average of 2.4 hours a day immersed in games, shared experiences, and more. There’s a clear trend of players coming to Roblox over any other platform, and this is an enormous opportunity for creators to build quickly and find new audiences of active and engaged players, the company said.\nStreamlining Roblox Studio for Creators\nRoblox leads the rise of user-generated content.\nFor creators big and small, Roblox Studio is the 3D immersive content creation ecosystem and engine. And at GDC, the company is rolling out new features.\nIt is dramatically improving Roblox Studio’s performance and reliability. This includes improvements to Explorer, which the company has rebuilt in Luau with a more performant architecture. Developers can now manage thousands of Explorer instances without slowdowns.\nRoblox is also making Studio even easier to use with an updated UI and the ability to customize toolbars. It is upgrading Studio’s Asset Manager with unified asset management, improved search and filtering, and increased performance and stability.\nTo enable quick and efficient communication between developers, the company is making it possible for developers to collaborate in real time by leaving comments and annotations for one another. AI powers many of the tools provided to Roblox creators to build their games and quickly explore new creative directions. At GDC, it is announcing several new tools that showcase how AI can make things even easier for them.\nRoblox is unveiling its text generation API, which lets creators offer interactive possibilities in their experiences. And it is announcing a real-time translation API, which automatically translates in-experience text.\nAnd Roblox will be updating its open-source voice safety classifier in the coming weeks to detect policy violations in several additional languages.\nDuring one of Roblox’s GDC sessions, principal engineer Peter McNeill will demonstrate Studio’s\nnew look and collaboration tools as he builds and publishes a new game. Attendees will see how quickly creators can advance from a concept to a published multiplayer game on every major platform.\nConnecting Millions of Users Through Experiences\nDavid Baszucki is CEO of Roblox.\nThe goal for discovery was to surface, in a fair way, more diverse content on the homepage, making it easier for people to find their next favorite experience. At RDC, Roblox said it was commited to open and transparent insight into its algorithm. That includes what the company values, how it rewards attributes that drive distribution, and how developers’ experiences are performing against those attributes. Thanks to these changes, the company has seen more experiences breaking into the top 100. The firm plans to continue evolving these discovery systems to feature more games in the homepage sorts, search results, and recommendations.\nMore than 90% of Roblox traffic originates from the homepage. To help players find the experiences and connections most relevant to their interests, the company is making the homepage even more dynamic and personalized. Earlier this year, the company introduced dynamic ranking, which means the “People You May Know” and “Continue” sorts will be dynamically positioned on the page based on how the user engages with them.\nRoblox also rolled out the “Recently Visited” sort in search results so players can quickly return to experiences they enjoy.\nIn the coming weeks, Roblox will roll out the full release of the Friend Referral Program, which enables developers to create a program within their experience that rewards existing players for inviting new players to their experience, using referral links. They can also create in-experience rewards, such as a custom badge or in-game currency. Roblox has seen that those who play with their friends are more likely to spend additional time in creators’ games. In fact, the company found that co-play sessions are typically 1.9 times longer than solo sessions.\n“Making a hit game doesn’t require expensive marketing or tech licenses or lengthy development lifecycles. It takes focusing on creating experiences that are simple and fun to play with your friends,” said Janzen Madsen, CEO of Splitting Point Studios, who’s speaking Thursday at GDC, in a statement. “When we launched a dusty trip, the automatic thumbnail personalization function, part of homepage discovery, really helped us find our audience, and we’re now at over a billion plays with no signs of slowing down.”\nRoblox’s Ads Manager gives creators another way to drive growth and reach more players through sponsored experiences, search ads, and more. In the coming weeks, Roblox is introducing significant, long-requested upgrades to the product, including engaging ad units, improved performance, and simplified setup and ROI-focused reporting.\nWith the powerful creation and discovery tools we provide, developers have the opportunity to transform their ideas into growing businesses, Roblox said. And it’s not just the highest level of earners who have success on Roblox. In 2024, the top 1,000 developers on Roblox made an average of $820,000, up 570% since 2019. The top 10 developers made an average of $33.9 million, and the top 100 earned an average of $6 million—up 450% and 500%, respectively, since 2019.\nThe Roblox economy allows creators to increase their earnings by minimizing costs and providing monetization tools for studios of every size. From server architecture to distribution to moderation, the company has creators covered. Instead of spending hours figuring out complicated systems, they can invest more of their time building great games. And they can monetize them in ways that make the most sense for their game and player base.\nRoblox is also providing creators with more ways to earn. Roblox Rewarded Video Ads are currently live in beta within select experiences, and it be rolling them out to more creators in the second quarter of this year. This will enable creators to offer their users more value at critical points within their game while also creating high-performance ads for brands. Roblox expects this combination to drive higher earnings while making the experiences themselves more likely to power engagement.\nThe company is also launching a way for experience creators to sell in-game currency, boosts, items, and\nother developer products on their experience details pages. This makes it easier for creators to offer their items outside of their experiences, in other areas of the Roblox platform.\nRoblox Assistant update\nRoblox AI Assistant.\nSince launching Roblox Assistant last December, the company has been expanding its capabilities and improving its performance. Roblox said it is committed to making it a useful resource in Studio with devs in the driver’s seat, designed to accelerate productivity.\nDevs should feel in control of what Assistant is doing in their games at all times and be adjustable if it has made an error. With this in mind, devs can now review the changes that it makes to scripts before they’re applied, undo/redo changes that Assistant makes with the standard cmd+z /cmd+shift+z hotkeys and edit previous prompts.\nAdditionally, Assistant now has a script debugging capability. It’ll run a root cause analysis and attempt to identify problem areas with a dev’s code, provide recommended changes or point the dev in the right direction of why code may be failing. There’s more updates coming.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "GGWP adds voice moderation for Unity’s Vivox voice chat and Gorilla Tag",
        "url": "https://venturebeat.com/games/ggwp-adds-voice-moderation-for-unitys-vivox-voice-chat-and-gorilla-tag/",
        "content": "GGWP, a brand safety platform for online communities, has added proactive voice moderation tools and been selected by Unity to be the safety partner of Vivox voice chat.\nOn top of that, Another Axiom will use GGWP‘s voice moderation tools for its popular virtual reality game Gorilla Tag.\n212_Does marketing D2C work outside your mobile game.mp4\nGGWP’s comprehensive suite of tools is enhancing community experiences for Gorilla Tag and powering\nsecure voice moderation with seamless integration for Unity’s Vivox customers worldwide.\nGGWP said it brings the most comprehensive communications safety suite to Vivox, delivering low-latency, tamper-proof voice moderation with seamless server-side integration for all Vivox customers.\n“This integration of GGWP’s tools with Vivox is an incredible value add for all Vivox tech users, and we’re thrilled to offer it to them. Vivox is the best-in-class communication solution and pairing it with the best-in-class voice moderation solution enables gaming communities to come together like never before,” said Scott Chapman, director of multiplayer services at Unity, in a statement. “The integration provides developers a one-stop solution for communication and moderation, helping developers focus their resources on building the core gameplay experience.”\nDennis Fong is CEO of GGWP.\nBoth existing and new Vivox users can deploy this technology into their games to reap the benefits of GGWP’s robust detection and moderation. GGWP’s context-aware AI analyzes communication across voice and text, providing a 360-degree view of the user’s positive and negative behavior.\n“Voice chat is a key aspect to online game experiences, and in some cases, it is the main form of interaction,” said GGWP founder and CEO Dennis Fong. “We are excited about our top tier partners joining us on our mission to deploy our voice solution in their toolkits and games. At GGWP our goal is to create awesome communities for brands and help their businesses thrive; and now with Voice, we are offering a complete end-to-end platform powered by AI and our expert services to model and maintain healthy communities.”\nThe progress with voice chat screening is good for GGWP, considering it wasn’t first to market. Modulate score a high-profile deal with Activision for Call of Duty voice chat screening in 2023.\nADVERTISEMENT\nGorilla Tag is using GGWP’s voice chat moderation.\n“We didn’t offer voice moderation back in 2023 when Call of Duty was looking for a solution, so it wasn’t an option for us then.  We believe our voice moderation solution is best-in-class and always welcome being tested to compare our results with others,” Fong said in a message to GamesBeat.\nADVERTISEMENT\nRecently, Another Axiom posted a blog detailing the steps that they’re taking to protect players\nin Gorilla Tag, which is built on Unity. As one of the most popular VR games with young\naudiences, GGWP’s platform and custom services will enable the developer to quickly respond\nto undesirable behavior and highlight positive interactions to continue creating a positive\ncommunity. Since the players are in a VR environment, voice chat is a critical way that players\ninteract; with GGWP’s support, Another Axiom can build a healthier and more fun experience for\nall.\n“Our community is evolving and growing, so the tools that we use to help maintain a fun, playful experience need to be cutting edge and work at-scale,” said Kerestell Smith, co-founder and CCO, Another Axiom, in a statement. “We’re committed to expanding Gorilla Tag’s place in popular culture and\nwith the help of GGWP, we are excited to foster the best community experience for all Gorilla Tag players.”\nAsked what sets GGWP apart, Fong said, “GGWP is the only comprehensive solution in the market that provides moderation across text, voice, usernames, in-game actions, and Discord. This holistic approach is essential because players interact across multiple channels in game – if you’re only looking at voice, you’re likely missing important context about who and what was responsible for an incident.”\nFong added, “We also combine a user’s history across all of those channels into a unified profile and reputation score.  This really enables moderators to work more efficiently without having to switch to different platforms mid-review.”\nFong said the company has 40 customers to date, many of whom use the voice and complete platform. And he noted that the Unity / Vivox partnership means that GGWP will be able to reach Vivox’s massive customer base – at last count, they powered the voice chat for majority of multiplayer games today.\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Klang Games partners with Google Cloud on AI-driven simulation game Seed",
        "url": "https://venturebeat.com/games/klang-games-partners-with-google-cloud-on-ai-driven-simulation-game-seed/",
        "content": "Klang, the pioneering studio behind the ambitious simulation Seed, announced a strategic partnership with Google Cloud to bring its vision of a dynamic, AI-driven society to life.\nBy using Google Cloud’s cutting-edge technologies, including Google Kubernetes Engine (GKE), Vertex AI, and Gemini model, Berlin-based Klang is creating a persistent, evolving virtual world inhabited by hundreds of thousands of autonomous virtual humans, known as Seedlings.\n104_Building games players actually want to PLAY - How Web3 is enabling new innovation and IP into the gaming space.mp4\nSeed is a groundbreaking massively multiplayer online (MMO) experience where AI-powered Seedlings live, interact, and evolve in real-time, even when players are offline. Klang Games is using Google Cloud’s AI tools to bring its society simulation game seed to life.\nSeedlings have a conversation in Klang Games’ Seed.\nSeed is an ambitious game simulating the future of humanity. At the end of the 21st century, humanity left Earth to find a new home. The planet Avesta in the Tau Ceti system was chosen for its many similarities to Earth. On this new lush planet, players are in charge of nurturing and guiding human inhabitants called “Seedlings” — each with a mind of their own.\nThe player guides Seedlings by setting broad, strategic goals rather than micromanaging every moment of their day. This includes choosing educational paths and specializations—such as focusing on particular skill sets or fields—shaping their future careers and personal growth. Your influence also extends to relationships, purchases, housing, and décor, defining each Seedling’s aspirations even when you’re not online.\nA calendar system is currently under development to track major life events, making it simple to see what has happened or what’s coming up. Beyond that, you can talk directly with your Seedling about their experiences—like catching up with a friend or relative you haven’t seen in a while.\nGoogle Cloud’s infrastructure provides the scalability, performance, and reliability necessary to power Seed’s complex ecosystem, ensuring seamless growth and continuous operation.\n“We’re very excited and we’ve been running on the Google Cloud since the very start,” said Mundi Vondi, CEO of Klang, in an interview with GamesBeat.\nBerlin-based Klang Games got started with prototyping with a small team of four people back in 2016. They got some funding in 2017 and a larger round in 2019. Now the company has more than 100 people and the company hopes to have a soft launch later in 2025.\nADVERTISEMENT\nA Seedling at work in Seed.\n“It’s in a pretty decent state right now. It’s playable,” Vondi said. “I think we have something really special here.”\nJack Buser, global director for games at Google Cloud, said in an interview with GamesBeat that Klang Games has been ahead of the curve.\n“This is a sign of things that we see in development, but they are further along than most. We’ll be able to go into this GDC and show how far Seed has come and how they’re using Google Cloud to scale inference, things like that. It is a sign of the future,” Buser said.\nADVERTISEMENT\nThe company started with four people working on prototypes in 2016. They got funding in 2017 and kept growing, said Vondi.\nThe company now has more than 100 people and it is aiming to launch into early access this year. And now the project has incorporated generative AI into the gameplay.\n“At Klang, we are building the largest simulation of humanity’s future ever attempted, with an\nunprecedented level of detail,” said Vondi. “From vast cities where every house can be furnished and fully operational, to Seedlings that behave, remember, learn, and grow with a level of fidelity like nothing we’ve seen before.”\nVondi added, “It’s incredibly exciting to partner with Google Cloud, who is helping us build the technology to scale this vision beyond what was previously possible, connecting thousands, if not millions, of players.”\nSeedlings live their lives in Seed.\nGoogle Cloud’s technology enables Seed’s Seedlings to exhibit unique personalities, form relationships,\nand shape emergent societies through natural conversations and persistent interactions. Vertex AI and Gemini 2.0 allow for rich, nuanced character interactions, while GKE ensures the complex and ever-\ngrowing world scales seamlessly. Multi-region GKE clusters facilitate fast expansion of the game servers ensuring low latency and seamless transition between in-game zones to establish and maintain a scalable backbone for the massive, persistent online world with a large number of players.\n“Klang’s vision for Seed represents the cutting edge of interactive entertainment, and we’re thrilled that Google Cloud’s technology is empowering them to bring it to life,” Buser. “The scale and complexity of Seed demands a robust and innovative cloud infrastructure with state-of-the-art AI, and we’re proud to provide the technology and expertise necessary to support their ambitious journey.”\n“Seed requires a cloud platform that can handle the complexity of a continuously evolving society\nsimulation,” said Oddur Magnusson, CTO of Klang. “Google Cloud with Vertex AI provide the\nperformance, reliability, and AI capabilities we need to bring this ambitious vision to life.”\nThe partnership also includes Google’s consulting and technical expertise to optimize generative AI\nsolutions for cost and scalability, ensuring Seed delivers an unparalleled AI-powered, in-game\nexperience.\nLiving games\nSeedlings at the gym in Seed.\nThis kind of game is what Buser had in mind when he predicted that AI would lead to “living games.”\n“We had several other data points that made it become clear that this was the direction that the industry is moving in and, as we stand here today, all that momentum’s coming for real,” Buser said.\nVondi added, “It’s truly incredible what’s capable of doing that with AI. It’s really coming to life in a very exciting way.”\nGenerative AI came along at the perfect time for Seed. Before, we would have relied on emojis and limited interactions like in The Sims, as genuine conversations between Seedlings and players weren’t possible. Now, with generative AI, the devs can provide each Seedling with their context – personality traits, needs, history, and aspirations—allowing them to hold conversations. Vondi said you can ask the nihilist to watch TV, but it may reply, “I will watch TV. Do not expect me to enjoy it.”\nThe characters will be able to reference things from the backstory as their memories and they can use these memories in conversations with players.\nBuser said the game can scale as needed thanks to the backend infrastructure where the game can check a number of different AI models to see what will be the most relevant to use. Gemini 2.0 Flash delivers low-latency answers where time delays matter for inference at scale, Buser said. That’s where conversations are happening all over the place in the game.\nThe landscape of Seed.\n“We will launch with some scale limitations but after we progress” it will get better, he said, in terms of doing AI processing on a large scale, Vondi said.\nBuser said a traditional online game will tap infrastructure such as gamer servers and services, databases and analytics tools.\n“What Klang is designing with Seed is truly just a fantastic example of a living game where you have an AI-native game design,” Buser said. We sat down for a conversation to see the vision that Klang had and how it meshed with the thinking we had done at Google Cloud. These things came together. It’s not often you have that kind of serendipity where the pieces fit together.”\nThe more consumers buy AI PCs, the more it’s possible to offload work to local computers and further reduce the cost of AI.\n“We’ve done a lot of thinking at Google over the last couple years about building systems within GKE that can assist the game and knowing which workloads are going to work best on the cloud, and which workloads are going to work best on the edge by device,” Buser said. “It’s top of mind for a lot of developers. If you are operating a game and you want to run inference on device, but the device isn’t powerful enough, or perhaps you’re already running enough inference workloads on the device that it can’t take anymore, you might want to reroute that to the cloud.”\nStay in the know! Get the latest news in your inbox daily\nSubscribe"
    },
    {
        "title": "Inside Zoom’s AI evolution: From basic meeting tools to agentic productivity platform powered by LLMs and SLMs",
        "url": "https://venturebeat.com/ai/inside-zooms-ai-evolution-from-basic-meeting-tools-to-agentic-productivity-platform-powered-by-llms-and-slms/",
        "content": "Zoom became a household name during the pandemic as remote work became the norm nearly overnight.\nWhile the company was once synonymous only with video conferencing, it has been quietly building a sophisticated AI infrastructure over the last several years with an aim to redefine workplace productivity. While video conferencing is important and remains the cornerstone of Zoom’s business, there’s a lot more now, too, thanks to AI.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nMoving from meeting to milestone\nEveryone knows that Zoom is a technology for meetings. But what is the meeting for?\nIn a business context, certainly there can be meetings that have no purpose, but those should be outliers. Meetings should lead to something, whether that’s an action item or some other milestone.\n“In the agentic AI era, finally technology is reaching the point that we can transform from meeting to milestone,” Zoom CTO Xuedong (X.D.) Huang told VentureBeat in an exclusive interview.\nToday, Zoom is announcing an aggressive agentic AI strategy that includes a series of new services. The update introduces agentic capabilities that promise to transform meetings from communication events into action-oriented workflows, alongside a new AI Studio that lets enterprises create customized AI agents.\nThe hidden technical evolution behind Zoom’s agentic AI \nPrior to joining Zoom, Huang spent 30 years at Microsoft, working on speech technologies as well as Microsoft’s Azure OpenAI service. He carried forward a lot of lessons learned from that experience when he joined Zoom in 2023.\nUnder Huang’s direction, Zoom began quietly building an AI architecture designed to facilitate tasks rather than just summarize conversations. Zoom publicly announced a partnership with Anthropic in May 2023 — but that’s not the only large language model (LLM) used at Zoom.\nWhile Microsoft Teams generally relies on OpenAI via the Microsoft OpenAI Azure service, and Google Meet is supported by Google Gemini, Zoom has taken an agnostic approach to LLMs.\nADVERTISEMENT\nHuang explained that when Zoom launched the first iteration of its AI companion in 2023, it wasn’t based on any one single LLM. Instead, the company started off with a federated approach, using multiple LLMs including its own custom built small language model (SLM).\n“We’ve partnered with the best models out there, including OpenAI and Anthropic, but we’ve also built our own highly customized 2 billion parameter language model,” said Huang.\nADVERTISEMENT\nZoom’s AI Companion uses a federated approach in which the smaller Zoom model is used in conjunction with larger, industry-leading language models. The smaller model initially evaluates and processes the input, and the partial results are then passed to larger models to produce the final output. This approach allows Zoom to take advantage of the strengths of both the smaller, customized model and the larger, more powerful models, while reducing costs and improving performance.\nHow the small language model is at the center of Zoom’s agentic AI journey\nPerhaps the most technically intriguing aspect of Zoom’s AI strategy is its focus on SLMs. Rather than following the industry trend of distilling smaller models from larger ones, Zoom built its 2-billion parameter model entirely from scratch.\nThe technical advantage of this approach becomes apparent when customizing for specific domains. “When you customize, it takes more effort, it’s just hard to steer a bigger ship,” Huang explained.\nAs it turns out, the ability to customize the small model is a critical component to the development of specific agentic AI workflows. Looking ahead, Zoom envisions its SLMs eventually running directly on user devices, enabling both better privacy and more personalized experiences.\nAI companion 2.0: Agentic AI transforms meetings to milestones\nAt the heart of Zoom’s updates is AI Companion 2.0, which transforms Zoom’s AI capabilities from meeting support to fully agentic functions. With 2.0, Zoom is evolving from assistant to agentic AI that is capable of reasoning, memory and task execution.\nThe evolved AI Companion can now execute multi-step actions on behalf of users, orchestrating tasks like scheduling meetings, generating video clips and creating documents.\nKey updates include:\nAgentic skills: Calendar management, clip generation, advanced writing assistance;\nTask management: Automatic detection of action items from meetings and chats;\nMeeting enhancements: AI-powered agendas, live notes and voice recording;\nDocument creation: Advanced references and automatic data table generation in Zoom Docs;\nVirtual agents: Self-service capabilities for customer service with both chat and voice support;\nIndustry solutions: Specialized tools for frontline workers, healthcare professionals and educators;\nZoom Drive: New central repository for meeting assets and productivity documents;\nCustom avatars: AI-generated video avatars for creating presentation clips.\nMost features will roll out between March and July 2025. While the standard AI Companion is included at no additional cost for paid users, specialized agents and custom configurations will require additional fees.\n“The most important aspect for us of agentic AI is really enabling the action-oriented information flow,” said Huang. “What that means is that when you have a meeting, the action task will flow into Docs or chat or into other actions you have to take.”\nAI Studio: Building custom agents for enterprises \nWhile Zoom is providing a lot of different agentic AI capabilities out-of-the-box for users, Huang recognized that enterprises often need more customized options.\nThat’s where AI Studio comes in, allowing companies to create customized AI agents tailored to specific business needs. These can be deeply integrated with company-specific knowledge and workflow processes.\nAs an example, Huang detailed one practical application for human resources policy. Enterprises can use the AI Studio to upload all of their internal HR policy documents. The AI companion will then be trained on this company-specific HR policy information, allowing it to accurately answer employee questions about HR guidelines and procedures.\nIT administrators can also use the AI Studio to connect the companion to other internal knowledge bases, like IT support documentation. The goal is to enable companies to create AI agents that are deeply integrated with their own processes, data and workflows, transforming the AI companion into a customized and valuable productivity tool.\nImplications for enterprise AI decision-makers\nFor technical decision-makers evaluating productivity AI solutions, Zoom’s approach offers several distinctive considerations compared to alternatives from Microsoft, Google and other vendors.\nThe action-oriented information flow model may better suit organizations where meetings are the primary collaboration medium, and where task completion is hampered by information fragmentation across various tools. Companies with high meeting volumes might find particular value in Zoom’s ability to connect conversations to subsequent actions.\nAdditionally, the federated AI approach combining SLMs and LLMs presents an architecture worth studying — potentially offering better economics while maintaining quality. As AI costs become a growing concern for enterprises scaling their AI implementations, this balanced approach could prove influential.\nFor enterprises looking to lead the way in AI adoption, Zoom’s evolution from meeting tool to comprehensive productivity platform offers valuable lessons in how to build on existing strengths rather than simply adding AI to existing workflows. By leveraging its meeting dominance to reimagine the entire productivity experience, Zoom is demonstrating how domain expertise can be combined with AI capabilities to create solutions that address specific business problems rather than generic AI assistants.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Inching towards AGI: How reasoning and deep research are expanding AI from statistical prediction to structured problem-solving",
        "url": "https://venturebeat.com/ai/inching-towards-agi-how-reasoning-and-deep-research-are-expanding-ai-from-statistical-prediction-to-structured-problem-solving/",
        "content": "AI has evolved at an astonishing pace. What seemed like science fiction just a few years ago is now an undeniable reality. Back in 2017, my firm launched an AI Center of Excellence. AI was certainly getting better at predictive analytics and many machine learning (ML) algorithms were being used for voice recognition, spam detection, spell checking (and other applications) — but it was early. We believed then that we were only in the first inning of the AI game.\nThe arrival of GPT-3 and especially GPT 3.5 — which was tuned for conversational use and served as the basis for the first ChatGPT in November 2022 — was a dramatic turning point, now forever remembered as the “ChatGPT moment.” \nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nSince then, there has been an explosion of AI capabilities from hundreds of companies. In March 2023 OpenAI released GPT-4, which promised “sparks of AGI” (artificial general intelligence). By that time, it was clear that we were well beyond the first inning. Now, it feels like we are in the final stretch of an entirely different sport.\nThe flame of AGI\nTwo years on, the flame of AGI is beginning to appear.\nOn a recent episode of the Hard Fork podcast, Dario Amodei — who has been in the AI industry for a decade, formerly as VP of research at OpenAI and now as CEO of Anthropic — said there is a 70 to 80% chance that we will have a “very large number of AI systems that are much smarter than humans at almost everything before the end of the decade, and my guess is 2026 or 2027.”\nAnthropic CEO Dario Amodei appearing on the Hard Fork podcast. Source: https://www.youtube.com/watch?v=YhGUSIvsn_Y \nThe evidence for this prediction is becoming clearer. Late last summer, OpenAI launched o1 — the first “reasoning model.” They’ve since released o3, and other companies have rolled out their own reasoning models, including Google and, famously, DeepSeek. Reasoners use chain-of-thought (COT), breaking down complex tasks at run time into multiple logical steps, just as a human might approach a complicated task. Sophisticated AI agents including OpenAI’s deep research and Google’s AI co-scientist have recently appeared, portending huge changes to how research will be performed. \nADVERTISEMENT\nUnlike earlier large language models (LLMs) that primarily pattern-matched from training data, reasoning models represent a fundamental shift from statistical prediction to structured problem-solving. This allows AI to tackle novel problems beyond its training, enabling genuine reasoning rather than advanced pattern recognition.\nI recently used Deep Research for a project and was reminded of the quote from Arthur C. Clarke: “Any sufficiently advanced technology is indistinguishable from magic.” In five minutes, this AI produced what would have taken me 3 to 4 days. Was it perfect? No. Was it close? Yes, very. These agents are quickly becoming truly magical and transformative and are among the first of many similarly powerful agents that will soon come onto the market.\nADVERTISEMENT\nThe most common definition of AGI is a system capable of doing almost any cognitive task a human can do. These early agents of change suggest that Amodei and others who believe we are close to that level of AI sophistication could be correct, and that AGI will be here soon. This reality will lead to a great deal of change, requiring people and processes to adapt in short order. \nBut is it really AGI?\nThere are various scenarios that could emerge from the near-term arrival of powerful AI. It is challenging and frightening that we do not really know how this will go. New York Times columnist Ezra Klein addressed this in a recent podcast: “We are rushing toward AGI without really understanding what that is or what that means.” For example, he claims there is little critical thinking or contingency planning going on around the implications and, for example, what this would truly mean for employment.\nOf course, there is another perspective on this uncertain future and lack of planning, as exemplified by Gary Marcus, who believes deep learning generally (and LLMs specifically) will not lead to AGI. Marcus issued what amounts to a take down of Klein’s position, citing notable shortcomings in current AI technology and suggesting it is just as likely that we are a long way from AGI. \nMarcus may be correct, but this might also be simply an academic dispute about semantics. As an alternative to the AGI term, Amodei simply refers to “powerful AI” in his Machines of Loving Grace blog, as it conveys a similar idea without the imprecise definition, “sci-fi baggage and hype.” Call it what you will, but AI is only going to grow more powerful.\nPlaying with fire: The possible AI futures\nIn a 60 Minutes interview, Alphabet CEO Sundar Pichai said he thought of AI as “the most profound technology humanity is working on. More profound than fire, electricity or anything that we have done in the past.” That certainly fits with the growing intensity of AI discussions. Fire, like AI, was a world-changing discovery that fueled progress but demanded control to prevent catastrophe. The same delicate balance applies to AI today.\nA discovery of immense power, fire transformed civilization by enabling warmth, cooking, metallurgy and industry. But it also brought destruction when uncontrolled. Whether AI becomes our greatest ally or our undoing will depend on how well we manage its flames. To take this metaphor further, there are various scenarios that could soon emerge from even more powerful AI:\nThe controlled flame (utopia): In this scenario, AI is harnessed as a force for human prosperity. Productivity skyrockets, new materials are discovered, personalized medicine becomes available for all, goods and services become abundant and inexpensive and individuals are freed from drudgery to pursue more meaningful work and activities. This is the scenario championed by many accelerationists, in which AI brings progress without engulfing us in too much chaos.\nThe unstable fire (challenging): Here, AI brings undeniable benefits — revolutionizing research, automation, new capabilities, products and problem-solving. Yet these benefits are unevenly distributed — while some thrive, others face displacement, widening economic divides and stressing social systems. Misinformation spreads and security risks mount. In this scenario, society struggles to balance promise and peril. It could be argued that this description is close to present-day reality.\nThe wildfire (dystopia): The third path is one of disaster, the possibility most strongly associated with so-called “doomers” and “probability of doom” assessments. Whether through unintended consequences, reckless deployment or AI systems running beyond human control, AI actions become unchecked, and accidents happen. Trust in truth erodes. In the worst-case scenario, AI spirals out of control, threatening lives, industries and entire institutions.\nWhile each of these scenarios appears plausible, it is discomforting that we really do not know which are the most likely, especially since the timeline could be short. We can see early signs of each: AI-driven automation increasing productivity, misinformation that spreads at scale, eroding trust and concerns over disingenuous models that resist their guardrails. Each scenario would cause its own adaptations for individuals, businesses, governments and society.\nOur lack of clarity on the trajectory for AI impact suggests that some mix of all three futures is inevitable. The rise of AI will lead to a paradox, fueling prosperity while bringing unintended consequences. Amazing breakthroughs will occur, as will accidents. Some new fields will appear with tantalizing possibilities and job prospects, while other stalwarts of the economy will fade into bankruptcy. \nWe may not have all the answers, but the future of powerful AI and its impact on humanity is being written now. What we saw at the recent Paris AI Action Summit was a mindset of hoping for the best, which is not a smart strategy. Governments, businesses and individuals must shape AI’s trajectory before it shapes us. The future of AI won’t be determined by technology alone, but by the collective choices we make about how to deploy it.\nGary Grossman is EVP of technology practice at Edelman.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    },
    {
        "title": "Launching your first AI project with a grain of RICE: Weighing reach, impact, confidence and effort to create your roadmap",
        "url": "https://venturebeat.com/ai/launching-your-first-ai-project-with-a-grain-of-rice-weighing-reach-impact-confidence-and-effort-to-create-your-roadmap/",
        "content": "Businesses know they can’t ignore AI, but when it comes to building with it, the real question isn’t, What can AI do — it’s, What can it do reliably? And more importantly: Where do you start?\nThis article introduces a framework to help businesses prioritize AI opportunities. Inspired by project management frameworks like the RICE scoring model for prioritization, it balances business value, time-to-market, scalability and risk to help you pick your first AI project.\nWhy Private Compute Should Be Part of Your AI Strategy - AI Impact Tour 2024\nWhere AI is succeeding today\nAI isn’t writing novels or running businesses just yet, but where it succeeds is still valuable. It augments human effort, not replaces it. \nIn coding, AI tools improve task completion speed by 55% and boost code quality by 82%. Across industries, AI automates repetitive tasks — emails, reports, data analysis—freeing people to focus on higher-value work.\nThis impact doesn’t come easy. All AI problems are data problems. Many businesses struggle to get AI working reliably because their data is stuck in silos, poorly integrated or simply not AI-ready. Making data accessible and usable takes effort, which is why it’s critical to start small.\nGenerative AI works best as a collaborator, not a replacement. Whether it’s drafting emails, summarizing reports or refining code, AI can lighten the load and unlock productivity. The key is to start small, solve real problems and build from there.\nA framework for deciding where to start with generative AI\nEveryone recognizes the potential of AI, but when it comes to making decisions about where to start, they often feel paralyzed by the sheer number of options.\nThat’s why having a clear framework to evaluate and prioritize opportunities is essential. It gives structure to the decision-making process, helping businesses balance the trade-offs between business value, time-to-market, risk and scalability.\nADVERTISEMENT\nThis framework draws on what I’ve learned from working with business leaders, combining practical insights with proven approaches like RICE scoring and cost-benefit analysis, to help businesses focus on what really matters: Delivering results without unnecessary complexity.\nWhy a new framework?\nWhy not use existing frameworks like RICE?\nWhile useful, they don’t fully account for AI’s stochastic nature. Unlike traditional products with predictable outcomes, AI is inherently uncertain. The “AI magic” fades fast when it fails, producing bad results, reinforcing biases or misinterpreting intent. That’s why time-to-market and risk are critical. This framework helps bias against failure, prioritizing projects with achievable success and manageable risk.\nADVERTISEMENT\nBy tailoring your decision-making process to account for these factors, you can set realistic expectations, prioritize effectively and avoid the pitfalls of chasing over-ambitious projects. In the next section, I’ll break down how the framework works and how to apply it to your business.\nThe framework: Four core dimensions\nBusiness value:\nWhat’s the impact? Start by identifying the potential value of the application. Will it increase revenue, reduce costs or enhance efficiency? Is it aligned with strategic priorities? High-value projects directly address core business needs and deliver measurable results.\nTime-to-market:\nHow quickly can this project be implemented? Evaluate the speed at which you can go from idea to deployment. Do you have the necessary data, tools and expertise? Is the technology mature enough to execute efficiently? Faster implementations reduce risk and deliver value sooner.\nRisk:\nWhat could go wrong?: Assess the risk of failure or negative outcomes. This includes technical risks (will the AI deliver reliable results?), adoption risks (will users embrace the tool?) and compliance risks (are there data privacy or regulatory concerns?). Lower-risk projects are better suited for initial efforts. Ask yourself if you can only achieve 80% accuracy, is that ok?\nScalability (long-term viability):\nCan the solution grow with your business? Evaluate whether the application can scale to meet future business needs or handle higher demand. Consider the long-term feasibility of maintaining and evolving the solution as your requirements grow or change.\nScoring and prioritization\nEach potential project is scored across these four dimensions using a simple 1-5 scale:\nBusiness value: How impactful is this project?\nTime-to-market: How realistic and quick is it to implement?\nRisk: How manageable are the risks involved? (Lower risk scores are better.)\nScalability: Can the application grow and evolve to meet future needs?\nFor simplicity, you can use T-shirt sizing (small, medium, large) to score dimensions instead of numbers.\nCalculating a prioritization score\nOnce you’ve sized or scored each project across the four dimensions, you can calculate a prioritization score:\nPrioritization score formula. Source: Sean Falconer\nHere, α (the risk weight parameter) allows you to adjust how heavily risk influences the score:\nα=1 (standard risk tolerance): Risk is weighted equally with other dimensions. This is ideal for organizations with AI experience or those willing to balance risk and reward.\nα> (risk-averse organizations): Risk has more influence, penalizing higher-risk projects more heavily. This is suitable for organizations new to AI, operating in regulated industries, or in environments where failures could have significant consequences. Recommended values: α=1.5 to α=2\nα<1 (high-risk, high-reward approach): Risk has less influence, favoring ambitious, high-reward projects. This is for companies comfortable with experimentation and potential failure. Recommended values: α=0.5 to α=0.9\nBy adjusting α, you can tailor the prioritization formula to match your organization’s risk tolerance and strategic goals. \nThis formula ensures that projects with high business value, reasonable time-to-market, and scalability — but manageable risk — rise to the top of the list.\nApplying the framework: A practical example\nLet’s walk through how a business could use this framework to decide which gen AI project to start with. Imagine you’re a mid-sized e-commerce company looking to leverage AI to improve operations and customer experience.\nStep 1: Brainstorm opportunities\nIdentify inefficiencies and automation opportunities, both internal and external. Here’s a brainstorming session output:\nInternal opportunities:\nAutomating internal meeting summaries and action items.\nGenerating product descriptions for new inventory.\nOptimizing inventory restocking forecasts.\nPerforming sentiment analysis and automatic scoring for customer reviews.\nExternal opportunities:\nCreating personalized marketing email campaigns.\nImplementing a chatbot for customer service inquiries.\nGenerating automated responses for customer reviews.\nStep 2: Build a decision matrix\nApplication Business value Time-to-market Scalability Risk Score\nMeeting Summaries 3 5 4 2 30\nProduct Descriptions 4 4 3 3 16\nOptimizing Restocking 5 2 4 5 8\nSentiment Analysis for Reviews 5 4 2 4 10\nPersonalized Marketing Campaigns 5 4 4 4 20\nCustomer Service Chatbot 4 5 4 5 16\nAutomating Customer Review Replies 3 4 3 5 7.2\nEvaluate each opportunity using the four dimensions: Business value, time-to-market, risk and scalability. In this example, we’ll assume a risk weight value of α=1. Assign scores (1-5) or use T-shirt sizes (small, medium, large) and translate them to numerical values.\nStep 3: Validate with stakeholders\nShare the decision matrix with key stakeholders to align on priorities. This might include leaders from marketing, operations and customer support. Incorporate their input to ensure the chosen project aligns with business goals and has buy-in.\nStep 4: Implement and experiment\nStarting small is critical, but success depends on defining clear metrics from the beginning. Without them, you can’t measure value or identify where adjustments are needed.\nStart small: Begin with a proof of concept (POC) for generating product descriptions. Use existing product data to train a model or leverage pre-built tools. Define success criteria upfront — such as time saved, content quality or the speed of new product launches.\nMeasure outcomes: Track key metrics that align with your goals. For this example, focus on:\nEfficiency: How much time is the content team saving on manual work?\nQuality: Are product descriptions consistent, accurate and engaging?\nBusiness impact: Does the improved speed or quality lead to better sales performance or higher customer engagement?\nMonitor and validate: Regularly track metrics like ROI, adoption rates and error rates. Validate that the POC results align with expectations and make adjustments as needed. If certain areas underperform, refine the model or adjust workflows to address those gaps.\nIterate: Use lessons learned from the POC to refine your approach. For example, if the product description project performs well, scale the solution to handle seasonal campaigns or related marketing content. Expanding incrementally ensures you continue to deliver value while minimizing risks.\nStep 5: Build expertise\nFew companies start with deep AI expertise — and that’s okay. You build it by experimenting. Many companies start with small internal tools, testing in a low-risk environment before scaling.\nThis gradual approach is critical because there’s often a trust hurdle for businesses that must be overcome. Teams need to trust that the AI is reliable, accurate and genuinely beneficial before they’re willing to invest more deeply or use it at scale. By starting small and demonstrating incremental value, you build that trust while reducing the risk of overcommitting to a large, unproven initiative.\nEach success helps your team develop the expertise and confidence needed to tackle larger, more complex AI initiatives in the future.\nWrapping Up\nYou don’t need to boil the ocean with AI. Like cloud adoption, start small, experiment and scale as value becomes clear.\nAI should follow the same approach: start small, learn, and scale. Focus on projects that deliver quick wins with minimal risk. Use those successes to build expertise and confidence before expanding into more ambitious efforts.\nGen AI has the potential to transform businesses, but success takes time. With thoughtful prioritization, experimentation and iteration, you can build momentum and create lasting value.\nSean Falconer is AI entrepreneur in residence at Confluent.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI."
    }
]